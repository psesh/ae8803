{
 "cells": [
  {
   "cell_type": "raw",
   "id": "1135697a-df43-4def-88fc-9967b24b7649",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Overview\"\n",
    "format:\n",
    "    html:\n",
    "        code-fold: true\n",
    "jupyter: python3\n",
    "fontsize: 1.2em\n",
    "linestretch: 1.5\n",
    "toc: true\n",
    "notebook-view: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf6f8f0",
   "metadata": {},
   "source": [
    "### Course Description:\n",
    "\n",
    "This graduate-level course offers a practical approach to probabilistic learning with Gaussian processes (GPs). GPs represent a powerful set of methods for modeling and predicting a wide variety of spatio-temporal phenomena. Today, they are used for problems that span both regression and classification, with theoretical foundations in Bayesian inference, reproducing kernel Hilbert spaces, eigenvalue problems, and numerical integration. Rather than focus *solely* on these theoretical foundations, this course balances theory with practical probabilistic programming, using a variety of ``python``-based packages. Moreover, practical engineering problems will also be discussed that see GP models that cut across other areas of machine learning including transfer learning, convolutional networks, and normalizing flows. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a99fd2",
   "metadata": {},
   "source": [
    "## Grading\n",
    "\n",
    "This course has four assignments; the grades are given below:\n",
    "\n",
    "\n",
    "| Assignment  | Grade percentage (%)    |\n",
    "| --------  | ------- |\n",
    "| Assignment 1: Mid-term (covering fundamentals)    | 20 |\n",
    "| Assignment 2: Build your own GP from scratch for a given dataset | 20 |\n",
    "| Assignment 3: Proposal (data and literature review)   | 20    |\n",
    "| Assignment 4: Final project (presentation and notebook)  | 40    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d26c5a",
   "metadata": {},
   "source": [
    "### Pre-requisites:\n",
    "\n",
    "- CS1371, MATH2551, MATH2552 (or equivalent)\n",
    "- Working knowledge of ``python`` including familiarity with ``numpy`` and ``matplotlib`` libraries. \n",
    "- Working local version of ``python`` and ``Jupyter``. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19a9fab",
   "metadata": {},
   "source": [
    "## Lectures\n",
    "\n",
    "Below you will find a list of the lectures that form the backbone of this course. Sub-topics for each lecture will be updated in due course. \n",
    "\n",
    "01.08: **L1. Introduction & probability fundamentals** | <a href=\"https://gtvault-my.sharepoint.com/:b:/g/personal/pseshadri34_gatech_edu/Edq6QWhxcXJDu1KONfch_30B3ELimiqkhzWTuYNZbLOuLg?e=Nfc5ZN\" target=\"_blank\" style=\"text-decoration: none\">Slides</a> | <a href=\"sample_problems/lecture_1.html\" style=\"text-decoration: none\">Examples</a>  \n",
    "<details>\n",
    "  <summary>Contents</summary>\n",
    "\n",
    "  1. Course overview.\n",
    "  2. Probability fundamentals (and Bayes' theorem).\n",
    "  3. Random variables.\n",
    "</details>\n",
    "\n",
    "01.10: **L2. Discrete probability distributions** | <a href=\"https://gtvault-my.sharepoint.com/:b:/g/personal/pseshadri34_gatech_edu/Ef-ZWBHcAFJMhceuoj68lS8B34zuF6xV11Vg1HnoEXQIQA?e=r5ojOP\" target=\"_blank\" style=\"text-decoration: none\">Slides</a> | <a href=\"sample_problems/lecture_2.html\" style=\"text-decoration: none\">Examples</a>  | <a href=\"useful_codes/discrete.html\" style=\"text-decoration: none\">Notebook</a> \n",
    "\n",
    "<details>\n",
    "  <summary>Contents</summary>\n",
    "\n",
    "  1. Expectation and variance.\n",
    "  2. Independence.\n",
    "  3. Bernoulli and Binomial distributions. \n",
    "\n",
    "</details>\n",
    "\n",
    "01.15: *No Class (Institute Holiday)*\n",
    "\n",
    "01.17: **L3. Continuous distributions** \n",
    "<details>\n",
    "  <summary>Contents</summary>\n",
    "\n",
    "  1. Fundamentals of continuous random variables.\n",
    "  2. Probability density function.\n",
    "  3. Exponential, Beta, and Gaussian distributions. \n",
    "</details>\n",
    "\n",
    "01.22: **L4. Manipulating and combining distributions** \n",
    "<details>\n",
    "  <summary>Contents</summary>\n",
    "\n",
    "  1. Functions of random variables. \n",
    "  2. Sums of random variables.\n",
    "  3. Transforming a distribution.\n",
    "  4. Central limit theorem. \n",
    "</details>\n",
    "\n",
    "01.24: **L5. Multivariate Gaussian distributions** \n",
    "<details>\n",
    "  <summary>Contents</summary>\n",
    "\n",
    "  1. Marginal distributions.\n",
    "  2. Conditional distributions.\n",
    "  3. Joint distribution and Schur complement. \n",
    "  4. Kullback-Leibler divergence and Wasserstein-2 distance.  \n",
    "\n",
    "</details>\n",
    "\n",
    "01.29: **L6. Bayesian inference in practice** \n",
    "<details>\n",
    "  <summary>Contents</summary>\n",
    "\n",
    "  1. Conjugacy in Bayesian inference.\n",
    "  2. Polynomial Bayesian inference: an example\n",
    "</details>\n",
    "\n",
    "\n",
    "01.31: **L7. Gaussian process regression** \n",
    "<details>\n",
    "  <summary>Contents</summary>\n",
    "\n",
    "  1. Contrast weight-space vs function-space perspective.\n",
    "  2. Introduction to a kernel. \n",
    "  3. Likelihood and prior for a Gaussian process.\n",
    "  3. Posterior mean and covariance. \n",
    "</details>\n",
    "\n",
    "02.05: *Fundamentals Mid-term*\n",
    "\n",
    "02.07: **L8. Hyperparameters and model selection** \n",
    "<details>\n",
    "  <summary>Contents</summary>\n",
    "\n",
    "  1. Maximum likelihood and maximum aposteriori estimate. \n",
    "  2. Cross validation.\n",
    "  3. Expectation maximization.\n",
    "  4. Markov chain Monte Carlo (Gibbs, NUTS, HMC). \n",
    "</details>\n",
    "\n",
    "02.12: **L9. Variational inference** \n",
    "<details>\n",
    "  <summary>Contents</summary>\n",
    "\n",
    "  1. Variational problem.\n",
    "  2. Deriving the ELBO.\n",
    "  3. Stochastic variational inference in practice. \n",
    "</details>\n",
    "\n",
    "02.14: **L10. Open-source resources** \n",
    "<details>\n",
    "  <summary>Contents</summary>\n",
    "\n",
    "  1. pymc.\n",
    "  2. gpytorch, gpflow.\n",
    "  3. GPjax.\n",
    "</details>\n",
    "\n",
    "02.14: **L11. Kernel learning** \n",
    "<details>\n",
    "  <summary>Contents</summary>\n",
    "    \n",
    "    1. Kernel trick re-visited.\n",
    "    2. Constructing kernels piece-by-piece.\n",
    "    3. Constructing kernels from learnt features.\n",
    "    4. Spectral representations of kernels. \n",
    "</details>\n",
    "\n",
    "\n",
    "02.19: **L12. Gaussian process classification** \n",
    "<details>\n",
    "  <summary>Contents</summary>\n",
    "\n",
    "  1. Bernoulli prior\n",
    "  2. Softmax for multi-class classification\n",
    "</details>\n",
    "\n",
    "02.21: **L13. Scaling up Gausssian processes I** \n",
    "<details>\n",
    "  <summary>Contents</summary>\n",
    "\n",
    "  1. Review of matrix inverse via Cholesky.\n",
    "  2. Subset of data approaches\n",
    "  3. Nystrom approximation\n",
    "  4. Inducing points\n",
    "  5. Kronecker product kernels.\n",
    "</details>\n",
    "\n",
    "02.26: **L14. Scaling up Gausssian processes II** \n",
    "<details>\n",
    "  <summary>Contents</summary>\n",
    "\n",
    "  1. Variational inference\n",
    "  2. ELBO derivation\n",
    "  3. Minimizing the KL-divergence practically using Adam. \n",
    "</details>\n",
    "\n",
    "02.28: **L15. Sparse (and subspace-based) Gaussian processes** \n",
    "<details>\n",
    "  <summary>Contents</summary>\n",
    "\n",
    "  1. Brief introduction to matrix manifolds.\n",
    "  2. Subspace-based projections.\n",
    "  3. Active subspaces.\n",
    "  4. Sparsity promoting priors.\n",
    "</details>\n",
    "\n",
    "03.04: **L16. Proposal and project** \n",
    "<details>\n",
    "  <summary>Contents</summary>\n",
    "\n",
    "  1. Chosen data-set(s) and problem statement.\n",
    "  2. Literature review.\n",
    "  3. Prior and likelihood definitions. \n",
    "</details>\n",
    "\n",
    "03.06: *Coding assignment due*\n",
    "\n",
    "03.06: **L17. Reproducing Kernel Hilbert Spaces** \n",
    "<details>\n",
    "  <summary>Contents</summary>\n",
    "\n",
    "  1. Hilbert space\n",
    "  2. Understanding a kernel. \n",
    "  3. Reproducing kernel Hilbert spaces. \n",
    "  4. Representer theoreom.\n",
    "</details>\n",
    "\n",
    "03.11: **L18. Multi-output Gaussian processes** \n",
    "<details>\n",
    "  <summary>Contents</summary>\n",
    "\n",
    "  1. Coregional models.\n",
    "  2. Transfer learning across covariance blocks.\n",
    "  3. Derivative (or gradient) enhancement.\n",
    "</details>\n",
    "\n",
    "03.13: **L19. Deep Gaussian processes** \n",
    "<details>\n",
    "  <summary>Contents</summary>\n",
    "\n",
    "  1. Single and deep MLPs\n",
    "  2. Depth in Gaussian processes. \n",
    "  3. Posterior inference and stochastic variational inference. \n",
    "</details>\n",
    "\n",
    "03.13: *Withdrawal Deadline*\n",
    "\n",
    "03.18-03.22: *Spring Break*\n",
    "\n",
    "03.25: *Project proposals due*\n",
    "\n",
    "03.25: **L20. Convolutional Gaussian processes** \n",
    "<details>\n",
    "  <summary>Contents</summary>\n",
    "\n",
    "  1. Convolution as a linear operator.\n",
    "  2. Deep convolutional Gaussian processes.\n",
    "</details>\n",
    "\n",
    "\n",
    "03.27: **L21. Latent models and unsupervised learning** \n",
    "<details>\n",
    "  <summary>Contents</summary>\n",
    "\n",
    "  1. Contrast standard regression with latent variable model.\n",
    "  2. Gaussian process latent variable model.\n",
    "  3. Coding demo. \n",
    "</details>\n",
    "\n",
    "\n",
    "04.01: **L22. State-space Gaussian processes** \n",
    "<details>\n",
    "  <summary>Contents</summary>\n",
    "\n",
    "  1. Application: time series models.\n",
    "  2. Gaussian state space model.\n",
    "  3. Parallels with Kalman filtering and smoothing.\n",
    "  4. Creating custom state-space kernels. \n",
    "</details>\n",
    "\n",
    "04.03: **L23. Bayesian optimization** \n",
    "<details>\n",
    "  <summary>Contents</summary>\n",
    "\n",
    "  1. Gaussian process surrogate.\n",
    "  2. Acquisition function.\n",
    "  3. Thompson's sampling. \n",
    "  4. Gaussian process dynamic model. \n",
    "</details>\n",
    "\n",
    "04.08: **L24. Guest Lecture** \n",
    "\n",
    "04.22: **L25. Project presentations** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620cd7d6",
   "metadata": {},
   "source": [
    "## Office hours\n",
    "\n",
    "Professor Seshadri's office hours:\n",
    "\n",
    "| Location  | Time    |\n",
    "| --------  | ------- |\n",
    "| MK 421    | Fridays 14:30 to 15:30 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aa2835",
   "metadata": {},
   "source": [
    "## Textbooks\n",
    "\n",
    "This course will make heavy use of the following texts:\n",
    "\n",
    "- Rasmussen, C. E., Williams, C. K. *Gaussian Processes for Machine Learning*, The MIT Press, 2006.\n",
    "- Murphy, K. P., *Probabilistic Machine Learning: Advanced Topics*, The MIT Press, 2023.\n",
    "\n",
    "Both these texts have been made freely available by the authors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ef18bc",
   "metadata": {},
   "source": [
    "## Important papers\n",
    "\n",
    "Students are encouraged to read through the following papers:\n",
    "\n",
    "- [Roberts, S., Osborne, M., Ebden, M., Reece, S., Gibson, N., Aigrain, S., (2013) *Gaussian processes for time-series modelling*, Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences.](https://doi.org/10.1098/rsta.2011.0550)\n",
    "\n",
    "- [Dunlop, M., Girolami, M., Stuart, A., Teckentrup, A., (2018) *How Deep Are Deep Gaussian Processes?*, Journal of Machine Learning Research 19, 1-46](https://www.jmlr.org/papers/volume19/18-015/18-015.pdf)\n",
    "\n",
    "- [Alvarez, M., Lawrence, N., (2011) *Computationally Efficient Convolved Multiple Output Gaussian Processes*, Journal of Machine Learning Research 12, 1459-1500](https://www.jmlr.org/papers/volume12/alvarez11a/alvarez11a.pdf)\n",
    "\n",
    "- [Van der Wilk, M., Rasmussen, C., Hensman, J., (2017) *Convolutional Gaussian Processes*, 31st Conference on Neural Information Processing Systems](https://dl.acm.org/doi/pdf/10.5555/3294996.3295044)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc9bb68",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Material used in this course has been adapted from \n",
    "\n",
    "- CUED Part IB probability course notes\n",
    "- Alto University's module on Gaussian Processes\n",
    "- Slides from the Gaussian Process Summer Schools"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
