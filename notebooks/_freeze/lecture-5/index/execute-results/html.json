{
  "hash": "5690f3293fd598acf48a0e7123c34639",
  "result": {
    "markdown": "---\ntitle: \"Lecture 5\"\nsubtitle: \"Multivariate Gaussians\"\nformat:\n  revealjs: \n    html-math-method: katex\n    slide-number: true\n    chalkboard: \n      buttons: true\n    theme: [serif, style.scss]\n    background-color: \"white\"\n    preview-links: auto\n    logo: images/quarto.png\n    footer: AE8803 | Gaussian Processes for Machine Learning\n    width: 1200\n    height: 700\nexecute:\n  freeze: true  # never re-render during project render\nresources:\n  - demo.pdf\n---\n\n## Multivariate Gaussians {background-color=\"white\"} \n\nAlthough we have introduced joint probabilities and learnt how to manipulate them in the lectures prior, we have thus far only stuied univariate densities.\n\nIn this lecture, we will focus on one multivariate density that sets the stage for our journey into machine learning: the Gaussian distribution!\n\n## Multivariate Gaussians {background-color=\"white\"} \n\nThe random vector, $\\mathbf{X} = \\left(X_1, X_2, \\ldots, X_n \\right)$ is a multivariate Gaussian $\\mathbf{X} \\sim \\mathcal{N} \\left( \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma} \\right)$ if\n\n$$\nf_{\\mathbf{X}} \\left( \\mathbf{x} \\right) = \\frac{1}{\\left( 2 \\pi \\right)^{n/2}} \\left|\\boldsymbol{\\Sigma} \\right|^{-\\frac{1}{2}} exp \\left(-\\frac{1}{2}\\left( \\mathbf{x}-\\boldsymbol{\\mu}\\right)^{T} \\boldsymbol{\\Sigma}^{-1} \\left( \\mathbf{x}-\\boldsymbol{\\mu}\\right) \\right)\n$$\n\nwhere \n\n- $\\boldsymbol{\\Sigma}$ is a $n \\times n$ *covariance matrix* \n- $\\boldsymbol{\\mu} = \\left( \\mu_1, \\mu_2, \\ldots, \\mu_{n} \\right)^{T}$ is a $n \\times 1$ *mean vector*. \n\n\n## Multivariate Gaussians {background-color=\"white\"} \n\n::: panel-tabset\n### Plot\n::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal\nfrom mpl_toolkits.mplot3d import Axes3D\nimport seaborn as sns\nsns.set(font_scale=1.0)\nsns.set_style(\"white\")\nsns.set_style(\"ticks\")\npalette = sns.color_palette('deep')\n\n#Parameters to set\nmu_x = -2\nvariance_x = 3\n\nmu_y = 1\nvariance_y = 6\n\n#Create grid and multivariate normal\nx = np.linspace(-10,10,500)\ny = np.linspace(-10,10,500)\nX, Y = np.meshgrid(x,y)\npos = np.empty(X.shape + (2,))\npos[:, :, 0] = X; pos[:, :, 1] = Y\nrv = multivariate_normal([mu_x, mu_y], [[variance_x, 0], [0, variance_y]])\n\n#Make a 3D plot\nfig, ax = plt.subplots(subplot_kw=dict(projection='3d'), figsize=(6,9))\nax.plot_surface(X, Y, rv.pdf(pos),cmap='viridis',linewidth=0)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.set_title(r'Probability density, $f_{\\mathbf{X}} (\\mathbf{x})$')\nplt.savefig('fig.png', dpi=150, bbox_inches='tight', transparent=True)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){width=488 height=489}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\nHere\n\n$$\n\\boldsymbol{\\mu}=\\left(\\begin{array}{c}\n-2\\\\\n1\n\\end{array}\\right),\\Sigma=\\left(\\begin{array}{cc}\n3 & 0\\\\\n0 & 6\n\\end{array}\\right)\n$$\n\n*Try adding non-zero entries into the off-diagonal. Note the matrix must be symmetric!*\n:::\n:::\n\n### Code \n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal\nfrom mpl_toolkits.mplot3d import Axes3D\nimport seaborn as sns\nsns.set(font_scale=1.0)\nsns.set_style(\"white\")\nsns.set_style(\"ticks\")\npalette = sns.color_palette('deep')\n\n#Parameters to set\nmu_x = -2\nvariance_x = 3\n\nmu_y = 1\nvariance_y = 6\n\n#Create grid and multivariate normal\nx = np.linspace(-10,10,500)\ny = np.linspace(-10,10,500)\nX, Y = np.meshgrid(x,y)\npos = np.empty(X.shape + (2,))\npos[:, :, 0] = X; pos[:, :, 1] = Y\nrv = multivariate_normal([mu_x, mu_y], [[variance_x, 0], [0, variance_y]])\n\n#Make a 3D plot\nfig, ax = plt.subplots(subplot_kw=dict(projection='3d'), figsize=(7,8))\nax.plot_surface(X, Y, rv.pdf(pos),cmap='viridis',linewidth=0)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.set_title(r'Probability density, $f_{\\mathbf{X}} (\\mathbf{x})$')\nplt.close()\n```\n:::\n\n\n:::\n\n## Multivariate Gaussians {background-color=\"white\"} \n\n- Remember that $\\mathbf{X}$ is a random vector and its possible values $\\mathbf{x}$ are also vectors. \n- The density, $f_{\\mathbf{X}} \\left( \\mathbf{x} \\right)$, is a scalar-valued function. \n- The coefficient \n$$\n\\frac{1}{\\left( 2 \\pi \\right)^{n/2}} \\left|\\boldsymbol{\\Sigma} \\right|^{-\\frac{1}{2}}\n$$\nacts as a normalizing constant.\n\n## Covariance matrix {background-color=\"white\"} \n\n- Elements of the covariance matrix have the following form:\n\n$$\n\\left[ \\boldsymbol{\\Sigma} \\right]_{ij} = \\mathbb{E} \\left[ \\left( X_i - \\mu_{i} \\right) \\left( X_j - \\mu_{j} \\right) \\right] = \\mathbb{E} \\left[ X_i X_j \\right] - \\mu_{i}\\mu_{j}. \n$$ {#eq-cov}\n\n- Following @eq-cov it is clear that the diagonal elements are simply the individual variances:\n\n$$\n\\left[ \\boldsymbol{\\Sigma} \\right]_{ii}  = \\mathbb{E} \\left[ X^2_i \\right] - \\mu^2_{i} = Var \\left(X_i \\right). \n$$ \n\n- The matrix is symmetric with off-diagonal terms being zero when two components $X_i$ and $X_j$ are independent, i.e., $\\left[ \\boldsymbol{\\Sigma} \\right]_{ij}  = \\mathbb{E} \\left[ X_i \\right] \\mathbb{E} \\left[ X_j \\right]  - \\mu_{i} \\mu_{j} = 0$. \n\n\n## Covariance matrix {background-color=\"white\"} \n\n- When the off-diagonal elements are not zero, i.e., when two components $X_i$ and $X_j$ are *related*, we can introduce a measure called the **correlation coefficient**\n\n$$\n\\rho_{ij} = \\frac{\\left[ \\boldsymbol{\\Sigma} \\right]_{ij} }{\\left( Var\\left(X_i \\right)Var\\left(X_j \\right) \\right)^{1/2} }.\n$$\n\n- This values satisfies $-1 \\leq \\rho_{ij} \\leq 1$, and depending upon the sign it is said to be either negatively correlated or positively correlated.\n\n- When $\\rho_{ij}=0$, i.e., when there is *no* correlation, $\\left[ \\boldsymbol{\\Sigma} \\right]_{ij}= 0$. \n\n## Marginal distribution {background-color=\"white\"} \n\n- It can be shown that the marginal density of any component $\\left(X_1, \\ldots, X_n \\right)$ of a multivariate Gaussian is a univariate Gaussian. \n\n- To see this, consider that\n\n$$\nf_{X_k}\\left( x \\right) = \\int_{-\\infty}^{\\infty} \\ldots \\int_{-\\infty}^{\\infty} f_{\\mathbf{X}} \\left(\\mathbf{x} \\right) dx_1 dx_2 \\ldots dx_{k-1} dx_{k+1} \\ldots dx_{n} \n$$\n\n$$\n= \\frac{1}{\\sqrt{2 \\pi \\left[ \\boldsymbol{\\Sigma} \\right]_{kk} } } exp \\left( \\frac{\\left( x - \\mu_{k} \\right)^2}{2 \\left[ \\boldsymbol{\\Sigma} \\right]_{kk} } \\right)\n$$\n\n- In practice any partial marginalization of a multivariate Gaussian will yield another multivariate Gaussian (but with reduced dimensions).\n\n## Marginal and conditional distribution {background-color=\"white\"} \n\n- Let $\\mathbf{X}$ and $\\mathbf{Y}$ be jointly Gaussian random vectors with marginals\n$$\n\\mathbf{X} \\sim \\mathcal{N}\\left(\\boldsymbol{\\mu}_{x}, \\mathbf{A} \\right), \\; \\; \\; \\text{and} \\; \\; \\; \\mathbf{Y} \\sim \\mathcal{N}\\left(\\boldsymbol{\\mu}_{y}, \\mathbf{B} \\right).\n$$\n\n- We can write the joint distribution as shown below\n\n$$\n\\left[\\begin{array}{c}\n\\mathbf{X}\\\\\n\\mathbf{Y}\n\\end{array}\\right]\\sim\\mathcal{N}\\left( \\underbrace{\\left[\\begin{array}{c}\n\\boldsymbol{\\mu}_{x}\\\\\n\\boldsymbol{\\mu}_{y}\n\\end{array}\\right]}_{\\boldsymbol{\\mu}}, \\underbrace{\\left[\\begin{array}{cc}\n\\mathbf{A} & \\mathbf{C}\\\\\n\\mathbf{C}^{T} & \\mathbf{B}\n\\end{array}\\right]}_{\\boldsymbol{\\Sigma}}\\right)\n$$\n\n- The *conditional distribution* of $\\mathbf{X}$ given $\\mathbf{Y}$ is \n\n$$\nf_{\\mathbf{X} | \\mathbf{Y}} \\left( \\mathbf{x}, \\mathbf{y} \\right) = \\mathcal{N} \\left( \\boldsymbol{\\mu}_{x} + \\mathbf{CB}^{-1} \\left(\\mathbf{y} - \\boldsymbol{\\mu}_{y} \\right), \\mathbf{A} - \\mathbf{CB}^{-1} \\mathbf{C}^{T} \\right)\n$$\n\n- Algebraically, this uses the [Schur complement](https://en.wikipedia.org/wiki/Schur_complement). To explore this further, consider the following schematic.\n\n## Marginal and conditional distribution {background-color=\"white\"} \n\n::: panel-tabset\n### Plot\n::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal, norm\nimport pandas as pd\nimport seaborn as sns\nsns.set(font_scale=1.0)\nsns.set_style(\"white\")\nsns.set_style(\"ticks\")\npalette = sns.color_palette('deep')\n\nvar_1 = 1.5\nvar_2 = 3.0\nrho = -0.6\noff_diag = np.sqrt(var_1 * var_2) * rho\n\n\nmu = np.array([0.5, 0.2])\ncov = np.array([[var_1, off_diag], \\\n       [off_diag, var_2]])\n\nrv = multivariate_normal(mu, cov)\n\n# Generate random samples from this multivariate normal (largely for plotting!)\ndata = rv.rvs(8500)\ndf = pd.DataFrame({'$x$': data[:,0].flatten(), '$y$': data[:,1].flatten()})\n\n# Now, to plot the conditional distribution of $X_1$ at $X_2=5.0$, we would have\ndef calculate_conditional(mu, cov, yy):\n    new_mu = mu[0] + cov[0,1] * (cov[1,1])**(-1) * (yy - mu[1])\n    new_var =  cov[0,0] - cov[0,1] * (cov[1,1])**(-1) * cov[0,1]\n    return new_mu, new_var\n\ny_new = 3.7\ncond_mu, cond_var = calculate_conditional(mu, cov, y_new)\n\n# Now, to plot the conditional distribution of $X_1$ at $X_2=5.0$, we would have\ndef calculate_conditional(mu, cov, yy):\n    new_mu = mu[0] + cov[0,1] * (cov[1,1])**(-1) * (yy - mu[1])\n    new_var =  cov[0,0] - cov[0,1] * (cov[1,1])**(-1) * cov[0,1]\n    return new_mu, new_var\n\ny_new = 3.7\ncond_mu, cond_var = calculate_conditional(mu, cov, y_new)\n\nX_samples = np.tile( np.linspace(-10, 10, 200).reshape(200,1) , (1, 2))\nX_samples[:,1] = X_samples[:,1]* 0 + y_new\n\nf_X = rv.pdf(X_samples)\nrv2 = multivariate_normal(cond_mu, cond_var)\nf_X1 = rv2.pdf(X_samples[:,0])\n\n# Plot!\ng = sns.JointGrid(data=df, x=\"$x$\", y=\"$y$\", space=0)\ng.plot_joint(sns.kdeplot, fill=True,  cmap=\"turbo\", thresh=0, levels=100)\ng.plot_marginals(sns.kdeplot, color=\"grey\", gridsize=100)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=556 height=560}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\nThe joint multivariate Gaussian distribution to the left has mean and covariance:\n\n$$\n\\boldsymbol{\\mu}=\\left(\\begin{array}{c}\n0.5\\\\\n0.2\n\\end{array}\\right),\\Sigma=\\left(\\begin{array}{cc}\n1.5 & -1.27\\\\\n-1.27 & 3\n\\end{array}\\right)\n$$\n\nAs an example, we wish to work out what $f_{X| Y} \\left( x, y=3.7 \\right)$ is (see code). \n\n*The conditional is Gaussian!*\n\n![](images/fig_1.png)\n\n:::\n:::\n\n### Code \n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal, norm\nimport pandas as pd\nimport seaborn as sns\nsns.set(font_scale=1.0)\nsns.set_style(\"white\")\nsns.set_style(\"ticks\")\npalette = sns.color_palette('deep')\n\nvar_1 = 1.5\nvar_2 = 3.0\nrho = -0.6\noff_diag = np.sqrt(var_1 * var_2) * rho\n\n\nmu = np.array([0.5, 0.2])\ncov = np.array([[var_1, off_diag], \\\n       [off_diag, var_2]])\n\nrv = multivariate_normal(mu, cov)\n\n# Generate random samples from this multivariate normal (largely for plotting!)\ndata = rv.rvs(8500)\ndf = pd.DataFrame({'$x$': data[:,0].flatten(), '$y$': data[:,1].flatten()})\n\n# Now, to plot the conditional distribution of $X_1$ at $X_2=5.0$, we would have\ndef calculate_conditional(mu, cov, yy):\n    new_mu = mu[0] + cov[0,1] * (cov[1,1])**(-1) * (yy - mu[1])\n    new_var =  cov[0,0] - cov[0,1] * (cov[1,1])**(-1) * cov[0,1]\n    return new_mu, new_var\n\ny_new = 3.7\ncond_mu, cond_var = calculate_conditional(mu, cov, y_new)\n\n# Now, to plot the conditional distribution of $X_1$ at $X_2=5.0$, we would have\ndef calculate_conditional(mu, cov, yy):\n    new_mu = mu[0] + cov[0,1] * (cov[1,1])**(-1) * (yy - mu[1])\n    new_var =  cov[0,0] - cov[0,1] * (cov[1,1])**(-1) * cov[0,1]\n    return new_mu, new_var\n\ny_new = 3.7\ncond_mu, cond_var = calculate_conditional(mu, cov, y_new)\n\nX_samples = np.tile( np.linspace(-10, 10, 200).reshape(200,1) , (1, 2))\nX_samples[:,1] = X_samples[:,1]* 0 + y_new\n\nf_X = rv.pdf(X_samples)\nrv2 = multivariate_normal(cond_mu, cond_var)\nf_X1 = rv2.pdf(X_samples[:,0])\n\n# Plot!\ng = sns.JointGrid(data=df, x=\"$x$\", y=\"$y$\", space=0)\ng.plot_joint(sns.kdeplot, fill=True,  cmap=\"turbo\", thresh=0, levels=100)\ng.plot_marginals(sns.kdeplot, color=\"grey\", gridsize=100)\nplt.close()\n\nfig = plt.figure(figsize=(8,3))\nplt.plot(X_samples[:,0], f_X1, 'r-')\nplt.xlabel('$x$')\nplt.title('Conditional distribution of $x$ at $y=3.7$')\nplt.close()\n```\n:::\n\n\n:::\n\n\n## Generating samples {background-color=\"white\"} \n\nIt will be useful to generate samples from a multivariate Gaussian. To understand how to do this, consider the following setup.\n\n- Let $\\mathbf{X} \\sim \\mathcal{N} \\left(\\mathbf{0}, \\mathbf{I}\\right)$. Thus, $\\mathbb{E} \\left[ \\mathbf{X} \\right] = \\mathbf{0}$, and $Cov\\left[ \\mathbf{X} \\right] = \\mathbf{I}$. \n\n- Now consider the map given by $\\tilde{\\mathbf{x}} = \\mathbf{S} \\mathbf{x} + \\mathbf{b}$, where $\\mathbf{x}$ is a particular value from the random variable $\\mathbf{X}$, where\n  - $\\mathbf{S} \\in \\mathbb{R}^{n \\times n}$ is a matrix;\n  - $\\mathbf{b} \\in \\mathbb{R}^{n}$ is a vector.\n\n- By linearity of the expectation we can show that \n\n$$\n\\mathbb{E} \\left[ \\tilde{\\mathbf{X}} \\right] = \\mathbf{b}, \\; \\; \\; \\text{and} \\; \\; \\; Cov   \\left[ \\tilde{\\mathbf{X}} \\right] = \\mathbf{SS}^{T}. \n$$\n\n- The distribution $\\mathcal{N}\\left( \\mathbf{b}, \\mathbf{SS}^{T} \\right)$ is valid (*i.e., it is Gaussian*), only if $\\mathbf{S}$ is non-singular, i.e., $\\mathbf{SS}^{T}$ is positive definite. \n\n- In practice, if we need to generate samples from $\\mathcal{N}\\left( \\mathbf{b}, \\mathbf{B} \\right)$, we would compute the [Cholesky decomposition](https://en.wikipedia.org/wiki/Cholesky_decomposition) of $\\mathbf{B}= \\mathbf{LL}^{T}$, and then use $\\tilde{\\mathbf{x}} = \\mathbf{b} + \\mathbf{L} \\mathbf{x}$. \n\n",
    "supporting": [
      "index_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}