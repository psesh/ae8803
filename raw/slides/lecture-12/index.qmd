---
title: "Lecture 12"
subtitle: "Hyperparameter inference"
format:
  revealjs: 
    html-math-method: katex
    slide-number: true
    chalkboard: 
      buttons: true
    theme: [simple, style.scss]
    background-color: "white"
    preview-links: auto
    footer: L12 | Hyperparameter inference
    width: 1200
    height: 700
resources:
  - demo.pdf
---

# Hyperparameters

Much of the content today will be based on Chapter 5 in RW. Please read sections 5.1 to 5.4 (inclusive). 

# Hyperparameters

- Almost all covariance functions have *unknown* hyperparameters. In many cases, the selection and subsequent optimal set of values of hyperparameters can provide useful insight about the function. 
- Consider the squared exponential kernel
$$
k \left( \mathbf{x}, \mathbf{x}' \right) = \sigma_{f}^2 exp \left( -\frac{1}{2l^2} \left\Vert  \mathbf{x} -  \mathbf{x}' \right\Vert _{2}^{2} \right)
$$
that has two hyperparameters, $\sigma_{f}$ and $l$. 

- From a Bayesian perspective, we would like to assign priors to each hyperparameter, and then compute their posteriors. 


# Hyperparameters

- Recall from before, the **marginal likelihood** for a Gaussian process (assuming a Gaussian likelihood and Gaussian model prior) is given by 
$$
p \left( \mathbf{t} | \sigma^2 \right) = \int p \left( \mathbf{t} | \mathbf{f}, \sigma^2 \right) p \left( \mathbf{f} \right) d \mathbf{f} = \mathcal{N} \left(\mathbf{0}, \mathbf{C} + \sigma^2 \mathbf{I}_{N} \right) 
$$

- Now, let $\boldsymbol{\theta} = \left[ \sigma_f, l, \ldots \right]$. Then, we want to work out
$$
p \left( \boldsymbol{\theta} | \mathbf{t} \right) = \frac{p \left( \mathbf{t} | \boldsymbol{\theta} \right)  p \left( \boldsymbol{\theta} \right) }{p \left( \mathbf{t} \right)}
$$

- However, in this case the marginal likelihood is usually intractable! 
$$
p \left( \mathbf{t} \right) = \int p \left( \mathbf{t} | \boldsymbol{\theta} \right)  p \left( \boldsymbol{\theta} \right) d \boldsymbol{\theta}
$$

# MAP

- The workaround is to introduce an approximation. We will use the MAP (maximum a posteriori) estimate. Note that $p\left( \mathbf{t} \right)$ is *constant* with respect to $\boldsymbol{\theta}$. 
$$
p \left( \boldsymbol{\theta} | \mathbf{t} \right) = \frac{p \left( \mathbf{t} | \boldsymbol{\theta} \right)  p \left( \boldsymbol{\theta} \right) }{p \left( \mathbf{t} \right)}
\propto p \left( \mathbf{t} | \boldsymbol{\theta} \right)  p \left( \boldsymbol{\theta} \right) 
$$

- The MAP estimate is given by
$$
\hat{\boldsymbol{\theta}}_{MAP} = \underset{\boldsymbol{\theta}}{argmax} \; log \;  p \left( \boldsymbol{\theta} | \mathbf{t} \right) = \underset{\boldsymbol{\theta}}{argmax} \; log \; p \left( \mathbf{t} | \boldsymbol{\theta} \right) + log \;  p \left( \boldsymbol{\theta} \right)
$$

# MAP vs Maximum likelihood
- Note, if the priors $p \left( \boldsymbol{\theta} \boldsymbol \right)$ are uniform, then 
$$
\begin{aligned}
\hat{\boldsymbol{\theta}}_{MAP} & = \underset{\boldsymbol{\theta}}{argmax} \; log \; p \left( \mathbf{t} | \boldsymbol{\theta} \right) + log \;  p \left( \boldsymbol{\theta} \right) \\
& = \underset{\boldsymbol{\theta}}{argmax} \; log \; p \left( \mathbf{t} | \boldsymbol{\theta} \right) + log \; \textrm{constant} \\
& \propto \underset{\boldsymbol{\theta}}{argmax} \; log \; p \left( \mathbf{t} | \boldsymbol{\theta} \right) \\
& = \hat{\boldsymbol{\theta}}_{ML}
\end{aligned}
$$
in which case we have the marginal likelihood estimate. 

# Marginal likelihood 
- Once again, we can write the marginal likelihood as 
$$
p \left( \mathbf{t} | \sigma^2, \boldsymbol{\theta} \right) = \int p \left( \mathbf{t} | \mathbf{f}, \sigma^2 \right) p \left( \mathbf{f} \right) d \mathbf{f} = \mathcal{N} \left(\mathbf{t} | \mathbf{0}, \mathbf{C}\left( \boldsymbol{\theta} \right) + \sigma^2 \mathbf{I}_{N} \right). 
$$

- From which, we can write 
$$
\begin{aligned}
log \left(p \left( \mathbf{t} | \sigma^2, \boldsymbol{\theta} \right)  \right) & = log \; \mathcal{N} \left(\mathbf{0}, \mathbf{C}\left( \boldsymbol{\theta} \right) + \sigma^2 \mathbf{I}_{N} \right) \\ 
& = log \left[  \left( 2 \pi \right)^{-\frac{d}{2}}  \right]
\end{aligned}
$$