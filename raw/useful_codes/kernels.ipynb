{
 "cells": [
  {
   "cell_type": "raw",
   "id": "2b67c5b5",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Notes on kernels\"\n",
    "format:\n",
    "    html:\n",
    "        code-fold: true\n",
    "jupyter: python3\n",
    "fontsize: 1.2em\n",
    "linestretch: 1.5\n",
    "toc: true\n",
    "notebook-view: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32950bff",
   "metadata": {},
   "source": [
    "## Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f2d8ae",
   "metadata": {},
   "source": [
    "One way to motivate the study of kernels, is to consider a linear regression problem where one has more unknowns than observational data. Let $\\mathbf{X} = \\left[\\mathbf{x}_{1}^{T}, \\mathbf{x}_{2}^{T}, \\ldots, \\mathbf{x}_{N}^{T}\\right]$ be the $N \\times d$ data corresponding to $N$ observations of $d$-dimensional data. These *input* observations are accompanied by an output observational vector, $\\mathbf{y} \\in \\mathbb{R}^{N}$. Let $\\boldsymbol{\\Phi} \\left( \\mathbf{X} \\right) \\in \\mathbb{R}^{N \\times M}$ be a parameterized matrix comprising of $M$ basis functions, i.e., \n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\Phi}\\left( \\mathbf{X} \\right) = \\left[\\begin{array}{cccc}\n",
    "\\phi_{1}\\left(\\mathbf{X} \\right), & \\phi_{2}\\left(\\mathbf{X} \\right), & \\ldots, & \\phi_{M}\\left(\\mathbf{X} \\right)\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "If we are interested in approximating $f \\left( \\mathbf{X} \\right) \\approx \\hat{f} \\left( \\mathbf{X} \\right) = y = \\mathbf{\\Phi} \\left( \\mathbf{X} \\right) \\boldsymbol{\\alpha}$, we can determine the unknown coefficients via least squares. This leads to the solution via the normal equations\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\alpha} = \\left( \\mathbf{\\Phi}^{T} \\mathbf{\\Phi}\\right)^{-1} \\boldsymbol{\\Phi}^{T} \\mathbf{y}\n",
    "$$\n",
    "\n",
    "Now, strictly speaking, one cannot use the normal equations to solve a problem where there are more unknowns than observations because $\\left( \\mathbf{\\Phi}^{T} \\mathbf{\\Phi}\\right)$ is not full rank. Recognizing that in such a situation, there may likely be numerous solutions to $\\boldsymbol{\\Phi}\\left( \\mathbf{X} \\right) \\boldsymbol{\\alpha} = \\mathbf{y}$, we want the solution with the lowest $L_2$ norm. This can be more conveniently formulated as as *minimum norm problem*, written as\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\underset{x}{minimize} & \\; \\boldsymbol{\\alpha}^{T} \\boldsymbol{\\alpha} \\\\ \n",
    "\\textsf{subject to} \\; \\; &  \\boldsymbol{\\Phi}\\left( \\mathbf{X} \\right) \\boldsymbol{\\alpha} = \\mathbf{y}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The easiest way to solve this via the method of Lagrange multipliers, i.e., we define the objective function \n",
    "\n",
    "$$\n",
    "L \\left( \\boldsymbol{\\alpha}, \\lambda \\right) = \\boldsymbol{\\alpha}^{T} \\boldsymbol{\\alpha}  + \\lambda^{T} \\left( \\boldsymbol{\\Phi}\\left( \\mathbf{X} \\right) \\boldsymbol{\\alpha} - \\mathbf{y}\\right), \n",
    "$$\n",
    "\n",
    "where $\\lambda$ comprises the Lagrange multipliers. The optimality conditions for this objective are given by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_{\\boldsymbol{\\alpha}} L & = 2 \\boldsymbol{\\alpha} + \\boldsymbol{\\Phi}^{T} \\lambda = 0, \\\\\n",
    "\\nabla_{\\lambda} L & = \\boldsymbol{\\Phi} \\boldsymbol{\\alpha} - \\mathbf{y} = 0. \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This leads to $\\boldsymbol{\\alpha} = - \\boldsymbol{\\Phi}^{T} \\lambda / 2$. Substituting this into the second expression above yields $\\lambda = -2 \\left(\\boldsymbol{\\Phi} \\boldsymbol{\\Phi}^{T} \\right)^{-1} \\mathbf{y}$. This leads to the minimum norm solution\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\alpha} = \\boldsymbol{\\Phi}^{T}  \\left( \\boldsymbol{\\Phi} \\boldsymbol{\\Phi}^{T}  \\right)^{-1} \\mathbf{y}.\n",
    "$$\n",
    "\n",
    "Note that unlike $\\left( \\boldsymbol{\\Phi}^{T} \\boldsymbol{\\Phi} \\right)$, $\\left( \\boldsymbol{\\Phi} \\boldsymbol{\\Phi}^{T}  \\right)$ does have full rank. The latter is an inner product between feature vectors. To see this, define the two-point kernel function\n",
    "\n",
    "$$\n",
    "k \\left( \\mathbf{x}, \\mathbf{x}' \\right) = \\boldsymbol{\\Phi} \\left( \\mathbf{x} \\right) \\boldsymbol{\\Phi}^{T} \\left( \\mathbf{x} \\right). \n",
    "$$\n",
    "\n",
    "and the associated covariance matrix, defined elementwise via\n",
    "\n",
    "$$\n",
    "\\left[ \\mathbf{K} \\left(\\mathbf{X}, \\mathbf{X}' \\right)\\right]_{ij} = k \\left( \\mathbf{x}_{i}, \\mathbf{x}_{j} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed96de3",
   "metadata": {},
   "source": [
    "### The kernel trick (feature maps $\\rightarrow$ kernels)\n",
    "\n",
    "From the coefficients $\\boldsymbol{\\alpha}$ computed via the minimum norm solution, it should be clear that approximate values of the true function at new locations $\\mathbf{X}_{\\ast}$ can be given via\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{f} \\left( \\mathbf{X}_{\\ast} \\right) & = \\Phi \\left( \\mathbf{X}_{\\ast} \\right) \\boldsymbol{\\alpha} \\\\\n",
    "& = \\boldsymbol{\\Phi} \\left( \\mathbf{X}_{\\ast} \\right)  \\boldsymbol{\\Phi}^{T} \\left( \\mathbf{X} \\right)  \\left( \\boldsymbol{\\Phi} \\left( \\mathbf{X} \\right)  \\boldsymbol{\\Phi}^{T} \\left( \\mathbf{X} \\right)   \\right)^{-1} \\mathbf{y} \\\\\n",
    "& = \\left( \\boldsymbol{\\Phi} \\left( \\mathbf{X}_{\\ast} \\right)  \\boldsymbol{\\Phi}^{T} \\left( \\mathbf{X} \\right)  \\right)  \\left( \\boldsymbol{\\Phi} \\left( \\mathbf{X} \\right)  \\boldsymbol{\\Phi} \\left( \\mathbf{X} \\right) ^{T}  \\right)^{-1} \\mathbf{y} \\\\\n",
    "& = \\mathbf{K} \\left( \\mathbf{X}_{\\ast}, \\mathbf{X} \\right) \\mathbf{K}^{-1} \\left( \\mathbf{X}, \\mathbf{X} \\right) \\mathbf{y} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "There are two points to note here:\n",
    "\n",
    "- The form of the expression above is exactly that of the posterior predictive mean of a noise-free Gaussian processes model.\n",
    "- One need not compute the full $N \\times M$ feature matrix $\\boldsymbol{\\Phi} \\left( \\mathbf{X} \\right)$ explictly to work out the $N \\times N$ matrix $\\mathbf{K}\\left( \\mathbf{X}, \\mathbf{X} \\right)$. \n",
    "\n",
    "This latter point is why this is called the *kernel trick*, i.e., for a very large number of features $M >> N$, it is more computationally efficient to work out $\\mathbf{K}$. \n",
    "\n",
    "Another way to interpret the kernel trick is to consider the "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a3b5ff",
   "metadata": {},
   "source": [
    "### Mercer's theorem (kernels $\\rightarrow$ feature maps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2db2fca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cfdd5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Method not declared.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import equadratures as eq\n",
    "\n",
    "p = eq.Parameter(lower=-1, upper=1, order=10)\n",
    "b = eq.Basis('univariate')\n",
    "poly = eq.Poly(p, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3ae40ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(8,1)*2 - 1.\n",
    "Phi = poly.get_poly(X).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e77aa20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 11)\n"
     ]
    }
   ],
   "source": [
    "print(Phi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0168bf83",
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Matrix is not positive definite",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPhi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPhi\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mcholesky\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/linalg/linalg.py:756\u001b[0m, in \u001b[0;36mcholesky\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    754\u001b[0m t, result_t \u001b[38;5;241m=\u001b[39m _commonType(a)\n\u001b[1;32m    755\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->D\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->d\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 756\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mgufunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap(r\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/linalg/linalg.py:92\u001b[0m, in \u001b[0;36m_raise_linalgerror_nonposdef\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_linalgerror_nonposdef\u001b[39m(err, flag):\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMatrix is not positive definite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Matrix is not positive definite"
     ]
    }
   ],
   "source": [
    "np.linalg.cholesky(Phi.T @ Phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b7e934",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
