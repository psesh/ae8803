{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa2278e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2f4f271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000012853e-200"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.det(np.eye(200) * 0.1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73c20627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-460.51701859880785"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log( np.linalg.det(np.eye(200) * 0.1 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5255968f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a997894a",
   "metadata": {},
   "source": [
    "\n",
    "## Model Selection for GP Regression\n",
    "\n",
    "Although Gaussian Process is a non-parametric model, the so called 'hyper-parameters' in the kernel heavily influence the inference process. It is therefore essential to select the best possible parameters for the kernel. For example in the R.B.F-kernel; the *length* ($\\large l$) and *scale* ($\\large \\sigma$) are the hyper-parameters.\n",
    "$$\\large k(x,x') = \\sigma^2 \\text{ exp }\\Big(\\frac{|x-x'|^2}{2l^2}\\Big)$$\n",
    "where, $\\large l$ controls the 'reach of influence on neighbors' and $\\large \\sigma$ dictates the average amplitude from the mean of the function.\n",
    "\n",
    "\n",
    "## Marginal Likelihood\n",
    "\n",
    "```ad-note\n",
    "title: Bayes' Rule\n",
    "collapse: open\n",
    "\n",
    "$$\\text{Posterior} = \\frac{\\text{Likelihood} \\times \\  \\text{Prior}}{\\text{Marginal Likelihood}}$$\n",
    "\n",
    "$$\\mathbb{P}(\\theta|y,X) = \\frac{\\mathbb{P}(y|X,\\theta) \\times \\  \\mathbb{P}(\\theta)}{\\mathbb{P}(y|X)}$$\n",
    "```\n",
    "\n",
    "A #Marginal-Likelihood is a likelihood function that has been integrated over the parameter space. It represents the probability of generating the observed sample from a #prior and is often referred to as the #model-evidence or #evidence.\n",
    "\n",
    "Let $\\large \\theta$ represent the parameters of the model. We now formulate a #prior over the output of the function as a #Gaussian-Process.\n",
    "$$\\large \\mathbb{P}(f|X,\\theta) = \\mathcal{N}\\Big(0, k(x,x')\\Big) \\tag{1}$$\n",
    "We can always transform the data to have zero mean and $(1)$ can be viewed as a general case. Assume that the #likelihood takes the following form $$\\large \\mathbb{P}(Y|f) \\sim \\mathcal{N}(f, \\sigma_n^2I) \\tag{2}$$\n",
    "$(2)$ tells that the observations $\\large y$ are subject to additive Gaussian noise. Now, the joint distribution is given by; $$\\large \\mathbb{P}(Y,f|X,\\theta) = \\mathbb{P}(Y|f)\\ \\mathbb{P}(f|X,\\theta) \\tag{3}$$\n",
    "It is worth noting that we would eventually like to optimize the hyper-parameters $\\theta$ for the kernel function. However, the #prior here is over the mapping $\\large f$ and not any parameters directly. In the <b><i>evidence-based</i></b> framework, which approximates Bayesian averaging by optimizing the #Marginal-Likelihood we can make use of the denominator part in the <i><b>Bayes' Rule</i></b> as an objective function for optimization. For this we take the joint distribution $(3)$ and marginalize over $\\large f$ since we are not directly interested in optimizing it. This can be done in the following way: $$\\large \\begin{align}\\mathbb{P}(Y|X,\\theta) &= \\int \\mathbb{P}(Y,f|X,\\theta)\\ df \\\\\n",
    "&= \\int \\mathbb{P}(Y|f)\\ \\mathbb{P}(f|X,\\theta)\\ df\\\\\n",
    "&= \\int \\mathcal{N}(y;f,\\sigma_n^2I)\\ \\ \\mathcal{N}(f;0, K)\n",
    "\\end{align} \\tag{4}$$$(4)$ is an integration performed all possible spaces of $\\large f$ and it aims to remove $\\large f$\n",
    "from the distribution of $\\large Y$. After marginalization $\\large Y$ is no longer dependent on $\\large f$ but it depends on the hyper-parameters $\\large \\theta$.\n",
    "\n",
    "As per <span style=\"color: orange\">Rasmussen & Williams</span>, the log marginal likelihood is given by; $$\\large \\text{log }\\mathbb{P}(y|X,\\theta) = -\\frac{1}{2}y^TK_y^{-1}y - \\frac{1}{2}\\text{log }|K_y| - \\frac{n}{2}\\text{log }(2\\pi) \\tag{5}$$\n",
    "### Derivation\n",
    "\n",
    "Let $\\Sigma = \\sigma_n^2I$. Now, (4) can be fleshed out as follows; \n",
    "$$\\begin{align}\n",
    "    \\mathbb{P}(y|X,\\theta) & = \\int \\frac{1}{(2\\pi)^{n/2}} |\\Sigma|^{-1/2} \\text{exp}\\ (-\\frac{1}{2} (f-y)^T\\Sigma^{-1}(f-y)) \\times \\frac{1}{(2\\pi)^{n/2}} |K|^{-1/2} \\text{exp}\\ (-\\frac{1}{2} (f)^TK^{-1}(f))\\ df \\\\\n",
    "    & = \\frac{1}{(2\\pi)^n}\\frac{1}{\\sqrt{|\\Sigma||K|}} \\int \\text{exp}\\ \\bigg(-\\frac{1}{2}\\big[(f-y)^T\\Sigma^{-1}(f-y) + f^TK^{-1}f\\big]\\bigg)\\ df \\\\\n",
    "\\end{align}\\tag{6}$$\n",
    "Looking at the exponent term in (6):\n",
    "$$\\begin{equation*}\n",
    "    \\begin{split}\n",
    "        &= f^T(\\Sigma^{-1}+K^{-1})f - 2f^T\\Sigma^{-1}y + y^T\\Sigma^{-1}y \\\\\n",
    "        &= f^T\\Pi^{-1}f - 2f^T\\Pi^{-1}\\nu + y^T\\Sigma^{-1}y\\\\\n",
    "        &= (f-\\nu)^T\\Pi^{-1}(f-\\nu) - \\nu^T\\Pi^{-1}\\nu + y^T\\Sigma^{-1}y\n",
    "    \\end{split}\n",
    "\\end{equation*}$$\n",
    "where $\\Pi = (\\Sigma^{-1}+K^{-1})^{-1}$ and $\\nu = \\Pi\\Sigma^{-1}y$.\n",
    "By definition we have;\n",
    "$$\\frac{1}{\\sqrt{2\\pi\\Pi}} \\int \\text{exp}\\bigg[  -\\frac{1}{2} (f-\\nu)^T\\Pi^{-1}(f-\\nu)  \\bigg] \\ df = 1$$\n",
    "Plugging this back to (6) gives the following expression;\n",
    "$$\\frac{\\sqrt{(2\\pi)^n|\\Pi|}}{(2\\pi)^n\\sqrt{|\\Sigma||K|}}\\ \\text{exp}\\ \\bigg[ \\frac{1}{2}(\\nu^T\\Pi^{-1}\\nu - y^T\\Sigma^{-1}y)   \\bigg]$$\n",
    "Substitute values for $\\Pi$ and $\\nu$ we get;\n",
    "$$\\begin{equation*}\n",
    "    \\begin{split}\n",
    "        \\mathbb{P}(y|X,\\theta) &= \\frac{1}{(2\\pi)^{n/2}} \\bigg(\\big|\\Sigma\\big|\\big|K\\big|\\big|\\Sigma^{-1}+K^{-1}\\big|\\bigg)^{-1/2} \\text{exp}\\ \\bigg[-\\frac{1}{2}\\big(y^T\\Sigma^{-1}(\\Sigma^{-1}+K^{-1})^{-1}K^{-1}y\\big) \\bigg] \\\\\n",
    "        &= \\frac{1}{(2\\pi)^{n/2}} \\bigg(\\big|\\Sigma\\big|\\big|K\\big|\\big|\\frac{\\Sigma + K}{\\Sigma K}\\big|\\bigg)^{-1/2} \\text{exp}\\ \\bigg[-\\frac{1}{2}\\big(y^T\\Sigma^{-1}(  \\frac{K + \\Sigma}{\\Sigma K}  )^{-1}K^{-1}y\\big) \\bigg] \\\\\n",
    "        &= \\frac{1}{(2\\pi)^{n/2}} \\bigg(\\big|\\Sigma + K \\big|\\bigg)^{-1/2} \\text{exp}\\ \\bigg[-\\frac{1}{2}\\big(y^T(  K + \\Sigma)^{-1}y\\big) \\bigg] \\\\\n",
    "        &= \\frac{1}{(2\\pi)^{n/2}} \\bigg(\\big|\\sigma_n^2I + K \\big|\\bigg)^{-1/2} \\text{exp}\\ \\bigg[-\\frac{1}{2}\\big(y^T(  K + \\sigma_n^2I)^{-1}y\\big) \\bigg] \\\\\n",
    "        &= \\frac{1}{(2\\pi)^{n/2}} \\big|K_y \\big|^{-1/2} \\text{exp}\\ \\bigg[-\\frac{1}{2}y^TK_y^{-1}y \\bigg] \\\\\n",
    "    \\end{split}\n",
    "\\end{equation*}$$\n",
    "\n",
    "Taking log on both sides yields (5)\n",
    "\n",
    "Note that this expression depends on the hyperparameters $\\theta$ of the kernel function through the kernel matrix $K$, which depends on the input values $x_i$ and the values of the hyperparameters. Therefore, the log marginal likelihood can be used to optimize the hyperparameters of the kernel function using numerical optimization techniques such as gradient descent or L-BFGS.\n",
    "\n",
    "\n",
    "## Gradients of Marginal Likelihood\n",
    "\n",
    "```ad-note\n",
    "collapse: open\n",
    "title: Recall\n",
    "\n",
    "$\\large \\frac{\\partial}{\\partial \\theta}K^{-1} = -K^{-1}\\frac{\\partial K}{\\partial \\theta}K^{-1}$\n",
    "\n",
    "\n",
    "$\\large \\frac{\\partial}{\\partial \\theta}\\text{log }|K| = \\text{trace }\\big(K^{-1}\\frac{\\partial K}{\\partial \\theta}\\big)$\n",
    "```\n",
    "\n",
    "Now, the partial derivatives w.r.t. the hyper-parameters is given by; $$\\large\\begin{align}\n",
    "\\frac{\\partial}{\\partial \\theta_j}\\text{log }\\mathbb{P}(y|X,\\theta) &= \\frac{1}{2}y^TK^{-1}\\frac{\\partial K}{\\partial \\theta_j}K^{-1}y - \\frac{1}{2}\\text{trace }\\big(K^{-1}\\frac{\\partial K}{\\partial \\theta_j} \\big)\\\\\n",
    "&= \\frac{1}{2}\\text{trace }\\bigg((\\alpha\\alpha^T - K^{-1})\\frac{\\partial K}{\\partial \\theta_j} \\bigg) \\tag{6}\n",
    "\\end{align}$$\n",
    "where, $\\large \\alpha = K^{-1}y$\n",
    "\n",
    "```ad-warning\n",
    "collapse: open\n",
    "\n",
    "In general, computing the inverse of a matrix directly (e.g: np.linalg.inv()) is not stable and there is a loss of precision. In the case when the matrix is positive definite, Cholesky decompostion can be used to compute inverse.\n",
    "\n",
    "Example:<br>\n",
    "\n",
    "Let $K$ be a symmetric positive definite matrix. Now, if we want to calculate $\\alpha = K^{-1}y$, we can do the following:\n",
    "\n",
    "$$K = \\text{Cholesky } \\rightarrow LL^T$$\n",
    "\n",
    "\n",
    "$$K^{-1} = (L^{T})^{-1}L^{-1}$$\n",
    "\n",
    "\n",
    "$$\\alpha = \\text{np.linalg.solve(L.T, np.linalg.solve(L, y))}$$\n",
    "```\n",
    "\n",
    "<hr>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b886d41a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1907959429.py, line 85)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 85\u001b[0;36m\u001b[0m\n\u001b[0;31m    ```\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def kernel(x, xp, σ, l):\n",
    "\t'''k(x,x') = sigma^2 exp(-0.5*length^2*|x-x'|^2)'''\n",
    "\tlength = l\n",
    "\tsq_norm = (scipy.spatial.distance.cdist(x, xp))**2\n",
    "\treturn σ**2 * np.exp(-0.5*sq_norm/(length**2))\n",
    "\n",
    "\n",
    "def dKdL(x1, x2, σ, l):\n",
    "\t'''\n",
    "\tcomputes partial derivative of K w.r.t length (l)\n",
    "\targ: x1 = (N1, D), x2 = (N2, D)\n",
    "\treturn: (N1, N2)\n",
    "\t'''\n",
    "\tsq_norm = (scipy.spatial.distance.cdist(x1, x2))**2\n",
    "\treturn (σ**2) * np.exp(-sq_norm/(2*l**2)) * (sq_norm) / (l**3)\n",
    "\n",
    "def dKdσ(x1, x2, σ, l):\n",
    "\t'''\n",
    "\tcomputes partal derivatice of K w.r.t sigma (std not variance)\n",
    "\targ: x1 = (N1, D), x2 = (N2, D)\n",
    "\treturn: (N1, N2)\n",
    "\t'''\n",
    "\tsq_norm = (scipy.spatial.distance.cdist(x1, x2))**2\n",
    "\treturn 2*σ*np.exp(-sq_norm/(2*l**2))\n",
    "\n",
    "\n",
    "def dLdt(a, iKxx, dKdt):\n",
    "\t'''\n",
    "\tcomputes gradient of log marginal likelihood w.r.t. a hyper-parameter\n",
    "\ti.e. either sigma or length\n",
    "\t'''\n",
    "\treturn 0.5**np.trace(np.dot(a @ a.T - iKxx), dKdt)\n",
    "\n",
    "\n",
    "def f_opt(kernel, X, y, σ, l):\n",
    "\t'''\n",
    "\tEvalaute Negative-Log Marginal Likelihood\n",
    "\t'''\n",
    "\tσ_n = 0.1 # std of noise hard-coded for now\n",
    "\tK = kernel(X,X, σ = σ, l = l) + (σ_n**2)*np.eye(X.shape[0])\n",
    "\tL = np.linalg.cholesky(K) + 1e-12 # Cholesky decomposition\n",
    "\ta = np.linalg.solve(L.T, np.linalg.solve(L, y)) # compute alpha\n",
    "\t\n",
    "\t#log_likelihood = -0.5 * y.T @ a - 0.5 * np.trace(np.log(L)) - 0.5 * X.shape[0] * np.log(2*np.pi)\n",
    "\tlog_likelihood = -0.5 * y.T @ a - 0.5 * np.log(np.linalg.det(K)) - 0.5 * X.shape[0] * np.log(2*np.pi)\n",
    "\t\n",
    "\treturn -log_likelihood\n",
    "\n",
    "\n",
    "def grad_f(kernel, X, y, l, σ):\n",
    "\t'''\n",
    "\tCompute gradient of objective function w.r.t. two parameters\n",
    "\t'''\n",
    "\tl, σ = params\n",
    "\tσ_n = 0.1 # std of noise hard-coded for now\n",
    "\tK = kernel(X,X, σ = σ, l = l) + (σ_n**2)*np.eye(X.shape[0])\n",
    "\tL = np.linalg.cholesky(K) # Cholesky decomposition\n",
    "\ta = np.linalg.solve(L.T, np.linalg.solve(L, y)) # compute alpha\n",
    "\t\n",
    "\tinv_k = np.linalg.inv(K)\n",
    "\tgrad = np.empty([2,])\n",
    "\tgrad[0] = dLdt(a = a, iKxx = inv_k, dKdt = dKdσ(X, X, σ, l)) # gradient w.r.t sigma\n",
    "\tgrad[1] = dLdt(a = a, iKxx = inv_k, dKdt = dKdL(X, X, σ, l)) # gradient w.r.t length\n",
    "\t\n",
    "\treturn grad\n",
    "\n",
    "  \n",
    "\n",
    "def marginal(params, X, y):\n",
    "\t'''\n",
    "\tEvalaute Negative-Log Marginal Likelihood -- for scipy optimization\n",
    "\t'''\n",
    "\t#print (params)\n",
    "\tl, σ = params\n",
    "\tσ_n = 0.1 # std of noise hard-coded for now\n",
    "\tK = kernel(X, X, σ = σ, l = l) + (σ_n**2)*np.eye(X.shape[0])\n",
    "\tL = np.linalg.cholesky(K) + 1e-12 # Cholesky decomposition\n",
    "\ta = np.linalg.solve(L.T, np.linalg.solve(L, y)) # compute alpha\n",
    "\t\n",
    "\t#log_likelihood = -0.5 * y.T @ a - 0.5 * np.trace(np.log(L)) - 0.5 * X.shape[0] * np.log(2*np.pi)\n",
    "\t\n",
    "\tlog_likelihood = -0.5 * y.T @ a - 0.5 * np.log(np.linalg.det(K)) - 0.5 * X.shape[0] * np.log(2*np.pi)\n",
    "\n",
    "\treturn -log_likelihood\n",
    "```\n",
    "\n",
    "```python\n",
    "'''\n",
    "True function f(x) = sin(x) & X ~ Unif(-4,4) with 10 samples\n",
    "'''\n",
    "f_sin = lambda x: (np.sin(x)).flatten()\n",
    "X = np.random.uniform(-4, 4, size = (10, 1))\n",
    "y = f_sin(X)\n",
    "\n",
    "# Scipy-optimization via SLSQP\n",
    "\n",
    "lim = [10**-3, 10**3]\n",
    "bound = [lim, lim]\n",
    "start = [0.3, 0.1] # initial hyper-parameters\n",
    "result = scipy.optimize.minimize(fun = marginal, x0 = start, args = (X, y), method = 'SLSQP', options = {'disp':True}, bounds = bound, tol = 0.0001)\n",
    "```\n",
    "\n",
    "```python\n",
    "'''\n",
    "Contour Plot\n",
    "'''\n",
    "\n",
    "L = np.linspace(10**-3, 10**2, 1000)\n",
    "S = np.linspace(10**-3, 10**3, 1000)\n",
    "σ, l = np.meshgrid(L, S)\n",
    "\n",
    "func_val = np.zeros_like(σ)\n",
    "\n",
    "for i in range(σ.shape[0]):\n",
    "\tfor j in range(l.shape[0]):\n",
    "\t\tfunc_val[i, j] = f_opt(kernel = kernel, X = X.reshape(-1,1), y = y.reshape(-1,1), σ = S[i], l = L[j])\n",
    "  \n",
    "plt.contourf(σ, l, func_val, cmap = 'plasma')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.scatter(result.x[0], result.x[1], color = 'black', marker = 'x')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80a265a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
