---
title: "Overview"
format:
    html:
        code-fold: true
jupyter: python3
fontsize: 1.2em
linestretch: 1.5
toc: true
notebook-view: true
---

### Course Description:

This graduate-level course offers a practical approach to probabilistic learning with Gaussian processes (GPs). GPs represent a powerful set of methods for modeling and predicting a wide variety of spatio-temporal phenomena. Today, they are used for problems that span both regression and classification, with theoretical foundations in Bayesian inference, reproducing kernel Hilbert spaces, eigenvalue problems, and numerical integration. Rather than focus *solely* on these theoretical foundations, this course balances theory with practical probabilistic programming, using a variety of ``python``-based packages. Moreover, practical engineering problems will also be discussed that see GP models that cut across other areas of machine learning including transfer learning, convolutional networks, and normalizing flows. 


## Grading

This course has four assignments; the grades are given below:


| Assignment  | Grade percentage (%)    |
| --------  | ------- |
| Assignment 1: <a href="https://psesh.github.io/ae8803/midterm/midterm_2024_solutions.html">Take-home mid-term (covering fundamentals) </a>    | 20 |
| Assignment 2: <a href="https://psesh.github.io/ae8803/coding-1/coding-1.html">Build your own GP from scratch for a given dataset </a> | 20 |
| Assignment 3: <a href="https://psesh.github.io/ae8803/project/proposals_and_projects.html">Proposal</a>   | 20    |
| Assignment 4: Final project (presentation and notebook)  | 40    |


### Pre-requisites:

- CS1371, MATH2551, MATH2552 (or equivalent)
- Working knowledge of ``python`` including familiarity with ``numpy`` and ``matplotlib`` libraries. 
- Working local version of ``python`` and ``Jupyter``. 

## Lectures

Below you will find a list of the lectures that form the backbone of this course. Sub-topics for each lecture will be updated in due course. 

01.08: **L1. Introduction & probability fundamentals** | <a href="https://gtvault-my.sharepoint.com/:b:/g/personal/pseshadri34_gatech_edu/Edq6QWhxcXJDu1KONfch_30B3ELimiqkhzWTuYNZbLOuLg?e=Nfc5ZN" target="_blank" style="text-decoration: none">Slides</a> | <a href="sample_problems/lecture_1.html" style="text-decoration: none">Examples</a>  
<details>
  <summary>Contents</summary>

  1. Course overview.
  2. Probability fundamentals (and Bayes' theorem).
  3. Random variables.
</details>

01.10: **L2. Discrete probability distributions** | <a href="https://gtvault-my.sharepoint.com/:b:/g/personal/pseshadri34_gatech_edu/Ef-ZWBHcAFJMhceuoj68lS8B34zuF6xV11Vg1HnoEXQIQA?e=r5ojOP" target="_blank" style="text-decoration: none">Slides</a> | <a href="sample_problems/lecture_2.html" style="text-decoration: none">Examples</a>  | <a href="useful_codes/discrete.html" style="text-decoration: none">Notebook</a> 
<details>
  <summary>Contents</summary>

  1. Expectation and variance.
  2. Independence.
  3. Bernoulli and Binomial distributions. 

</details>

01.15: *No Class (Institute Holiday)*

01.17: **L3. Continuous distributions** | <a href="https://gtvault-my.sharepoint.com/:b:/g/personal/pseshadri34_gatech_edu/ESS4uUotKydPv19X7I-S-nwBSAb8eHrX_OprBEx9PTuIEQ?e=8JOnXp" target="_blank" style="text-decoration: none">Slides</a> | <a href="sample_problems/lecture_3.html" style="text-decoration: none">Examples</a>  
<details>
  <summary>Contents</summary>

  1. Fundamentals of continuous random variables.
  2. Probability density function.
  3. Gaussian and Beta distributions. 
</details>

01.22: **L4. Manipulating and combining distributions** | <a href="https://gtvault-my.sharepoint.com/:b:/g/personal/pseshadri34_gatech_edu/Ea3zGq-pugFPpzH9havolU8BVqh33PKmgpnzfmBdX3ayZw?e=Xn0ONO" target="_blank" style="text-decoration: none">Slides</a> | <a href="sample_problems/lecture_4.html" style="text-decoration: none">Examples</a> 
<details>
  <summary>Contents</summary>

  1. Functions of random variables. 
  2. Sums of random variables.
</details>

01.24: *No Class*

01.29: **L5. Multivariate Gaussian distributions** | <a href="slides/lecture-5/index.html"  style="text-decoration: none">Slides</a>
<details>
  <summary>Contents</summary>

  1. Marginal distributions.
  2. Conditional distributions.
  3. Joint distribution and Schur complement. 
</details>

01.31: **L6. Linear modelling** | <a href="slides/lecture-6/index.html"  style="text-decoration: none">Slides</a>
<details>
  <summary>Contents</summary>

  1. Least squares.
  2. Regularization.
  3. Gaussian noise model.  
</details>


02.05: **L7. Towards Bayesian Inference** | <a href="slides/lecture-7/index.html"  style="text-decoration: none">Slides</a>
<details>
  <summary>Contents</summary>

  1. Posterior mean and covariance for a linear model.
  2. Fisher information matrix.
  3. Bayesian model introduction.
  4. Posterior definition. 
</details>


02.07: **L8. Bayesian inference in action**  | <a href="slides/lecture-8/index.html"  style="text-decoration: none">Slides</a>
<details>
  <summary>Contents</summary>

  1. Analytical calculation of the posterior
  2. Conjugacy in Bayesian inference
  3. A function-space perspective
</details>

02.12: <a href="https://gtvault-my.sharepoint.com/:u:/g/personal/pseshadri34_gatech_edu/EUX0hSg8eIdNjlFIUsR4Qq0BRewWmfMuoVeoDfisYT_MCQ?e=tPl4Ql"> *Fundamentals Mid-term (take-home)*</a>

02.12: **L9. An introduction to Gaussian Processes** | <a href="slides/lecture-9/index.html"  style="text-decoration: none">Slides</a> | <a href="https://psesh.github.io/ae8803/useful_codes/gaussians.html"  style="text-decoration: none">Notebook</a>
<details>
  <summary>Contents</summary>

  1. Gaussian process prior 
  2. Noise-free regression 
  3. Kernel functions
  4. Midterm overview
</details>

02.14: **L10. More on Gaussian Processes and Kernels** | <a href="slides/lecture-10/index.html"  style="text-decoration: none">Slides</a> | <a href="https://psesh.github.io/ae8803/useful_codes/gaussians.html"  style="text-decoration: none">Notebook</a>
<details>
  <summary>Contents</summary>

  1. Noisy regression
  2. More about kernels 
  3. Kernel trick
</details>

02.19: **No class**

02.21: **No class**

02.26: **L11. More about Kernels** | <a href="https://psesh.github.io/ae8803/useful_codes/kernels.html"  style="text-decoration: none">Notebook 1</a> | <a href="https://psesh.github.io/ae8803/useful_codes/eigen.html"  style="text-decoration: none">Notebook 2</a> | <a href="https://psesh.github.io/ae8803/useful_codes/fourier.html"  style="text-decoration: none">Notebook 3</a> |
<details>
  <summary>Contents</summary>
    
    1. Minimum norm problems
    2. The case of infinitely many feature vectors
    3. Eigenfunction analysis
    4. Fourier analysis
</details>

02.28: *Coding assignment isseued*


02.28: **L12. Hyperparameters inference** <a href="slides/lecture-12/index.html"  style="text-decoration: none">Slides</a>
<details>
  <summary>Contents</summary>
    
    1. MAP
    2. Marginal likelihood
    3. Introduction to ``gpytorch`` and ``pymc``
</details>

03.04: **L13. Markov chain Monte Carlo** <a href="https://psesh.github.io/ae8803/useful_codes/mcmc.html"  style="text-decoration: none">Notebook</a> 
<details>
  <summary>Contents</summary>

  1. MAP vs MCMC
  2. Metropolis
  3. Metropolis-Hastings
  4. HMC and NUTS 
</details>


03.06: **L14. Approximate inference** | <a href="https://gtvault-my.sharepoint.com/:b:/g/personal/pseshadri34_gatech_edu/EfFEnhJyRWpDu3IEMCq8I8YBD9AzoyALXSxUBeabYgVXoQ?e=0WWEvj" target="_blank" style="text-decoration: none">Slides</a> | <a href="https://psesh.github.io/ae8803/useful_codes/vi.html"  style="text-decoration: none">Notebook</a> 
<details>
  <summary>Contents</summary>

  1. Review of approximate inference methods.
  2. KL divergence
  3. Variational inference
</details>

03.08: **L15. A Gaussian Process Case Study** | <a href="https://gtvault-my.sharepoint.com/:b:/g/personal/pseshadri34_gatech_edu/EYa7GOSz_kpMolQW4m8tqP4BzDEZ2qCm-vl92Am2irzMCQ?e=oi3iB0" target="_blank" style="text-decoration: none">Slides</a> 


03.13: *Withdrawal Deadline*

03.18-03.22: *Spring Break*

03.25: **L16. Scaling Gaussian Processes (Linear Algebra Perspective)** | <a href="https://gtvault-my.sharepoint.com/:b:/g/personal/pseshadri34_gatech_edu/ESJsyJpCJFhJkeF7DI0g3boBlM9TpN4KR8mXe8uUOj0ZIA?e=JFCvPb" target="_blank" style="text-decoration: none">Slides</a>
<details>
  <summary>Contents</summary>

  1. Nystrom approximation.
  2. Kronecker product structure.
  3. Toeplitz structure.
</details>


03.27: **L17. Scaling Gaussian Processes II (Probabilistic Perspective)** | <a href="https://gtvault-my.sharepoint.com/:b:/g/personal/pseshadri34_gatech_edu/Ee2mprSEltRPm60DhcqBb8QB3wm9gOiovn21PS7FcGVx-A?e=0xFhiC" target="_blank" style="text-decoration: none">Slides</a> | <a href="https://psesh.github.io/ae8803/useful_codes/sparse.html"  style="text-decoration: none">Notebook</a> 
<details>
  <summary>Contents</summary>

  1. Bayesian inference review.
  2. Deterministic training conditional (DTC).
  3. Fully independent training conditional (FITC).
</details>


04.01: **L18. Gaussian process classification** | <a href="https://gtvault-my.sharepoint.com/:b:/g/personal/pseshadri34_gatech_edu/EdaxndYyYe5HkghB81QSh2IBRTzz-X9HZBl6eM6n1hq_Nw?e=ldTZiR" target="_blank" style="text-decoration: none">Slides</a> 
<details>
  <summary>Contents</summary>

  1. Classification likelihood.
  2. MAP via Newton Raphson.
</details>

04.03: **L19. Live coding session:** | Code coming up shortly!

<details>
  <summary>Contents</summary>

  1. Newton Raphson classification example.
  2. A simple mulit-task model
</details>

04.08: **L20. Multi-task and Physics-Constrains in Kernels:** | <a href="https://gtvault-my.sharepoint.com/:b:/g/personal/pseshadri34_gatech_edu/EdMUBKI6awJIiCytd8rlDnQBp2NTGq0hS4T0-KiGjHzn3A?e=SaQLBJ" target="_blank" style="text-decoration: none">Slides</a> | <a href="https://psesh.github.io/ae8803/useful_codes/curl.html"  style="text-decoration: none">Notebook 1</a> | <a href="https://psesh.github.io/ae8803/useful_codes/div.html"  style="text-decoration: none">Notebook 2</a> 

<details>
  <summary>Contents</summary>

  1. Model of coregionalization.
  2. Divergence free kernel.
  3. Curl free kernel.
</details>

04.03: **L22. Bayesian optimization** 
<details>
  <summary>Contents</summary>

  1. Gaussian process surrogate.
  2. Acquisition function.
  3. Thompson's sampling. 
  4. Gaussian process dynamic model. 
</details>

04.08: **L23. Guest Lecture** 

04.22: **L24. Project presentations** 

## Office hours

Professor Seshadri's office hours:

| Location  | Time    |
| --------  | ------- |
| MK 421    | Fridays 14:30 to 15:30 |

## Textbooks

This course will make heavy use of the following texts:

- Rasmussen, C. E., Williams, C. K. *Gaussian Processes for Machine Learning*, The MIT Press, 2006.
- Murphy, K. P., *Probabilistic Machine Learning: Advanced Topics*, The MIT Press, 2023.

Both these texts have been made freely available by the authors.

## Important papers

Students are encouraged to read through the following papers:

- [Roberts, S., Osborne, M., Ebden, M., Reece, S., Gibson, N., Aigrain, S., (2013) *Gaussian processes for time-series modelling*, Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences.](https://doi.org/10.1098/rsta.2011.0550)

- [Dunlop, M., Girolami, M., Stuart, A., Teckentrup, A., (2018) *How Deep Are Deep Gaussian Processes?*, Journal of Machine Learning Research 19, 1-46](https://www.jmlr.org/papers/volume19/18-015/18-015.pdf)

- [Alvarez, M., Lawrence, N., (2011) *Computationally Efficient Convolved Multiple Output Gaussian Processes*, Journal of Machine Learning Research 12, 1459-1500](https://www.jmlr.org/papers/volume12/alvarez11a/alvarez11a.pdf)

- [Van der Wilk, M., Rasmussen, C., Hensman, J., (2017) *Convolutional Gaussian Processes*, 31st Conference on Neural Information Processing Systems](https://dl.acm.org/doi/pdf/10.5555/3294996.3295044)

## References

Material used in this course has been adapted from 

- CUED Part IB probability course notes
- Alto University's module on Gaussian Processes
- Slides from the Gaussian Process Summer Schools