{
 "cells": [
  {
   "cell_type": "raw",
   "id": "1135697a-df43-4def-88fc-9967b24b7649",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Overview\"\n",
    "format:\n",
    "    html:\n",
    "        code-fold: true\n",
    "jupyter: python3\n",
    "fontsize: 1.2em\n",
    "linestretch: 1.5\n",
    "toc: true\n",
    "notebook-view: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf6f8f0",
   "metadata": {},
   "source": [
    "### Course Description:\n",
    "\n",
    "This graduate-level course offers a practical approach to probabilistic learning with Gaussian processes (GPs). GPs represent a powerful set of methods for modeling and predicting a wide variety of spatio-temporal phenomena. Today, they are used for problems that span both regression and classification, with theoretical foundations in Bayesian inference, reproducing kernel Hilbert spaces, eigenvalue problems, and numerical integration. Rather than focus *solely* on these theoretical foundations, this course balances theory with practical probabilistic programming, using a variety of ``python``-based packages. Moreover, practical engineering problems will also be discussed, that see GP models that cut across other areas of machine learning including transfer learning, convolutional networks, and normalizing flows. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d26c5a",
   "metadata": {},
   "source": [
    "### Pre-requisites:\n",
    "\n",
    "- CS1371, MATH2551, MATH2552 (or equivalent)\n",
    "- Working knowledge of ``python`` including familiarity with ``numpy`` and ``matplotlib`` libraries. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19a9fab",
   "metadata": {},
   "source": [
    "## Lectures\n",
    "\n",
    "Below you will find a list of the lectures that form the backbone of this course.\n",
    "\n",
    "\n",
    "#### L1 | Probability fundamentals I\n",
    "<details>\n",
    "<summary>Contents</summary>\n",
    "    \n",
    "- Probability\n",
    "- Conditional probability and independence\n",
    "- Expectation of a random variable\n",
    "- Probability density function for a continuous random variable\n",
    "- Key discrete probability mass functions\n",
    "- Key continuous probability density functions\n",
    "</details>\n",
    "\n",
    "#### L2 | Probability fundamentals II\n",
    "<details>\n",
    "<summary>Contents</summary>\n",
    "    \n",
    "- Functions of random variables\n",
    "- Multivariate distributions\n",
    "- Decision and estimation: basic definitions\n",
    "- Tests of significance\n",
    "</details>\n",
    "\n",
    "\n",
    "#### L3 | Introduction to Bayesian inference\n",
    "<details>\n",
    "<summary>Contents</summary>\n",
    "    \n",
    "- Introduction to Bayesian modelling\n",
    "- Conjugacy with distributions\n",
    "- Bayesian polynomial regression\n",
    "</details>\n",
    "\n",
    "#### L4 | The uniqueness of the Normal distribution\n",
    "<details>\n",
    "<summary>Contents</summary>\n",
    "    \n",
    "- Marginal distributions\n",
    "- Conditional distributions\n",
    "- Nataf (and other) transforms\n",
    "</details>\n",
    "\n",
    "#### L5 | Exact vs. approximate inference\n",
    "<details>\n",
    "<summary>Contents</summary>\n",
    "    \n",
    "- Maximum likelihood and maximum aposteriori estimate\n",
    "- Markov chain monte Carlo\n",
    "- Expectation maximization\n",
    "</details>\n",
    "\n",
    "#### L6 | Introduction to Gaussian processes\n",
    "\n",
    "<details>\n",
    "<summary>Contents</summary>\n",
    "    \n",
    "- Motivation and parallels with Bayesian optimization\n",
    "- Mercer kernels (spectral densities, periodic, Matern, squared exponential)\n",
    "- Making new kernels from old ones\n",
    "- Hyperparameters (and their hyperparameters)\n",
    "</details>\n",
    "\n",
    "#### L7 | Gaussian likelihoods\n",
    "\n",
    "<details>\n",
    "<summary>Contents</summary>\n",
    "    \n",
    "- Prediction using noise-free observations\n",
    "- Prediction with noisy observations\n",
    "- Weight-space vs. function-space perspectives\n",
    "- Semi-parametric models\n",
    "</details>\n",
    "\n",
    "#### L8 | Gaussian & non-Gaussian likelihoods\n",
    "\n",
    "<details>\n",
    "<summary>Contents</summary>\n",
    "    \n",
    "- Reproducing kernel Hilbert spaces\n",
    "- *Representer* theorem\n",
    "- Non-Gaussian likelihoods and classification\n",
    "</details>\n",
    "\n",
    "#### L9 | Scaling Gaussian processes\n",
    "\n",
    "<details>\n",
    "<summary>Contents</summary>\n",
    "    \n",
    "- Computing the matrix inverse via Cholesky decomposition\n",
    "- Subset of data approaches\n",
    "- Nystrom approximation\n",
    "- Inducing points\n",
    "</details>\n",
    "\n",
    "#### L10 | Scaling Gaussian processes II\n",
    "\n",
    "<details>\n",
    "<summary>Contents</summary>\n",
    "    \n",
    "- Variational inference\n",
    "- ELBO derivation\n",
    "- Minimizing the KL-divergence practically\n",
    "</details>\n",
    "\n",
    "#### L11 | Multiple kernel learning\n",
    "\n",
    "<details>\n",
    "<summary>Contents</summary>\n",
    "    \n",
    "- Empirical Bayes\n",
    "- Multiple kernel learning\n",
    "- Generalized additive models\n",
    "</details>\n",
    "\n",
    "#### L12 | Revisiting hyperparameter training\n",
    "\n",
    "<details>\n",
    "<summary>Contents</summary>\n",
    "    \n",
    "- HMC, NUTS, and Gibbs sampling for MCMC\n",
    "</details>\n",
    "\n",
    "#### L13 | Gaussian processes and deep neural networks\n",
    "\n",
    "<details>\n",
    "<summary>Contents</summary>\n",
    "    \n",
    "- Single and deep MLPs\n",
    "- Deep Gaussian processes\n",
    "- Posterior inference\n",
    "</details>\n",
    "\n",
    "\n",
    "#### L14 | Designing bespoke deep Gaussian processes\n",
    "\n",
    "<details>\n",
    "<summary>Contents</summary>\n",
    "    \n",
    "- Covariance structure\n",
    "- Warping inputs\n",
    "- Parallels to normalizing flows\n",
    "- Hierarchy of hyperparameters\n",
    "</details>\n",
    "\n",
    "#### L15 | Time-series forecasting with Gaussian processes\n",
    "\n",
    "<details>\n",
    "<summary>Contents</summary>\n",
    "    \n",
    "- Linear Gaussian state space model\n",
    "- Kalman smoother\n",
    "</details>\n",
    "\n",
    "#### L16 | Conditioning on linear operators\n",
    "\n",
    "<details>\n",
    "<summary>Contents</summary>\n",
    "    \n",
    "- Integral operators\n",
    "- Differential operators\n",
    "</details>\n",
    "\n",
    "#### L17 | Multi-output Gaussian processes\n",
    "\n",
    "<details>\n",
    "<summary>Contents</summary>\n",
    "    \n",
    "- Coregional models\n",
    "- Transfer learning across outputs\n",
    "</details>\n",
    "\n",
    "#### L18 | Group Polynomial and other kernels\n",
    "\n",
    "<details>\n",
    "<summary>Contents</summary>\n",
    "    \n",
    "- Orthogonality and data distribution\n",
    "- Numerical quadrature\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620cd7d6",
   "metadata": {},
   "source": [
    "## Office hours\n",
    "\n",
    "Professor Seshadri's office hours:\n",
    "\n",
    "| Location  | Time    |\n",
    "| --------  | ------- |\n",
    "| GU 341    | TBD   |\n",
    "| GU 341    |TBD    |\n",
    "\n",
    "Location may change during term. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aa2835",
   "metadata": {},
   "source": [
    "## Textbooks\n",
    "\n",
    "This course will make heavy use of the following texts:\n",
    "\n",
    "- Rasmussen, C. E., Williams, C. K. *Gaussian Processes for Machine Learning*, The MIT Press, 2006.\n",
    "- Murphy, K. P., *Probabilistic Machine Learning: Advanced Topics*, The MIT Press, 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8118721",
   "metadata": {},
   "source": [
    "## Important papers\n",
    "\n",
    "Students are encouraged to read through the following papers:\n",
    "\n",
    "- [Roberts, S., Osborne, M., Ebden, M., Reece, S., Gibson, N., Aigrain, S., (2013) *Gaussian processes for time-series modelling*, Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences.](https://doi.org/10.1098/rsta.2011.0550)\n",
    "\n",
    "- [Dunlop, M., Girolami, M., Stuart, A., Teckentrup, A., (2018) *How Deep Are Deep Gaussian Processes?*, Journal of Machine Learning Research 19, 1-46](https://www.jmlr.org/papers/volume19/18-015/18-015.pdf)\n",
    "\n",
    "- [Alvarez, M., Lawrence, N., (2011) *Computationally Efficient Convolved Multiple Output Gaussian Processes*, Journal of Machine Learning Research 12, 1459-1500](https://www.jmlr.org/papers/volume12/alvarez11a/alvarez11a.pdf)\n",
    "\n",
    "- [Van der Wilk, M., Rasmussen, C., Hensman, J., (2017) *Convolutional Gaussian Processes*, 31st Conference on Neural Information Processing Systems](https://dl.acm.org/doi/pdf/10.5555/3294996.3295044)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
