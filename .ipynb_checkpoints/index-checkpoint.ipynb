{
 "cells": [
  {
   "cell_type": "raw",
   "id": "1135697a-df43-4def-88fc-9967b24b7649",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Overview\"\n",
    "format:\n",
    "    html:\n",
    "        code-fold: true\n",
    "jupyter: python3\n",
    "fontsize: 1.2em\n",
    "linestretch: 1.5\n",
    "toc: true\n",
    "notebook-view: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf6f8f0",
   "metadata": {},
   "source": [
    "### Course Description:\n",
    "\n",
    "This graduate-level course offers a practical approach to probabilistic learning with Gaussian processes (GPs). GPs represent a powerful set of methods for modeling and predicting a wide variety of spatio-temporal phenomena. It is used for problems that span both regression and classification, with theoretical foundations in Bayesian inference, reproducing kernel Hilbert spaces, eigenvalue problems, and numerical integration. Rather than focus solely on these theoretical foundations, this course balances theory with practical probabilistic programming, using a variety of ``python``-based packages. Practical engineering-relevant problems will also be discussed, cutting across other areas of machine learning such as transfer learning, deep models, and normalizing flows. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d26c5a",
   "metadata": {},
   "source": [
    "### Pre-requisites:\n",
    "\n",
    "- CS1371, MATH2551, MATH2552 (or equivalent)\n",
    "- Working knowledge of ``python`` including familiarity with ``numpy`` and ``matplotlib`` libraries. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19a9fab",
   "metadata": {},
   "source": [
    "## Lectures\n",
    "\n",
    "This is a preliminary schedule; it may change throughout term. \n",
    "\n",
    "\n",
    "#### L1 | Probability fundamentals I\n",
    "<details>\n",
    "<summary>Contents</summary>\n",
    "    \n",
    "- Probability\n",
    "- Conditional probability and independence\n",
    "- Expectation of a random variable\n",
    "- Probability density function for a continuous random variable\n",
    "- Key discrete probability mass functions\n",
    "- Key continuous probability density functions\n",
    "</details>\n",
    "\n",
    "#### L2 | Probability fundamentals II\n",
    "<details>\n",
    "<summary>Contents</summary>\n",
    "    \n",
    "- Functions of random variables\n",
    "- Multivariate distributions\n",
    "- Decision and estimation: basic definitions\n",
    "- Tests of significance\n",
    "</details>\n",
    "\n",
    "\n",
    "#### L3 | Introduction to Bayesian inference\n",
    "<details>\n",
    "<summary>Contents</summary>\n",
    "    \n",
    "- Introduction to Bayesian modelling\n",
    "- Conjugacy with distributions\n",
    "- Bayesian polynomial regression\n",
    "</details>\n",
    "\n",
    "#### L4 | The uniqueness of the Normal distribution\n",
    "<details>\n",
    "<summary>Contents</summary>\n",
    "    \n",
    "- Marginal distributions\n",
    "- Conditional distributions\n",
    "- Nataf (and other) transforms\n",
    "</details>\n",
    "\n",
    "#### L5 | Exact and approximate inference\n",
    "<details>\n",
    "<summary>Contents</summary>\n",
    "    \n",
    "- Maximum likelihood and maximum aposteriori estimate\n",
    "- Markov chain monte Carlo\n",
    "- Expectation maximization\n",
    "</details>\n",
    "\n",
    "#### L6 | Introduction to Gaussian processes\n",
    "\n",
    "<details>\n",
    "<summary>Contents</summary>\n",
    "    \n",
    "- Motivation and parallels with Bayesian optimization\n",
    "- Mercer kernels (spectral densities, periodic, Matern, squared exponential)\n",
    "- Making new kernels from old ones\n",
    "- Hyperparameters (and their hyperparameters)\n",
    "</details>\n",
    "\n",
    "#### L7 | Gaussian likelihoods\n",
    "\n",
    "<details>\n",
    "<summary>Contents</summary>\n",
    "    \n",
    "- Prediction using noise-free observations\n",
    "- Prediction with noisy observations\n",
    "- Weight-space vs. function-space perspectives\n",
    "- Semi-parametric models\n",
    "</details>\n",
    "\n",
    "#### L8 | Gaussian & non-Gaussian likelihoods\n",
    "\n",
    "<details>\n",
    "<summary>Contents</summary>\n",
    "    \n",
    "- Reproducing kernel Hilbert spaces\n",
    "- *Representer* theorem\n",
    "- Non-Gaussian likelihoods and classification\n",
    "</details>\n",
    "\n",
    "#### L9 | Scaling Gaussian processes\n",
    "\n",
    "<details>\n",
    "<summary>Contents</summary>\n",
    "    \n",
    "- Computing the matrix inverse via Cholesky decomposition\n",
    "- Subset of data approaches\n",
    "- Nystrom approximation\n",
    "- Inducing points\n",
    "</details>\n",
    "\n",
    "#### L10 | Scaling Gaussian processes II\n",
    "\n",
    "<details>\n",
    "<summary>Contents</summary>\n",
    "    \n",
    "- Variational inference\n",
    "- ELBO derivation\n",
    "- Minimizing the KL-divergence practically\n",
    "</details>\n",
    "\n",
    "#### L11 | Multiple kernel learning\n",
    "\n",
    "<details>\n",
    "<summary>Contents</summary>\n",
    "    \n",
    "- Empirical Bayes\n",
    "- Multiple kernel learning\n",
    "- Generalized additive models\n",
    "</details>\n",
    "\n",
    "#### L12 | Gaussian processes and deep neural networks\n",
    "\n",
    "<details>\n",
    "<summary>Contents</summary>\n",
    "    \n",
    "- Single and deep MLPs\n",
    "- Deep Gaussian processes\n",
    "- Posterior inference\n",
    "</details>\n",
    "\n",
    "\n",
    "#### L13 | Designing bespoke deep Gaussian processes\n",
    "\n",
    "<details>\n",
    "<summary>Contents</summary>\n",
    "    \n",
    "- Covariance structure\n",
    "- Warping inputs\n",
    "- Parallels to normalizing flows\n",
    "- Hierarchy of hyperparameters\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620cd7d6",
   "metadata": {},
   "source": [
    "## Office hours\n",
    "\n",
    "Professor Seshadri's office hours:\n",
    "\n",
    "| Location  | Time    |\n",
    "| --------  | ------- |\n",
    "| GU 341    | TBD   |\n",
    "| GU 341    |TBD    |\n",
    "\n",
    "Location may change during term. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aa2835",
   "metadata": {},
   "source": [
    "## Textbooks\n",
    "\n",
    "This course will make heavy use of the following texts:\n",
    "\n",
    "- Rasmussen, C. E., Williams, C. K. *Gaussian Processes for Machine Learning*, The MIT Press, 2006.\n",
    "- Murphy, K. P., *Probabilistic Machine Learning: Advanced Topics*, The MIT Press, 2023.\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99ecde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Papers\n",
    "\n",
    "This course will also rely on the following papers; students are encouraged to read this in their own time. \n",
    "\n",
    "- Deep GP\n",
    "- Aeroengine\n",
    "- GPs with polynomial kernels\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
