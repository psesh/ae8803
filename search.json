[
  {
    "objectID": "useful_codes/discrete.html",
    "href": "useful_codes/discrete.html",
    "title": "Discrete distributions",
    "section": "",
    "text": "This notebook is has useful boiler plate code for generating distributions and visualizing them.\n\n\nCode\nimport numpy as np \nfrom scipy.stats import bernoulli, binom\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.special import comb\nsns.set(font_scale=1.0)\nsns.set_style(\"white\")\nsns.set_style(\"ticks\")\npalette = sns.color_palette('deep')\n#plt.style.use('dark_background') # cosmetic!\n\n\n\n\nThe probability mass function for a Bernoulli distribution is given by\n\\[\np \\left( x \\right) = \\begin{cases}\n\\begin{array}{c}\n1 - p  \\; \\; \\; \\textrm{if} \\; x = 0 \\\\\np \\; \\; \\; \\textrm{if} \\; x = 1\n\\end{array}\\end{cases}\n\\]\nfor \\(x \\in \\left\\{0, 1 \\right\\}\\) and where \\(0 \\leq p \\leq 1\\).\n\n\nCode\np = 0.4 # Bernoulli parameter\nx = np.linspace(0, 1, 2)\nprobabilities = bernoulli.pmf(x, p)\n\nfig = plt.figure(figsize=(8,4))\n\nplt.plot(x, probabilities, 'o', ms=8, color='orangered')\nplt.vlines(x, 0, probabilities, colors='orangered', lw=5, alpha=0.5)\nplt.xlabel('x')\nplt.ylabel('Probability')\nplt.savefig('pdf.png', dpi=150, bbox_inches='tight', transparent=True)\n\nplt.show()\n\n\n\n\n\nOne can generate random values from this distribution, i.e.,\n\n\nCode\nX = bernoulli.rvs(p, size=500)\nprint(X)\n\n\n[1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0 1 0 1 1 0 1 0 0\n 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0\n 0 1 1 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0\n 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0 0\n 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0\n 0 0 1 0 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 0 1\n 0 0 0 1 0 1 0 0 0 1 1 1 0 0 1 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0\n 1 1 0 1 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0\n 0 1 0 0 0 1 0 0 1 1 1 0 1 1 0 0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 0 0 0 1\n 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 1 0 0 0 0 1 1 0 1\n 1 0 1 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 1\n 1 0 1 0 1 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 1\n 1 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0 1]\n\n\nThus, random values from a Bernoulli distribution are inherently binary, and the number of 0s vs 1s will vary depending on the choice of the parameter, \\(p\\). We will see later on (in another notebook) how this relatively simple idea can be used to train a Naive Bayes Classifier. For now, we will plot the expected value of the Bernoulli random variable with increasing number of samples.\n\n\nCode\nnumbers = [10, 50, 100, 200, 300, 500, 1000, 2000, 5000, 10000]\nmeans = []\nstds = []\nfor j in numbers:\n    X_val = []\n    for q in range(0, 10):\n        X = bernoulli.rvs(p, size=j)\n        X_val.append(np.mean(X))\n    means.append(np.mean(X_val))\n    stds.append(np.std(X_val))\n\nmeans = np.array(means)\nstds = np.array(stds)\nnumbers = np.array(numbers)\n\nfig = plt.figure(figsize=(8,4))\nplt.plot(numbers, means, 'ro-', lw=2)\nplt.fill_between(numbers, means + stds, means - stds, color='crimson', alpha=0.3)\nplt.xlabel('Number of random samples')\nplt.ylabel('Expectation')\nplt.savefig('convergence.png', dpi=150, bbox_inches='tight', transparent=True)\nplt.show()\n\n\n\n\n\n\n\n\nNext, we consider the Binomial distribution. It has a probability mass function\n\\[\np \\left( x \\right) = \\left(\\begin{array}{c}\nn\\\\\nx\n\\end{array}\\right)p^{x}\\left(1-p\\right)^{n-x}\n\\]\nfor \\(x \\in \\left\\{0, 1, \\ldots, n \\right\\}\\) and where \\(0 \\leq p \\leq 1\\).\n\n\nCode\np = 0.3 # Bernoulli parameter\nn = 7\nx = np.arange(0, n+1)\nprobabilities = binom(n, p)\n\nfig = plt.figure(figsize=(8,4))\nplt.plot(x, probabilities.pmf(x), 'o', ms=8, color='deeppink')\nplt.vlines(x, 0, probabilities.pmf(x), colors='deeppink', lw=5 )\nplt.xlabel('x')\nplt.ylabel('Probability')\nplt.savefig('pdf_2.png', dpi=150, bbox_inches='tight', transparent=True)\nplt.show()\n\n\n\n\n\nTo work out the probability at \\(x=3\\), we can compute:\n\n\nCode\nprob = comb(N=n, k=3) * p**3 * (1 - p)**(n - 3)\nprint(prob)\n\n\n0.22689449999999992"
  },
  {
    "objectID": "useful_codes/discrete.html#scope",
    "href": "useful_codes/discrete.html#scope",
    "title": "Discrete distributions",
    "section": "",
    "text": "This notebook is has useful boiler plate code for generating distributions and visualizing them.\n\n\nCode\nimport numpy as np \nfrom scipy.stats import bernoulli, binom\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.special import comb\nsns.set(font_scale=1.0)\nsns.set_style(\"white\")\nsns.set_style(\"ticks\")\npalette = sns.color_palette('deep')\n#plt.style.use('dark_background') # cosmetic!\n\n\n\n\nThe probability mass function for a Bernoulli distribution is given by\n\\[\np \\left( x \\right) = \\begin{cases}\n\\begin{array}{c}\n1 - p  \\; \\; \\; \\textrm{if} \\; x = 0 \\\\\np \\; \\; \\; \\textrm{if} \\; x = 1\n\\end{array}\\end{cases}\n\\]\nfor \\(x \\in \\left\\{0, 1 \\right\\}\\) and where \\(0 \\leq p \\leq 1\\).\n\n\nCode\np = 0.4 # Bernoulli parameter\nx = np.linspace(0, 1, 2)\nprobabilities = bernoulli.pmf(x, p)\n\nfig = plt.figure(figsize=(8,4))\n\nplt.plot(x, probabilities, 'o', ms=8, color='orangered')\nplt.vlines(x, 0, probabilities, colors='orangered', lw=5, alpha=0.5)\nplt.xlabel('x')\nplt.ylabel('Probability')\nplt.savefig('pdf.png', dpi=150, bbox_inches='tight', transparent=True)\n\nplt.show()\n\n\n\n\n\nOne can generate random values from this distribution, i.e.,\n\n\nCode\nX = bernoulli.rvs(p, size=500)\nprint(X)\n\n\n[1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0 1 0 1 1 0 1 0 0\n 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0\n 0 1 1 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0\n 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0 0\n 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0\n 0 0 1 0 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 0 1\n 0 0 0 1 0 1 0 0 0 1 1 1 0 0 1 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0\n 1 1 0 1 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0\n 0 1 0 0 0 1 0 0 1 1 1 0 1 1 0 0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 0 0 0 1\n 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 1 0 0 0 0 1 1 0 1\n 1 0 1 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 1\n 1 0 1 0 1 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 1\n 1 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0 1]\n\n\nThus, random values from a Bernoulli distribution are inherently binary, and the number of 0s vs 1s will vary depending on the choice of the parameter, \\(p\\). We will see later on (in another notebook) how this relatively simple idea can be used to train a Naive Bayes Classifier. For now, we will plot the expected value of the Bernoulli random variable with increasing number of samples.\n\n\nCode\nnumbers = [10, 50, 100, 200, 300, 500, 1000, 2000, 5000, 10000]\nmeans = []\nstds = []\nfor j in numbers:\n    X_val = []\n    for q in range(0, 10):\n        X = bernoulli.rvs(p, size=j)\n        X_val.append(np.mean(X))\n    means.append(np.mean(X_val))\n    stds.append(np.std(X_val))\n\nmeans = np.array(means)\nstds = np.array(stds)\nnumbers = np.array(numbers)\n\nfig = plt.figure(figsize=(8,4))\nplt.plot(numbers, means, 'ro-', lw=2)\nplt.fill_between(numbers, means + stds, means - stds, color='crimson', alpha=0.3)\nplt.xlabel('Number of random samples')\nplt.ylabel('Expectation')\nplt.savefig('convergence.png', dpi=150, bbox_inches='tight', transparent=True)\nplt.show()\n\n\n\n\n\n\n\n\nNext, we consider the Binomial distribution. It has a probability mass function\n\\[\np \\left( x \\right) = \\left(\\begin{array}{c}\nn\\\\\nx\n\\end{array}\\right)p^{x}\\left(1-p\\right)^{n-x}\n\\]\nfor \\(x \\in \\left\\{0, 1, \\ldots, n \\right\\}\\) and where \\(0 \\leq p \\leq 1\\).\n\n\nCode\np = 0.3 # Bernoulli parameter\nn = 7\nx = np.arange(0, n+1)\nprobabilities = binom(n, p)\n\nfig = plt.figure(figsize=(8,4))\nplt.plot(x, probabilities.pmf(x), 'o', ms=8, color='deeppink')\nplt.vlines(x, 0, probabilities.pmf(x), colors='deeppink', lw=5 )\nplt.xlabel('x')\nplt.ylabel('Probability')\nplt.savefig('pdf_2.png', dpi=150, bbox_inches='tight', transparent=True)\nplt.show()\n\n\n\n\n\nTo work out the probability at \\(x=3\\), we can compute:\n\n\nCode\nprob = comb(N=n, k=3) * p**3 * (1 - p)**(n - 3)\nprint(prob)\n\n\n0.22689449999999992"
  },
  {
    "objectID": "sample_problems/lecture_3.html",
    "href": "sample_problems/lecture_3.html",
    "title": "L3 examples",
    "section": "",
    "text": "In going through some historical records, you find that scientists from a lost civilization tried to measure the distance from the ground to some clouds. Based on the data you assume that the distance is a Gaussian random variable with a mean of 1830m and a standard deviation of 460m. What is the probability that the clouds would actually be at a height of 2750m?\n\n\nSolution\n\nLet \\(X\\) be this Gaussian random variable. This problem essentially requires us to work out \\(p \\left( X &gt; 2750 \\right)\\). This can be expressed as\n\\[\n\\large\np \\left(X &gt; 2750 \\right) = 1 - p \\left( X \\leq 2750 \\right) = 1 - \\Phi \\left( z \\right)\n\\]\nwhere \\(z = (2750 - 1830)/460 = 2\\). Thus we have\n\\[\n\\large\n1 - \\Phi \\left( 2 \\right) = 1 - 0.9772 = 0.0228\n\\]"
  },
  {
    "objectID": "sample_problems/lecture_3.html#problem-1",
    "href": "sample_problems/lecture_3.html#problem-1",
    "title": "L3 examples",
    "section": "",
    "text": "In going through some historical records, you find that scientists from a lost civilization tried to measure the distance from the ground to some clouds. Based on the data you assume that the distance is a Gaussian random variable with a mean of 1830m and a standard deviation of 460m. What is the probability that the clouds would actually be at a height of 2750m?\n\n\nSolution\n\nLet \\(X\\) be this Gaussian random variable. This problem essentially requires us to work out \\(p \\left( X &gt; 2750 \\right)\\). This can be expressed as\n\\[\n\\large\np \\left(X &gt; 2750 \\right) = 1 - p \\left( X \\leq 2750 \\right) = 1 - \\Phi \\left( z \\right)\n\\]\nwhere \\(z = (2750 - 1830)/460 = 2\\). Thus we have\n\\[\n\\large\n1 - \\Phi \\left( 2 \\right) = 1 - 0.9772 = 0.0228\n\\]"
  },
  {
    "objectID": "sample_problems/lecture_1.html",
    "href": "sample_problems/lecture_1.html",
    "title": "L1 examples",
    "section": "",
    "text": "The probability that a scheduled flight departs on time is 0.83 and the probability that it arrives on time is 0.92. The probability that it both departs and arrives on time is 0.78. Find the probability that\n\nthe plane arrives on time given that it departed on time;\nthe plane did not depart on time given that it did not arrive on time.\n\n\n\nSolution\n\nIt will be useful to consider the Venn diagram shown below.\n\nLet \\(\\require{color}{\\color[rgb]{0.000066,0.001801,0.998229}A}\\) denote the event that the plane arrives on time, while \\({\\color[rgb]{0.986252,0.007236,0.027423}D}\\) denotes th event that the plane departs on time. To construct the Venn diagram above note that \\(P\\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) = 0.78\\). From the sum rule of probabilities, we have:\n\\[\n\\require{color}\n\\large\nP\\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\right) = P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap \\bar{{\\color[rgb]{0.986252,0.007236,0.027423}D}} \\right) + P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right)\n\\]\n\\[\n\\require{color}\n\\large\n0.92= 0.78+ P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right)\n\\]\nwhich implies that \\(\\require{color} P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) = 0.92 - 0.78 = 0.14\\). Similarly, we have:\n\\[\n\\require{color}\n\\large\nP \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) = P \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\cap \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) + P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right)\n\\]\n\\[\n\\require{color}\n\\large\n\\Rightarrow 0.83 = P \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\cap \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) + 0.78\n\\]\nwhich implies that \\(P \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\cap \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) = 0.05\\). With these probabilities, we can now answer the questions.\n\nThe plane arrives on time conditioned that it departed on time:\n\n\\[\n\\large\n\\require{color}\nP \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} | {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) = \\frac{P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) }{P \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) } = \\frac{0.78}{0.83} = 0.94\n\\]\n\nThe plane did not depart on time conditioned on it having not arrived on time:\n\n\\[\n\\large\n\\require{color}\nP \\left( \\bar{{\\color[rgb]{0.986252,0.007236,0.027423}D}} | \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) = \\frac{P \\left( \\bar{{\\color[rgb]{0.986252,0.007236,0.027423}D } }\\cap \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) }{P \\left( \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) }\n\\]\n\\[\n\\large\n\\require{color}\n= \\frac{1 - P\\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\right) - P \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) + P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) }{1 - P\\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\right) } = \\frac{1 - 0.92 - 0.83 + 0.78}{1 - 0.92} = \\frac{0.03}{0.08} = 0.375\n\\]"
  },
  {
    "objectID": "sample_problems/lecture_1.html#problem-1",
    "href": "sample_problems/lecture_1.html#problem-1",
    "title": "L1 examples",
    "section": "",
    "text": "The probability that a scheduled flight departs on time is 0.83 and the probability that it arrives on time is 0.92. The probability that it both departs and arrives on time is 0.78. Find the probability that\n\nthe plane arrives on time given that it departed on time;\nthe plane did not depart on time given that it did not arrive on time.\n\n\n\nSolution\n\nIt will be useful to consider the Venn diagram shown below.\n\nLet \\(\\require{color}{\\color[rgb]{0.000066,0.001801,0.998229}A}\\) denote the event that the plane arrives on time, while \\({\\color[rgb]{0.986252,0.007236,0.027423}D}\\) denotes th event that the plane departs on time. To construct the Venn diagram above note that \\(P\\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) = 0.78\\). From the sum rule of probabilities, we have:\n\\[\n\\require{color}\n\\large\nP\\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\right) = P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap \\bar{{\\color[rgb]{0.986252,0.007236,0.027423}D}} \\right) + P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right)\n\\]\n\\[\n\\require{color}\n\\large\n0.92= 0.78+ P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right)\n\\]\nwhich implies that \\(\\require{color} P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) = 0.92 - 0.78 = 0.14\\). Similarly, we have:\n\\[\n\\require{color}\n\\large\nP \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) = P \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\cap \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) + P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right)\n\\]\n\\[\n\\require{color}\n\\large\n\\Rightarrow 0.83 = P \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\cap \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) + 0.78\n\\]\nwhich implies that \\(P \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\cap \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) = 0.05\\). With these probabilities, we can now answer the questions.\n\nThe plane arrives on time conditioned that it departed on time:\n\n\\[\n\\large\n\\require{color}\nP \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} | {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) = \\frac{P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) }{P \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) } = \\frac{0.78}{0.83} = 0.94\n\\]\n\nThe plane did not depart on time conditioned on it having not arrived on time:\n\n\\[\n\\large\n\\require{color}\nP \\left( \\bar{{\\color[rgb]{0.986252,0.007236,0.027423}D}} | \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) = \\frac{P \\left( \\bar{{\\color[rgb]{0.986252,0.007236,0.027423}D } }\\cap \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) }{P \\left( \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) }\n\\]\n\\[\n\\large\n\\require{color}\n= \\frac{1 - P\\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\right) - P \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) + P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) }{1 - P\\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\right) } = \\frac{1 - 0.92 - 0.83 + 0.78}{1 - 0.92} = \\frac{0.03}{0.08} = 0.375\n\\]"
  },
  {
    "objectID": "sample_problems/lecture_1.html#problem-2",
    "href": "sample_problems/lecture_1.html#problem-2",
    "title": "L1 examples",
    "section": "Problem 2",
    "text": "Problem 2\nToss a coin three times, what is the probability of at least two heads?\n\n\nSolution\n\nThere are 8 possible outcomes which, if the coin is unbiased, should all be equally likely:\n\nHHH\nHHT\nHTH\nHTT\nTHH\nTHT\nTTH\nTTT\n\nTwo or more heads result from 4 outcomes. The probability of two or more heads is therefore \\(4/8=1/2\\)."
  },
  {
    "objectID": "sample_problems/lecture_1.html#problem-3",
    "href": "sample_problems/lecture_1.html#problem-3",
    "title": "L1 examples",
    "section": "Problem 3",
    "text": "Problem 3\nThis problem introduces the idea that whilst it may be tempting to add probabilities, the context is very important.\nAround 0.9% of the population are blue-green color blind and roughly 1 in 5 is left-handed. Assuming these characteristics are inherited independently, calculate the probability that a person, chosen at random will:\n\nbe both color-blind and left-handed\nbe color-blind and not left-handed\nbe color-blind or left-handed\nbe neither color-blind nor left-handed\n\n\n\nSolution\n\nConsider the diagram shown below; given that the characteristics are inherited independently, each sub-branch of the population can be divided into color-blind and non-color-blind groups.\n\n\nthe probability of being both color-blind and left-handed is: \\(0.009 \\times 0.2 = 0.0018\\) or \\(0.18 \\%\\).\nthe probability of being color-blind and right-handed is: \\(0.009 \\times 0.8 = 0.0072\\).\nthis is the sum of all probabilities within the first branch and the probability calculated in the prior step, i.e., \\(0.20 + 0.0072 = 0.2072\\).\nthis is given by the last group, i.e., \\(0.991 \\times 0.8 = 0.7928\\)."
  },
  {
    "objectID": "sample_problems/lecture_1.html#problem-4",
    "href": "sample_problems/lecture_1.html#problem-4",
    "title": "L1 examples",
    "section": "Problem 4",
    "text": "Problem 4\nThis problem has two parts.\n\nDerive Bayes’ rule.\nThe chance of an honest citizen lying is 1 in 1000. Assume that such a citizen is tested with a lie detector which correctly identifies both truth and false statements 95 times out of 100.\n\n\nWhat is the probability that the lie detector indicates falsehood?\nIn this case, what is the probability that the person is actually lying?\n\n\n\nSolution\n\n\nTo derive Bayes’ rule, we will use the definition of the conditional probability, and the fact that \\(p \\left({\\color[rgb]{0.986252,0.007236,0.027423}A} \\cap {\\color[rgb]{0.131302,0.999697,0.023594}B} \\right) = p \\left( {\\color[rgb]{0.131302,0.999697,0.023594}B} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}A} \\right)\\), which leads to\n\n\\[\n\\large\n\\require{color}\np \\left( {\\color[rgb]{0.986252,0.007236,0.027423}A} | {\\color[rgb]{0.131302,0.999697,0.023594}B} \\right) p \\left( {\\color[rgb]{0.131302,0.999697,0.023594}B} \\right) = p \\left( {\\color[rgb]{0.131302,0.999697,0.023594}B}| {\\color[rgb]{0.986252,0.007236,0.027423}A} \\right) p \\left({\\color[rgb]{0.986252,0.007236,0.027423}A} \\right)\n\\]\nFrom this one can write\n\\[\n\\large\n\\require{color}\np \\left( {\\color[rgb]{0.986252,0.007236,0.027423}A} | {\\color[rgb]{0.131302,0.999697,0.023594}B} \\right) = \\frac{p \\left( {\\color[rgb]{0.131302,0.999697,0.023594}B} | {\\color[rgb]{0.986252,0.007236,0.027423}A} \\right) p \\left( {\\color[rgb]{0.986252,0.007236,0.027423}A} \\right) }{p \\left( {\\color[rgb]{0.131302,0.999697,0.023594}B} \\right) }\n\\]\n\nThe probability that the lie detector indicates a falsehood is based on (i) the citizen is lying, and (ii) the citizen is being honest, but the detector makes an error. Let \\(F\\) be the probability that the lie detector indicates a falsehood. Thus\n\n\\[\n\\large\np \\left( F \\right) = \\frac{1}{1000} \\times 0.95 + \\frac{999}{1000} \\times 0.05 = 0.0509.\n\\]\nLet $p ( L ) be the probability that the person is actually lying. Thus, what we want is\n\\[\n\\large\np \\left( L | F \\right) = \\frac{p \\left( F | L \\right) p \\left( L \\right) }{p \\left( F \\right) } = \\frac{0.95 \\times 0.001}{0.0509} = 0.01866.\n\\]"
  },
  {
    "objectID": "slides/lecture-5/index.html#multivariate-gaussians",
    "href": "slides/lecture-5/index.html#multivariate-gaussians",
    "title": "Lecture 5",
    "section": "Multivariate Gaussians",
    "text": "Multivariate Gaussians\nAlthough we have introduced joint probabilities and learnt how to manipulate them in the lectures prior, we have thus far only stuied univariate densities.\nIn this lecture, we will focus on one multivariate density that sets the stage for our journey into machine learning: the Gaussian distribution!"
  },
  {
    "objectID": "slides/lecture-5/index.html#multivariate-gaussians-1",
    "href": "slides/lecture-5/index.html#multivariate-gaussians-1",
    "title": "Lecture 5",
    "section": "Multivariate Gaussians",
    "text": "Multivariate Gaussians\nThe random vector, \\mathbf{X} = \\left(X_1, X_2, \\ldots, X_n \\right) is a multivariate Gaussian \\mathbf{X} \\sim \\mathcal{N} \\left( \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma} \\right) if\n\nf_{\\mathbf{X}} \\left( \\mathbf{x} \\right) = \\frac{1}{\\left( 2 \\pi \\right)^{n/2}} \\left|\\boldsymbol{\\Sigma} \\right|^{-\\frac{1}{2}} exp \\left(-\\frac{1}{2}\\left( \\mathbf{x}-\\boldsymbol{\\mu}\\right)^{T} \\boldsymbol{\\Sigma}^{-1} \\left( \\mathbf{x}-\\boldsymbol{\\mu}\\right) \\right)\n\nwhere\n\n\\boldsymbol{\\Sigma} is a n \\times n covariance matrix\n\\boldsymbol{\\mu} = \\left( \\mu_1, \\mu_2, \\ldots, \\mu_{n} \\right)^{T} is a n \\times 1 mean vector."
  },
  {
    "objectID": "slides/lecture-5/index.html#multivariate-gaussians-2",
    "href": "slides/lecture-5/index.html#multivariate-gaussians-2",
    "title": "Lecture 5",
    "section": "Multivariate Gaussians",
    "text": "Multivariate Gaussians\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nHere\n\n\\boldsymbol{\\mu}=\\left(\\begin{array}{c}\n-2\\\\\n1\n\\end{array}\\right),\\Sigma=\\left(\\begin{array}{cc}\n3 & 0\\\\\n0 & 6\n\\end{array}\\right)\n\nTry adding non-zero entries into the off-diagonal. Note the matrix must be symmetric!\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal\nfrom mpl_toolkits.mplot3d import Axes3D\nimport seaborn as sns\nsns.set(font_scale=1.0)\nsns.set_style(\"white\")\nsns.set_style(\"ticks\")\npalette = sns.color_palette('deep')\n\n#Parameters to set\nmu_x = -2\nvariance_x = 3\n\nmu_y = 1\nvariance_y = 6\n\n#Create grid and multivariate normal\nx = np.linspace(-10,10,500)\ny = np.linspace(-10,10,500)\nX, Y = np.meshgrid(x,y)\npos = np.empty(X.shape + (2,))\npos[:, :, 0] = X; pos[:, :, 1] = Y\nrv = multivariate_normal([mu_x, mu_y], [[variance_x, 0], [0, variance_y]])\n\n#Make a 3D plot\nfig, ax = plt.subplots(subplot_kw=dict(projection='3d'), figsize=(7,8))\nax.plot_surface(X, Y, rv.pdf(pos),cmap='viridis',linewidth=0)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.set_title(r'Probability density, $f_{\\mathbf{X}} (\\mathbf{x})$')\nplt.close()"
  },
  {
    "objectID": "slides/lecture-5/index.html#multivariate-gaussians-3",
    "href": "slides/lecture-5/index.html#multivariate-gaussians-3",
    "title": "Lecture 5",
    "section": "Multivariate Gaussians",
    "text": "Multivariate Gaussians\n\nRemember that \\mathbf{X} is a random vector and its possible values \\mathbf{x} are also vectors.\nThe density, f_{\\mathbf{X}} \\left( \\mathbf{x} \\right), is a scalar-valued function.\nThe coefficient \n\\frac{1}{\\left( 2 \\pi \\right)^{n/2}} \\left|\\boldsymbol{\\Sigma} \\right|^{-\\frac{1}{2}}\n acts as a normalizing constant."
  },
  {
    "objectID": "slides/lecture-5/index.html#covariance-matrix",
    "href": "slides/lecture-5/index.html#covariance-matrix",
    "title": "Lecture 5",
    "section": "Covariance matrix",
    "text": "Covariance matrix\n\nElements of the covariance matrix have the following form:\n\n\n\\left[ \\boldsymbol{\\Sigma} \\right]_{ij} = \\mathbb{E} \\left[ \\left( X_i - \\mu_{i} \\right) \\left( X_j - \\mu_{j} \\right) \\right] = \\mathbb{E} \\left[ X_i X_j \\right] - \\mu_{i}\\mu_{j}.\n\\tag{1}\n\nFollowing Equation 1 it is clear that the diagonal elements are simply the individual variances:\n\n\n\\left[ \\boldsymbol{\\Sigma} \\right]_{ii}  = \\mathbb{E} \\left[ X^2_i \\right] - \\mu^2_{i} = Var \\left(X_i \\right).\n\n\nThe matrix is symmetric with off-diagonal terms being zero when two components X_i and X_j are independent, i.e., \\left[ \\boldsymbol{\\Sigma} \\right]_{ij} = \\mathbb{E} \\left[ X_i \\right] \\mathbb{E} \\left[ X_j \\right] - \\mu_{i} \\mu_{j} = 0."
  },
  {
    "objectID": "slides/lecture-5/index.html#covariance-matrix-1",
    "href": "slides/lecture-5/index.html#covariance-matrix-1",
    "title": "Lecture 5",
    "section": "Covariance matrix",
    "text": "Covariance matrix\n\nWhen the off-diagonal elements are not zero, i.e., when two components X_i and X_j are related, we can introduce a measure called the correlation coefficient\n\n\n\\rho_{ij} = \\frac{\\left[ \\boldsymbol{\\Sigma} \\right]_{ij} }{\\left( Var\\left(X_i \\right)Var\\left(X_j \\right) \\right)^{1/2} }.\n\n\nThis values satisfies -1 \\leq \\rho_{ij} \\leq 1, and depending upon the sign it is said to be either negatively correlated or positively correlated.\nWhen \\rho_{ij}=0, i.e., when there is no correlation, \\left[ \\boldsymbol{\\Sigma} \\right]_{ij}= 0."
  },
  {
    "objectID": "slides/lecture-5/index.html#marginal-distribution",
    "href": "slides/lecture-5/index.html#marginal-distribution",
    "title": "Lecture 5",
    "section": "Marginal distribution",
    "text": "Marginal distribution\n\nIt can be shown that the marginal density of any component \\left(X_1, \\ldots, X_n \\right) of a multivariate Gaussian is a univariate Gaussian.\nTo see this, consider that\n\n\nf_{X_k}\\left( x \\right) = \\int_{-\\infty}^{\\infty} \\ldots \\int_{-\\infty}^{\\infty} f_{\\mathbf{X}} \\left(\\mathbf{x} \\right) dx_1 dx_2 \\ldots dx_{k-1} dx_{k+1} \\ldots dx_{n}\n\n\n= \\frac{1}{\\sqrt{2 \\pi \\left[ \\boldsymbol{\\Sigma} \\right]_{kk} } } exp \\left( \\frac{\\left( x - \\mu_{k} \\right)^2}{2 \\left[ \\boldsymbol{\\Sigma} \\right]_{kk} } \\right)\n\n\nIn practice any partial marginalization of a multivariate Gaussian will yield another multivariate Gaussian (but with reduced dimensions)."
  },
  {
    "objectID": "slides/lecture-5/index.html#marginal-and-conditional-distribution",
    "href": "slides/lecture-5/index.html#marginal-and-conditional-distribution",
    "title": "Lecture 5",
    "section": "Marginal and conditional distribution",
    "text": "Marginal and conditional distribution\n\nLet \\mathbf{X} and \\mathbf{Y} be jointly Gaussian random vectors with marginals \n\\mathbf{X} \\sim \\mathcal{N}\\left(\\boldsymbol{\\mu}_{x}, \\mathbf{A} \\right), \\; \\; \\; \\text{and} \\; \\; \\; \\mathbf{Y} \\sim \\mathcal{N}\\left(\\boldsymbol{\\mu}_{y}, \\mathbf{B} \\right).\n\nWe can write the joint distribution as shown below\n\n\n\\left[\\begin{array}{c}\n\\mathbf{X}\\\\\n\\mathbf{Y}\n\\end{array}\\right]\\sim\\mathcal{N}\\left( \\underbrace{\\left[\\begin{array}{c}\n\\boldsymbol{\\mu}_{x}\\\\\n\\boldsymbol{\\mu}_{y}\n\\end{array}\\right]}_{\\boldsymbol{\\mu}}, \\underbrace{\\left[\\begin{array}{cc}\n\\mathbf{A} & \\mathbf{C}\\\\\n\\mathbf{C}^{T} & \\mathbf{B}\n\\end{array}\\right]}_{\\boldsymbol{\\Sigma}}\\right)\n\n\nThe conditional distribution of \\mathbf{X} given \\mathbf{Y} is\n\n\nf_{\\mathbf{X} | \\mathbf{Y}} \\left( \\mathbf{x}, \\mathbf{y} \\right) = \\mathcal{N} \\left( \\boldsymbol{\\mu}_{x} + \\mathbf{CB}^{-1} \\left(\\mathbf{y} - \\boldsymbol{\\mu}_{y} \\right), \\mathbf{A} - \\mathbf{CB}^{-1} \\mathbf{C}^{T} \\right)\n\n\nAlgebraically, this uses the Schur complement. To explore this further, consider the following schematic."
  },
  {
    "objectID": "slides/lecture-5/index.html#marginal-and-conditional-distribution-1",
    "href": "slides/lecture-5/index.html#marginal-and-conditional-distribution-1",
    "title": "Lecture 5",
    "section": "Marginal and conditional distribution",
    "text": "Marginal and conditional distribution\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nThe joint multivariate Gaussian distribution to the left has mean and covariance:\n\n\\boldsymbol{\\mu}=\\left(\\begin{array}{c}\n0.5\\\\\n0.2\n\\end{array}\\right),\\Sigma=\\left(\\begin{array}{cc}\n1.5 & -1.27\\\\\n-1.27 & 3\n\\end{array}\\right)\n\nAs an example, we wish to work out what f_{X| Y} \\left( x, y=3.7 \\right) is (see code).\nThe conditional is Gaussian!\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal, norm\nimport pandas as pd\nimport seaborn as sns\nsns.set(font_scale=1.0)\nsns.set_style(\"white\")\nsns.set_style(\"ticks\")\npalette = sns.color_palette('deep')\n\nvar_1 = 1.5\nvar_2 = 3.0\nrho = -0.6\noff_diag = np.sqrt(var_1 * var_2) * rho\n\n\nmu = np.array([0.5, 0.2])\ncov = np.array([[var_1, off_diag], \\\n       [off_diag, var_2]])\n\nrv = multivariate_normal(mu, cov)\n\n# Generate random samples from this multivariate normal (largely for plotting!)\ndata = rv.rvs(8500)\ndf = pd.DataFrame({'$x$': data[:,0].flatten(), '$y$': data[:,1].flatten()})\n\n# Now, to plot the conditional distribution of $X_1$ at $X_2=5.0$, we would have\ndef calculate_conditional(mu, cov, yy):\n    new_mu = mu[0] + cov[0,1] * (cov[1,1])**(-1) * (yy - mu[1])\n    new_var =  cov[0,0] - cov[0,1] * (cov[1,1])**(-1) * cov[0,1]\n    return new_mu, new_var\n\ny_new = 3.7\ncond_mu, cond_var = calculate_conditional(mu, cov, y_new)\n\n# Now, to plot the conditional distribution of $X_1$ at $X_2=5.0$, we would have\ndef calculate_conditional(mu, cov, yy):\n    new_mu = mu[0] + cov[0,1] * (cov[1,1])**(-1) * (yy - mu[1])\n    new_var =  cov[0,0] - cov[0,1] * (cov[1,1])**(-1) * cov[0,1]\n    return new_mu, new_var\n\ny_new = 3.7\ncond_mu, cond_var = calculate_conditional(mu, cov, y_new)\n\nX_samples = np.tile( np.linspace(-10, 10, 200).reshape(200,1) , (1, 2))\nX_samples[:,1] = X_samples[:,1]* 0 + y_new\n\nf_X = rv.pdf(X_samples)\nrv2 = multivariate_normal(cond_mu, cond_var)\nf_X1 = rv2.pdf(X_samples[:,0])\n\n# Plot!\ng = sns.JointGrid(data=df, x=\"$x$\", y=\"$y$\", space=0)\ng.plot_joint(sns.kdeplot, fill=True,  cmap=\"turbo\", thresh=0, levels=100)\ng.plot_marginals(sns.kdeplot, color=\"grey\", gridsize=100)\nplt.close()\n\nfig = plt.figure(figsize=(8,3))\nplt.plot(X_samples[:,0], f_X1, 'r-')\nplt.xlabel('$x$')\nplt.title('Conditional distribution of $x$ at $y=3.7$')\nplt.close()"
  },
  {
    "objectID": "slides/lecture-5/index.html#generating-samples",
    "href": "slides/lecture-5/index.html#generating-samples",
    "title": "Lecture 5",
    "section": "Generating samples",
    "text": "Generating samples\nIt will be useful to generate samples from a multivariate Gaussian. To understand how to do this, consider the following setup.\n\nLet \\mathbf{X} \\sim \\mathcal{N} \\left(\\mathbf{0}, \\mathbf{I}\\right). Thus, \\mathbb{E} \\left[ \\mathbf{X} \\right] = \\mathbf{0}, and Cov\\left[ \\mathbf{X} \\right] = \\mathbf{I}.\nNow consider the map given by \\tilde{\\mathbf{x}} = \\mathbf{S} \\mathbf{x} + \\mathbf{b}, where \\mathbf{x} is a particular value from the random variable \\mathbf{X}, where\n\n\\mathbf{S} \\in \\mathbb{R}^{n \\times n} is a matrix;\n\\mathbf{b} \\in \\mathbb{R}^{n} is a vector.\n\nBy linearity of the expectation we can show that\n\n\n\\mathbb{E} \\left[ \\tilde{\\mathbf{X}} \\right] = \\mathbf{b}, \\; \\; \\; \\text{and} \\; \\; \\; Cov   \\left[ \\tilde{\\mathbf{X}} \\right] = \\mathbf{SS}^{T}.\n\n\nThe distribution \\mathcal{N}\\left( \\mathbf{b}, \\mathbf{SS}^{T} \\right) is valid (i.e., it is Gaussian), only if \\mathbf{S} is non-singular, i.e., \\mathbf{SS}^{T} is positive definite.\nIn practice, if we need to generate samples from \\mathcal{N}\\left( \\mathbf{b}, \\mathbf{B} \\right), we would compute the Cholesky decomposition of \\mathbf{B}= \\mathbf{LL}^{T}, and then use \\tilde{\\mathbf{x}} = \\mathbf{b} + \\mathbf{L} \\mathbf{x}.\n\n\n\nAE8803 | Gaussian Processes for Machine Learning"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Overview",
    "section": "",
    "text": "This graduate-level course offers a practical approach to probabilistic learning with Gaussian processes (GPs). GPs represent a powerful set of methods for modeling and predicting a wide variety of spatio-temporal phenomena. Today, they are used for problems that span both regression and classification, with theoretical foundations in Bayesian inference, reproducing kernel Hilbert spaces, eigenvalue problems, and numerical integration. Rather than focus solely on these theoretical foundations, this course balances theory with practical probabilistic programming, using a variety of python-based packages. Moreover, practical engineering problems will also be discussed that see GP models that cut across other areas of machine learning including transfer learning, convolutional networks, and normalizing flows."
  },
  {
    "objectID": "index.html#grading",
    "href": "index.html#grading",
    "title": "Overview",
    "section": "Grading",
    "text": "Grading\nThis course has four assignments; the grades are given below:\n\n\n\n\n\n\n\nAssignment\nGrade percentage (%)\n\n\n\n\nAssignment 1: Mid-term (covering fundamentals)\n20\n\n\nAssignment 2: Build your own GP from scratch for a given dataset\n20\n\n\nAssignment 3: Proposal (data and literature review)\n20\n\n\nAssignment 4: Final project (presentation and notebook)\n40\n\n\n\n\nPre-requisites:\n\nCS1371, MATH2551, MATH2552 (or equivalent)\nWorking knowledge of python including familiarity with numpy and matplotlib libraries.\nWorking local version of python and Jupyter."
  },
  {
    "objectID": "index.html#lectures",
    "href": "index.html#lectures",
    "title": "Overview",
    "section": "Lectures",
    "text": "Lectures\nBelow you will find a list of the lectures that form the backbone of this course. Sub-topics for each lecture will be updated in due course.\n01.08: L1. Introduction & probability fundamentals | Slides | Examples\n\n\n\nContents\n\n\nCourse overview.\nProbability fundamentals (and Bayes’ theorem).\nRandom variables.\n\n\n01.10: L2. Discrete probability distributions | Slides | Examples | Notebook\n\n\nContents\n\n\nExpectation and variance.\nIndependence.\nBernoulli and Binomial distributions.\n\n\n01.15: No Class (Institute Holiday)\n01.17: L3. Continuous distributions | Slides | Examples\n\n\n\nContents\n\n\nFundamentals of continuous random variables.\nProbability density function.\nGaussian and Beta distributions.\n\n\n01.22: L4. Manipulating and combining distributions | Slides | Examples\n\n\nContents\n\n\nFunctions of random variables.\nSums of random variables.\n\n\n01.24: No Class\n01.29: L5. Multivariate Gaussian distributions | Slides\n\n\nContents\n\n\nMarginal distributions.\nConditional distributions.\nJoint distribution and Schur complement.\n\n\n01.31: L6. Linear modelling | Slides\n\n\nContents\n\n\nLeast squares.\nRegularization.\nGaussian noise model.\n\n\n02.05: L7. Gaussian process regression\n\n\nContents\n\n\nContrast weight-space vs function-space perspective.\nIntroduction to a kernel.\nLikelihood and prior for a Gaussian process.\nPosterior mean and covariance.\n\n\n02.07: L8. Hyperparameters and model selection\n\n\nContents\n\n\nMaximum likelihood and maximum aposteriori estimate.\nCross validation.\nExpectation maximization.\nMarkov chain Monte Carlo (Gibbs, NUTS, HMC).\n\n\n02.12: Fundamentals Mid-term\n02.14: L9. Variational inference\n\n\nContents\n\n\nVariational problem.\nDeriving the ELBO.\nStochastic variational inference in practice.\n\n\n02.19: L10. Open-source resources\n\n\nContents\n\n\npymc.\ngpytorch, gpflow.\nGPjax.\n\n\n02.21: L11. Kernel learning\n\n\nContents\n\n\nKernel trick re-visited. 2. Constructing kernels piece-by-piece. 3. Constructing kernels from learnt features. 4. Spectral representations of kernels.\n\n\n02.26: L12. Gaussian process classification\n\n\nContents\n\n\nBernoulli prior\nSoftmax for multi-class classification\n\n\n02.28: L13. Scaling up Gausssian processes I\n\n\nContents\n\n\nReview of matrix inverse via Cholesky.\nSubset of data approaches\nNystrom approximation\nInducing points\nKronecker product kernels.\n\n\n03.04: L14. Scaling up Gausssian processes II\n\n\nContents\n\n\nVariational inference\nELBO derivation\nMinimizing the KL-divergence practically using Adam.\n\n\n03.06: Coding assignment due\n03.06: L15. Sparse (and subspace-based) Gaussian processes\n\n\nContents\n\n\nBrief introduction to matrix manifolds.\nSubspace-based projections.\nActive subspaces.\nSparsity promoting priors.\n\n\n03.11: L16. Reproducing Kernel Hilbert Spaces\n\n\nContents\n\n\nProject overview\nHilbert space\nUnderstanding a kernel.\nReproducing kernel Hilbert spaces.\nRepresenter theoreom.\n\n\n03.13: L17. Multi-output and deep Gaussian processes\n\n\nContents\n\n\nCoregional models.\nTransfer learning across covariance blocks.\nDerivative (or gradient) enhancement.\nDepth in Gaussian processes.\nPosterior inference and stochastic variational inference\n\n\n03.13: Withdrawal Deadline\n03.18-03.22: Spring Break\n03.25: Project proposals due\n03.25: L19. Convolutional Gaussian processes\n\n\nContents\n\n\nConvolution as a linear operator.\nDeep convolutional Gaussian processes.\n\n\n03.27: L20. Latent models and unsupervised learning\n\n\nContents\n\n\nContrast standard regression with latent variable model.\nGaussian process latent variable model.\nCoding demo.\n\n\n04.01: L21. State-space Gaussian processes\n\n\nContents\n\n\nApplication: time series models.\nGaussian state space model.\nParallels with Kalman filtering and smoothing.\nCreating custom state-space kernels.\n\n\n04.03: L22. Bayesian optimization\n\n\nContents\n\n\nGaussian process surrogate.\nAcquisition function.\nThompson’s sampling.\nGaussian process dynamic model.\n\n\n04.08: L23. Guest Lecture\n04.22: L24. Project presentations"
  },
  {
    "objectID": "index.html#office-hours",
    "href": "index.html#office-hours",
    "title": "Overview",
    "section": "Office hours",
    "text": "Office hours\nProfessor Seshadri’s office hours:\n\n\n\nLocation\nTime\n\n\n\n\nMK 421\nFridays 14:30 to 15:30"
  },
  {
    "objectID": "index.html#textbooks",
    "href": "index.html#textbooks",
    "title": "Overview",
    "section": "Textbooks",
    "text": "Textbooks\nThis course will make heavy use of the following texts:\n\nRasmussen, C. E., Williams, C. K. Gaussian Processes for Machine Learning, The MIT Press, 2006.\nMurphy, K. P., Probabilistic Machine Learning: Advanced Topics, The MIT Press, 2023.\n\nBoth these texts have been made freely available by the authors."
  },
  {
    "objectID": "index.html#important-papers",
    "href": "index.html#important-papers",
    "title": "Overview",
    "section": "Important papers",
    "text": "Important papers\nStudents are encouraged to read through the following papers:\n\nRoberts, S., Osborne, M., Ebden, M., Reece, S., Gibson, N., Aigrain, S., (2013) Gaussian processes for time-series modelling, Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences.\nDunlop, M., Girolami, M., Stuart, A., Teckentrup, A., (2018) How Deep Are Deep Gaussian Processes?, Journal of Machine Learning Research 19, 1-46\nAlvarez, M., Lawrence, N., (2011) Computationally Efficient Convolved Multiple Output Gaussian Processes, Journal of Machine Learning Research 12, 1459-1500\nVan der Wilk, M., Rasmussen, C., Hensman, J., (2017) Convolutional Gaussian Processes, 31st Conference on Neural Information Processing Systems"
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Overview",
    "section": "References",
    "text": "References\nMaterial used in this course has been adapted from\n\nCUED Part IB probability course notes\nAlto University’s module on Gaussian Processes\nSlides from the Gaussian Process Summer Schools"
  },
  {
    "objectID": "slides/lecture-6/index.html#the-three-model-levels",
    "href": "slides/lecture-6/index.html#the-three-model-levels",
    "title": "Lecture 6",
    "section": "The three model levels",
    "text": "The three model levels\nIn this lecture, we will explore three distinct but related flavors of modelling.\n\nLinear least squares model\nGaussian noise model (introducing the idea of likelihood)\nFull Bayesian treatment (next time!)\n\n\nMuch of the exposition shown here is based on the first three chapters of Rogers and Girolami’s, A First Course in Machine Learning."
  },
  {
    "objectID": "slides/lecture-6/index.html#linear-least-squares",
    "href": "slides/lecture-6/index.html#linear-least-squares",
    "title": "Lecture 6",
    "section": "Linear least squares",
    "text": "Linear least squares\n\nConsider the data shown in the plot below. It shows the winning times for the men’s 100 meter race at the Summer Olympics for many years.\n\n\n\n\n\n\nOur goal will be to fit a model to this data. To begin, we will consider a linear model, i.e., \nt = f \\left( x; {\\color{blue}{w_0}}, {\\color{blue}{w_1}} \\right) = {\\color{blue}{w_0}} + {\\color{blue}{w_1}} x\n where x is the year and t is the winning time.\n{\\color{blue}{w_0}} and {\\color{blue}{w_1}} are unknown model parameters that we need to ascertain.\nGood sense would suggest that the best line passes as closely as possible through all the data points on the left."
  },
  {
    "objectID": "slides/lecture-6/index.html#linear-least-squares-1",
    "href": "slides/lecture-6/index.html#linear-least-squares-1",
    "title": "Lecture 6",
    "section": "Linear least squares",
    "text": "Linear least squares\nDefining a good model\n\nOne common strategy for defining this is based on the squared distance between the truth and the model. Thus, for a given year, t_i, this is written as: \n\\mathcal{L}_{i} = \\left( t_i - f \\left( x_i; {\\color{blue}{w_0}}, {\\color{blue}{w_1}} \\right) \\right)^2.\n\nHowever, as we want a model that fits well across all the data, we may consider the average across the entire data set, i.e., all N data points. This is given by: \n\\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^{N} \\mathcal{L}_{i} = \\frac{1}{N} \\sum_{i=1}^{N} \\left( t_i - f \\left( x_i; {\\color{blue}{w_0}}, {\\color{blue}{w_1}} \\right) \\right)^2\n\nNote that this loss function is always positive, and the lower it is the better! Finding optimal values for {\\color{blue}{w_0}}, {\\color{blue}{w_1}} can be expressed as \n\\underset{{\\color{blue}{w_0}}, {\\color{blue}{w_1}}}{argmin} \\; \\; \\frac{1}{N} \\sum_{i=1}^{N} \\left( t_i - f \\left( x_i; {\\color{blue}{w_0}}, {\\color{blue}{w_1}} \\right) \\right)^2\n\n\n\nNote that other loss functions can be considered. A common example is the absolute loss, i.e., \\mathcal{L}_{i} = | t_i - f \\left( x_i; {\\color{blue}{w_0}}, {\\color{blue}{w_1}} \\right)|"
  },
  {
    "objectID": "slides/lecture-6/index.html#linear-least-squares-2",
    "href": "slides/lecture-6/index.html#linear-least-squares-2",
    "title": "Lecture 6",
    "section": "Linear least squares",
    "text": "Linear least squares\nMatrix-vector notation\n\nIt will be very useful to work with vectors and matrices. For convenience, we define: \n\\mathbf{X}=\\left[\\begin{array}{cc}\n1 & x_{1}\\\\\n1 & x_{2}\\\\\n\\vdots & \\vdots\\\\\n1 & x_{N}\n\\end{array}\\right] = \\left[\\begin{array}{c}\n\\mathbf{x}_{1}^{T}\\\\\n\\mathbf{x}_{2}^{T}\\\\\n\\vdots \\\\\n\\mathbf{x}_{N}^{T}\n\\end{array}\\right], \\; \\; \\; \\; \\; \\mathbf{t} =\\left[\\begin{array}{c}\nt_{1}\\\\\nt_{2}\\\\\n\\vdots\\\\\nt_{N}\n\\end{array}\\right], \\; \\; \\; \\; \\; \\mathbf{{\\color{blue}{w}}} = \\left[\\begin{array}{c}\n{\\color{blue}{w_0}}\\\\\n{\\color{blue}{w_1}}\n\\end{array}\\right]\n\nThe loss function from the prior slide is equivalent to writing \n\\mathcal{L} = \\frac{1}{N} \\left( \\mathbf{t} - \\mathbf{X} \\mathbf{{\\color{blue}{w}}} \\right)^{T} \\left( \\mathbf{t} - \\mathbf{X} \\mathbf{{\\color{blue}{w}}} \\right).\n\nThis can be expanded to yield \n\\mathcal{L} = \\frac{1}{N} \\left( \\mathbf{t}^{T} - \\left( \\mathbf{X} \\mathbf{{\\color{blue}{w}}} \\right)^T  \\right)\\left( \\mathbf{t} - \\mathbf{X} \\mathbf{{\\color{blue}{w}}} \\right) = \\frac{1}{N} \\left[ \\mathbf{t}^{T} \\mathbf{t} - 2 \\mathbf{t}^{T} \\mathbf{X}  \\mathbf{{\\color{blue}{w}}} +   \\mathbf{{\\color{blue}{w}}}^T \\mathbf{X}^{T} \\mathbf{X} \\mathbf{{\\color{blue}{w}}} \\right]"
  },
  {
    "objectID": "slides/lecture-6/index.html#linear-least-squares-3",
    "href": "slides/lecture-6/index.html#linear-least-squares-3",
    "title": "Lecture 6",
    "section": "Linear least squares",
    "text": "Linear least squares\nMinimizing the loss\n\nAs our objective is to minimize the loss, the obvious idea is to find out for which \\mathbf{{\\color{blue}{w}}}, the derivative of the loss function, \\partial \\mathcal{L} / \\partial \\mathbf{{\\color{blue}{w}}}, goes to zero.\nNote that in practice, we refer to these points as turning points as they may equally correspond to maxima, minima, or saddle points. A positive second derivative is a sure sign of a minima.\nPrior to working out the derivatives, it will be useful to take note of the following identities on the left below.\n\n\n\n\n\n\n\n\n\n\ng \\left( \\mathbf{{\\color{red}{v}}} \\right)\n\\partial g / \\partial \\mathbf{{\\color{red}{v}}}\n\n\n\n\n\\mathbf{{\\color{red}{v}}}^{T}\\mathbf{x}\n\\mathbf{x}\n\n\n\\mathbf{x}^{T} \\mathbf{{\\color{red}{v}}}\n\\mathbf{x}\n\n\n\\mathbf{{\\color{red}{v}}}^{T} \\mathbf{{\\color{red}{v}}}\n2\\mathbf{{\\color{red}{v}}}\n\n\n\\mathbf{{\\color{red}{v}}}^{T} \\mathbf{C} \\mathbf{{\\color{red}{v}}}\n2\\mathbf{C} \\mathbf{{\\color{red}{v}}}\n\n\n\n\n\nThe derivative of the loss function is given by \n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{{\\color{blue}{w}}}} = - \\frac{2}{N} \\mathbf{X}^{T} \\mathbf{t} + \\frac{2}{N} \\mathbf{X}^{T} \\mathbf{X} \\mathbf{{\\color{blue}{w}}}\n\nSetting the derivative to zero, we have \n\\mathbf{X}^{T} \\mathbf{X} \\mathbf{{\\color{blue}{w}}} = \\mathbf{X}^{T} \\mathbf{t} \\; \\; \\; \\Rightarrow \\; \\; \\; \\hat{\\mathbf{{\\color{blue}{w}}}} = \\left( \\mathbf{X}^{T} \\mathbf{X} \\right)^{-1} \\mathbf{X}^{T} \\mathbf{t}\n where \\hat{\\mathbf{{\\color{blue}{w}}}} represents the value of \\mathbf{{\\color{blue}{w}}} that minimizes the loss."
  },
  {
    "objectID": "slides/lecture-6/index.html#linear-least-squares-4",
    "href": "slides/lecture-6/index.html#linear-least-squares-4",
    "title": "Lecture 6",
    "section": "Linear least squares",
    "text": "Linear least squares\n\n\n\nPlotCode\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(font_scale=1.0)\nsns.set_style(\"white\")\nsns.set_style(\"ticks\")\npalette = sns.color_palette('deep')\n\ndf = pd.read_csv('notebooks/data/data100m.csv')\ndf.columns=['Year', 'Time']\nN = df.shape[0]\n\nmax_year, min_year = df['Year'].values.max() , df['Year'].values.min()\n\nx = (df['Year'].values.reshape(N,1) - min_year)/(max_year - min_year)\nt = df['Time'].values.reshape(N,1)\nX_func = lambda u : np.hstack([np.ones((u.shape[0],1)), u])\nX = X_func(x)\nw_hat = np.linalg.inv(X.T @ X) @ X.T @ t\nloss_func = 1./N * (t - X @ w_hat).T @ (t - X @ w_hat)\n\nxgrid = np.linspace(0, 1, 100).reshape(100,1)\nXg = X_func(xgrid)\ntime_grid = Xg @ w_hat\n\nfig = plt.figure(figsize=(6,4))\nplt.plot(df['Year'].values, df['Time'].values, 'o', color='crimson', label='Data')\nplt.plot(xgrid*(max_year - min_year) + min_year, time_grid, '-', color='dodgerblue', label='Model')\nplt.xlabel('Year')\nplt.ylabel('Time (seconds)')\nloss_title = r'Loss function, $\\mathcal{L}=$'+str(np.around(float(loss_func), 5))+'; \\t norm of $\\hat{\\mathbf{w}}$='+str(np.around(float(np.linalg.norm(w_hat,2)), 3))\nplt.title(loss_title)\nplt.legend()\nplt.savefig('olympics_0.png', dpi=150, bbox_inches='tight')\nplt.close()\n\n\n\n\n\n\nFor this result we set \n\\mathbf{X}=\\left[\\begin{array}{cc}\n1 & x_{1}\\\\\n1 & x_{2}\\\\\n\\vdots & \\vdots\\\\\n1 & x_{N}\n\\end{array}\\right]\n and solve for \\hat{\\mathbf{{\\color{blue}{w}}}} = \\left( \\mathbf{X}^{T} \\mathbf{X} \\right)^{-1} \\mathbf{X}^{T} \\mathbf{t}.\n\n\n\nOnce these weights are obtained, we can extrapolate (blue line) over the years.\n\n\n\nNote the graph title shows the loss function value and the L_2 norm, \\left\\Vert\\hat{\\mathbf{{\\color{blue}{w}}}}\\right\\Vert_{2}."
  },
  {
    "objectID": "slides/lecture-6/index.html#linear-least-squares-5",
    "href": "slides/lecture-6/index.html#linear-least-squares-5",
    "title": "Lecture 6",
    "section": "Linear least squares",
    "text": "Linear least squares\n\n\n\nPlotCode\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(font_scale=1.0)\nsns.set_style(\"white\")\nsns.set_style(\"ticks\")\npalette = sns.color_palette('deep')\n\ndf = pd.read_csv('notebooks/data/data100m.csv')\ndf.columns=['Year', 'Time']\nN = df.shape[0]\n\nmax_year, min_year = df['Year'].values.max() , df['Year'].values.min()\n\nx = (df['Year'].values.reshape(N,1) - min_year)/(max_year - min_year)\nt = df['Time'].values.reshape(N,1)\nX_func = lambda u : np.hstack([np.ones((u.shape[0],1)), u , u**2, u**3])\nX = X_func(x)\nw_hat = np.linalg.inv(X.T @ X) @ X.T @ t\nloss_func = 1./N * (t - X @ w_hat).T @ (t - X @ w_hat)\n\nxgrid = np.linspace(0, 1, 100).reshape(100,1)\nXg = X_func(xgrid)\ntime_grid = Xg @ w_hat\n\nfig = plt.figure(figsize=(6,4))\nplt.plot(df['Year'].values, df['Time'].values, 'o', color='crimson', label='Data')\nplt.plot(xgrid*(max_year - min_year) + min_year, time_grid, '-', color='dodgerblue', label='Model')\nplt.xlabel('Year')\nplt.ylabel('Time (seconds)')\nloss_title = r'Loss function, $\\mathcal{L}=$'+str(np.around(float(loss_func), 5))+'; \\t norm of $\\hat{\\mathbf{w}}$='+str(np.around(float(np.linalg.norm(w_hat,2)), 3))\nplt.title(loss_title)\nplt.legend()\nplt.savefig('olympics_3.png', dpi=150, bbox_inches='tight')\nplt.close()\n\n\n\n\n\n\nFor this result we set \n\\mathbf{X}=\\left[\\begin{array}{cccc}\n1 & x_{1} & x_{1}^2 & x_{1}^3\\\\\n1 & x_{2} & x_{2}^2 & x_{2}^3\\\\\n\\vdots & \\vdots & \\vdots & \\vdots\\\\\n1 & x_{N} & x_{N}^2 & x_{N}^3\n\\end{array}\\right]\n and solve for \\hat{\\mathbf{{\\color{blue}{w}}}} = \\left( \\mathbf{X}^{T} \\mathbf{X} \\right)^{-1} \\mathbf{X}^{T} \\mathbf{t}.\n\n\n\nOnce these weights are obtained, we can extrapolate (blue line) over the years.\n\n\n\nNote the graph title shows the loss function value and the L_2 norm, \\left\\Vert\\hat{\\mathbf{{\\color{blue}{w}}}}\\right\\Vert_{2}."
  },
  {
    "objectID": "slides/lecture-6/index.html#linear-least-squares-6",
    "href": "slides/lecture-6/index.html#linear-least-squares-6",
    "title": "Lecture 6",
    "section": "Linear least squares",
    "text": "Linear least squares\n\n\n\nPlotCode\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(font_scale=1.0)\nsns.set_style(\"white\")\nsns.set_style(\"ticks\")\npalette = sns.color_palette('deep')\n\ndf = pd.read_csv('notebooks/data/data100m.csv')\ndf.columns=['Year', 'Time']\nN = df.shape[0]\n\nmax_year, min_year = df['Year'].values.max() , df['Year'].values.min()\n\nx = (df['Year'].values.reshape(N,1) - min_year)/(max_year - min_year)\nt = df['Time'].values.reshape(N,1)\nX_func = lambda u : np.hstack([np.ones((u.shape[0],1)), u , u**2, u**3])\nX = X_func(x)\nw_hat = np.linalg.inv(X.T @ X) @ X.T @ t\nloss_func = 1./N * (t - X @ w_hat).T @ (t - X @ w_hat)\n\nxgrid = np.linspace(0, 1, 100).reshape(100,1)\nXg = X_func(xgrid)\ntime_grid = Xg @ w_hat\n\nfig = plt.figure(figsize=(6,4))\nplt.plot(df['Year'].values, df['Time'].values, 'o', color='crimson', label='Data')\nplt.plot(xgrid*(max_year - min_year) + min_year, time_grid, '-', color='dodgerblue', label='Model')\nplt.xlabel('Year')\nplt.ylabel('Time (seconds)')\nloss_title = r'Loss function, $\\mathcal{L}=$'+str(np.around(float(loss_func), 5))+'; \\t norm of $\\hat{\\mathbf{w}}$='+str(np.around(float(np.linalg.norm(w_hat,2)), 3))\nplt.title(loss_title)\nplt.legend()\nplt.savefig('olympics_8.png', dpi=150, bbox_inches='tight')\nplt.close()\n\n\n\n\n\n\nFor this result we set \n\\mathbf{X}=\\left[\\begin{array}{cccccc}\n1 & x_{1} & x_{1}^2 & x_{1}^3 & \\ldots & x_{1}^{8} \\\\\n1 & x_{1} & x_{1}^2 & x_{1}^3 & \\ldots & x_{1}^{8} \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\ldots & \\vdots \\\\\n1 & x_{N} & x_{N}^2 & x_{N}^3 & \\ldots & x_{N}^{8} \\\\\n\\end{array}\\right]\n and solve for \\hat{\\mathbf{{\\color{blue}{w}}}} = \\left( \\mathbf{X}^{T} \\mathbf{X} \\right)^{-1} \\mathbf{X}^{T} \\mathbf{t}.\n\n\n\nOnce these weights are obtained, we can extrapolate (blue line) over the years.\n\n\n\nNote the graph title shows the loss function value and the L_2 norm, \\left\\Vert\\hat{\\mathbf{{\\color{blue}{w}}}}\\right\\Vert_{2}."
  },
  {
    "objectID": "slides/lecture-6/index.html#linear-least-squares-7",
    "href": "slides/lecture-6/index.html#linear-least-squares-7",
    "title": "Lecture 6",
    "section": "Linear least squares",
    "text": "Linear least squares\nWith regularization\n\nThere is clearly a trade-off between the:\n\nComplexity of the model in terms of the number of weights, and\nthe value of the loss function.\n\nThere is also the risk of over-fitting to the data. For instance, if we had only 9 data points, then the last model would have interpolated each point, at the risk of not being generalizable.\nAs we do not want our model to be too complex, there are two relatively simple recipes:\n\nSplit the data into test and train (your homework!)\nAdd a regularization term, i.e., \n\\mathcal{L} = \\mathcal{L} + \\lambda \\mathbf{{\\color{blue}{w}}}^{T} \\mathbf{{\\color{blue}{w}}}\n where \\lambda is a constant."
  },
  {
    "objectID": "slides/lecture-6/index.html#maximum-likelihood",
    "href": "slides/lecture-6/index.html#maximum-likelihood",
    "title": "Lecture 6",
    "section": "Maximum likelihood",
    "text": "Maximum likelihood\n\nThe linear model from before is unable to capture each data point, and there are errors between the true data and the model.\nNow we will consider a paradigm where these errors are explicitly modelled.\nWe consider a model of the form \nt_j = f \\left( \\mathbf{x}_{n}; \\mathbf{{\\color{blue}{w}}} \\right) + \\epsilon_{n} \\; \\; \\; \\epsilon_{n} \\sim \\mathcal{N}\\left(0, \\sigma^2 \\right), \\; \\; \\; \\; \\text{where} \\; j \\in \\left[1, N \\right]\n\\tag{1}\nRecall, we had previously learnt that adding a constant to a Gaussian random variable alters its mean. Thus, the random variable t_j has a probability density function \np \\left( t_j | \\mathbf{x}_{j}, \\mathbf{{\\color{blue}{w}}}, \\sigma^2 \\right) = \\mathcal{N} \\left( \\mathbf{{\\color{blue}{w}}}^{T} \\mathbf{x}_{j} , \\sigma^2\\right)\n\nCarefully note the conditioning: the probability density function for t_j depends on particular values of \\mathbf{x}_{j} and \\mathbf{{\\color{blue}{w}}}."
  },
  {
    "objectID": "slides/lecture-6/index.html#maximum-likelihood-1",
    "href": "slides/lecture-6/index.html#maximum-likelihood-1",
    "title": "Lecture 6",
    "section": "Maximum likelihood",
    "text": "Maximum likelihood\nDefining the likelihood\nIf we evaluate the linear model from before, and assume that in Equation 1 \\sigma^2 = 0.05, we would find that \np \\left( t_j | \\mathbf{x}_{j} = \\left[ 1, 1980 \\right]^{T} ,\\mathbf{{\\color{blue}{w}}} = \\left[10.964, -1.31 \\right]^{T}, \\sigma^2 = 0.05 \\right) = \\mathcal{N} \\left( 10.03, 0.05 \\right)  \n This quantity is known as the likelihood of the n-th data point.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nNote that for a continuous random variable t, p\\left( t \\right) cannot be interpreted as a probability.\nThe height of the curve to the left tells us how likely it is that we observe a particular t for x=1980.\nThe most likely is B, followed by C and then A. Note the actual winning time is t_{n}=10.25.\nWhile we obviously cannot change the actual winning time, we can change \\mathbf{{\\color{blue}{w}}} and \\sigma^2 to move the density to make it as high as possible at t=10.25.\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom scipy.stats import multivariate_normal\n\n# Get the data \ndf = pd.read_csv('notebooks/data/data100m.csv')\ndf.columns=['Year', 'Time']\nN = df.shape[0]\nmax_year, min_year = df['Year'].values.max() , df['Year'].values.min()\nx = (df['Year'].values.reshape(N,1) - min_year)/(max_year - min_year)\nt = df['Time'].values.reshape(N,1)\nX_func = lambda u : np.hstack([np.ones((u.shape[0],1)), u])\nX = X_func(x)\nw_hat = np.linalg.inv(X.T @ X) @ X.T @ t\n\n# Specific year!\nyear_j = 1980\nX_j = X_func(np.array( [ (year_j - min_year) / (max_year - min_year) ] ).reshape(1,1) )\ntime_j = float(X_j @ w_hat)\n\nT_1980 = multivariate_normal(time_j, 0.05)\nti = np.linspace(9, 11, 100)\npt_x = T_1980.pdf(ti)\n\nfig = plt.figure(figsize=(7,3))\nplt.plot(ti, pt_x, '-', color='orangered', lw=3, label='From linear model')\nplt.axvline(9.53, linestyle='-.', color='dodgerblue', label='A')\nplt.axvline(10.08, linestyle='-', color='green', label='B')\nplt.axvline(10.40, linestyle='--', color='navy', label='C')\nplt.xlabel('Time (seconds)')\nplt.ylabel(r'$p \\left( t | x \\right)$')\nplt.title(r'For $x=1980$')\nplt.legend()\nplt.close()"
  },
  {
    "objectID": "slides/lecture-6/index.html#maximum-likelihood-2",
    "href": "slides/lecture-6/index.html#maximum-likelihood-2",
    "title": "Lecture 6",
    "section": "Maximum likelihood",
    "text": "Maximum likelihood\nDefining the likelihood\n\nThis idea of finding parameters that can maximize the likelihood is very important in machine learning.\nHowever, in general, we are seldom interested in the likelihood of an isolated data point – we are interested in the likelihood across all the data.\nThis leads to the conditional distribution across all N data points \np \\left( t_1, \\ldots, t_N | \\mathbf{x}_1, \\ldots, \\mathbf{x}_{N}, \\mathbf{{\\color{blue}{w}}}, \\sigma^2 \\right)\n\nIf we assume the noise at each data point is independent, this conditional density can be factorized into N separate terms \n\\mathcal{L} = p  \\left( \\mathbf{t} | \\mathbf{X}, \\mathbf{{\\color{blue}{w}}}, \\sigma^2 \\right) = \\prod_{j=1}^{N} p \\left( t_j | \\mathbf{x}_{j}, \\mathbf{{\\color{blue}{w}}}, \\sigma^2 \\right) = \\prod_{j=1}^{N} \\mathcal{N} \\left(\\mathbf{{\\color{blue}{w}}}^{T} \\mathbf{x}_{n} \\right).\n\\tag{2}\nNote that the t_j values are not completely independent—times have clearly decreased over the years! They are conditionally independent. For a given value of \\mathbf{{\\color{blue}{w}}} the t_j are independent; otherwise they are not.\nWe will now maximize the likelihood (see Equation 2 )."
  },
  {
    "objectID": "slides/lecture-6/index.html#maximum-likelihood-3",
    "href": "slides/lecture-6/index.html#maximum-likelihood-3",
    "title": "Lecture 6",
    "section": "Maximum likelihood",
    "text": "Maximum likelihood\nMaximizing the logarithm of the likelihood\n\nPlugging in the definition of a Gaussian probability density function into Equation 2 we arrive at \n\\mathcal{L} = \\prod_{j=1}^{N} \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} exp \\left( -\\frac{1}{2 \\sigma^2} \\left(t_j - f \\left( \\mathbf{x}_{j}; \\mathbf{{\\color{blue}{w}}} \\right) \\right)^2  \\right)\n\nTaking the logarithm on both sides and simplifying: \nlog \\left( \\mathcal{L} \\right)  = \\sum_{j=1}^{N} log \\left( \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} exp \\left( -\\frac{1}{2 \\sigma^2} \\left(t_j - f \\left( \\mathbf{x}_{j}; \\mathbf{{\\color{blue}{w}}} \\right) \\right)^2  \\right) \\right)\n \n= \\sum_{j=1}^{N}  \\left( -\\frac{1}{2} log \\left( 2 \\pi \\right) - log \\left(\\sigma \\right) - \\frac{1}{2\\sigma^2} \\left( t_j - f \\left(\\mathbf{x}_{j}; \\mathbf{{\\color{blue}{w}}} \\right) \\right)^2 \\right)\n \n= -\\frac{N}{2} log \\left( 2 \\pi \\right) - N \\; log \\left( \\sigma \\right) - \\frac{1}{2 \\sigma^2} \\sum_{j=1}^{N} \\left(t_j - f \\left(\\mathbf{x}_{j}; \\mathbf{{\\color{blue}{w}}} \\right) \\right)^2"
  },
  {
    "objectID": "slides/lecture-6/index.html#maximum-likelihood-4",
    "href": "slides/lecture-6/index.html#maximum-likelihood-4",
    "title": "Lecture 6",
    "section": "Maximum likelihood",
    "text": "Maximum likelihood\nMaximizing the logarithm of the likelihood\n\nJust as we did earlier, with the least squares solution, we can set the derivative of the logarithm of the loss function to be zero. \n\\frac{\\partial \\; log \\left( \\mathcal{L} \\right) }{\\partial \\mathbf{{\\color{blue}{w}}} } = \\frac{1}{\\sigma^2} \\sum_{j=1}^{N} \\mathbf{x}_{j} \\left( t_{j} - \\mathbf{x}_{j}^{T} \\mathbf{{\\color{blue}{w}}} \\right) = \\frac{1}{\\sigma^2} \\sum_{j=1}^{N} \\mathbf{x}_{j} t_j - \\mathbf{x}_{j} \\mathbf{x}_{j}^{T} \\mathbf{{\\color{blue}{w}}} \\equiv 0\n\nJust as we did before, we can use matrix vector notation to write this out as\n\n\n\\frac{\\partial \\; log \\left( \\mathcal{L} \\right) }{\\partial \\mathbf{{\\color{blue}{w}}} } = \\frac{1}{\\sigma^2} \\left( \\mathbf{X}^{T} \\mathbf{t} - \\mathbf{X}^{T} \\mathbf{X} \\mathbf{{\\color{blue}{w}}}\\right) = 0\n\n\nSolving this expression leads to\n\n\\mathbf{X}^{T} \\mathbf{t} - \\mathbf{X}^{T} \\mathbf{X} \\mathbf{{\\color{blue}{w}}} = 0 \\Rightarrow \\hat{\\mathbf{{\\color{blue}{w}}}} = \\left( \\mathbf{X}^{T} \\mathbf{X} \\right)^{-1} \\mathbf{X}^{T} \\mathbf{t}.\n\n Thus, the maximum likelihood solution for \\mathbf{{\\color{blue}{w}}} is exactly the solution for the least squares problem!  Minimizing the squared loss is equivalent to the maximum likelihood solution if the noise is assumed Gaussian."
  },
  {
    "objectID": "slides/lecture-6/index.html#maximum-likelihood-5",
    "href": "slides/lecture-6/index.html#maximum-likelihood-5",
    "title": "Lecture 6",
    "section": "Maximum likelihood",
    "text": "Maximum likelihood\nMaximizing the logarithm of the likelihood\n\nWhat remains now is to compute the maximum likelihood estimate of the noise, \\sigma. Assuming that \\hat{\\mathbf{{\\color{blue}{w}}}} = \\mathbf{{\\color{blue}{w}}} we can write \n\\frac{\\partial \\; log \\left( \\mathcal{L} \\right) }{\\partial \\sigma }  = - \\frac{N}{\\sigma} + \\frac{1}{\\sigma^3} \\sum_{j=1}^{N} \\left( t_j - \\mathbf{x}^{T} \\hat{\\mathbf{{\\color{blue}{w}}} } \\right)^2 \\equiv 0.\n\nRearranging, this yields \\hat{\\sigma^2} = 1/N \\sum_{j=1}^{N} \\left( t_j - \\mathbf{x}^{T} \\hat{\\mathbf{{\\color{blue}{w}}} }\\right).\nThis expression states that the variance is the averaged squared error, which intuitively makes sense. Re-writing this using matrix notation, we have \n\\hat{\\sigma^2} = \\frac{1}{N} \\left( \\mathbf{t} - \\mathbf{X} \\hat{\\mathbf{{\\color{blue}{w}}} } \\right)^{T}  \\left( \\mathbf{t} - \\mathbf{X} \\hat{\\mathbf{{\\color{blue}{w}}} } \\right) = \\frac{1}{N} \\left(  \\mathbf{t}^{T}  \\mathbf{t} - 2  \\mathbf{t}^{T} \\mathbf{X} \\hat{\\mathbf{{\\color{blue}{w}}} } + \\hat{\\mathbf{{\\color{blue}{w}}} }^{T} \\mathbf{X}^{T} \\mathbf{X} \\hat{\\mathbf{{\\color{blue}{w}}} } \\right)\n\n\nPlugging in \\hat{\\mathbf{{\\color{blue}{w}}}} = \\left( \\mathbf{X}^{T} \\mathbf{X} \\right)^{-1} \\mathbf{X}^{T} \\mathbf{t}, we arrive at \n\\hat{\\sigma^2} = \\frac{1}{N} \\left(   \\mathbf{t}^{T}   \\mathbf{t} -  \\mathbf{t}^{T}  \\mathbf{X} \\left( \\mathbf{X}^{T} \\mathbf{X} \\right)^{-1} \\mathbf{X}^{T} \\mathbf{t}\\right)"
  },
  {
    "objectID": "slides/lecture-6/index.html#maximum-likelihood-6",
    "href": "slides/lecture-6/index.html#maximum-likelihood-6",
    "title": "Lecture 6",
    "section": "Maximum likelihood",
    "text": "Maximum likelihood\nVisualizing the Gaussian noise model\n\n\n\nPlotCode\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(font_scale=1.0)\nsns.set_style(\"white\")\nsns.set_style(\"ticks\")\npalette = sns.color_palette('deep')\n\ndf = pd.read_csv('notebooks/data/data100m.csv')\ndf.columns=['Year', 'Time']\nN = df.shape[0]\n\nmax_year, min_year = df['Year'].values.max() , df['Year'].values.min()\n\nx = (df['Year'].values.reshape(N,1) - min_year)/(max_year - min_year)\nt = df['Time'].values.reshape(N,1)\nX_func = lambda u : np.hstack([np.ones((u.shape[0],1)), u ])\nX = X_func(x)\nw_hat = np.linalg.inv(X.T @ X) @ X.T @ t\nloss_func = 1./N * (t - X @ w_hat).T @ (t - X @ w_hat)\n\nxgrid = np.linspace(0, 1, 100).reshape(100,1)\nXg = X_func(xgrid)\ntime_grid = Xg @ w_hat\n\n\nxi = xgrid*(max_year - min_year) + min_year\nxi = xi.flatten()\nsigma_hat_squared = float( (1. / N) * (t.T @ t - t.T @ X @ w_hat) )\nsigma_hat = np.sqrt(sigma_hat_squared)\nyi = xi* 0 + sigma_hat\n\nloss_func =  -N/2 * np.log(np.pi * 2)  - N * np.log(sigma_hat) - \\\n                   1./(2 * sigma_hat_squared) * np.sum((X @ w_hat - t)**2) \n\nfig = plt.figure(figsize=(6,4))\na, = plt.plot(df['Year'].values, df['Time'].values, 'o', color='crimson', label='Data')\nplt.plot(xi, time_grid, '-', color='r')\nc = plt.fill_between(xi, time_grid.flatten()-yi, time_grid.flatten()+yi, color='red', alpha=0.2, label='Model')\nplt.xlabel('Year')\nplt.ylabel('Time (seconds)')\nloss_title = r'Logarithm of loss function, $log \\left( \\mathcal{L} \\right)=$'+str(np.around(float(loss_func), 5))\nplt.title(loss_title)\nplt.legend([a,c], ['Data', 'Model'])\nplt.savefig('olympics_last.png', dpi=150, bbox_inches='tight')\nplt.close()\n\n\n\n\n\nThe graph on the left is the final model.\n\n\n\n\nAE8803 | Gaussian Processes for Machine Learning"
  },
  {
    "objectID": "sample_problems/lecture_2.html",
    "href": "sample_problems/lecture_2.html",
    "title": "L2 examples",
    "section": "",
    "text": "Commercial airline pilots need to pass four out of five separate tests for certification. Assume that the tests are equally difficult, and that the performance on separate tests are independent.\n\nIf the probability of failing each separate test is \\(p=0.2\\), then what is the probability of failing certification?\nTo improve safety, more stringent regulations require that pilots pass all five tests. To be able to meet the demand, the individual tests are made easier. What should the new individual failure rate be if the overall certification probability is to remain unchanged?\n\n\n\nSolution\n\n\nGiven that each test is independent, the combined probabilities follow a Binomial distribution. A pilot will fail certification if they fail two or more tests, and they will pass if they fail zero or one of the individual tests. Thus, the probability of passing certification is\n\n\\[\n\\large\np_{pass} = \\left(\\begin{array}{c}\n5\\\\\n0\n\\end{array}\\right) p^{0} \\left( 1 - p \\right)^{5} + \\left(\\begin{array}{c}\n5\\\\\n1\n\\end{array}\\right)p^{1} \\left( 1 - p \\right)^{4}\n\\]\nFrom the code snippet below this is roughly 0.737. Thus the combined failure rate is \\(1 - 0.7373 = 0.2627\\).\n\nUnder the new certification protocol, as there is no possibility of failing a test, we have\n\n\\[\n\\large\n\\left(1 - p_{fail, new} \\right)^{5} = 1 - 0.2627 \\Rightarrow p_{fail, new} = 0.06\n\\]\n\n\n\nCode\nfrom scipy.special import comb\nimport numpy as np\n\n# part a.\np = 0.2\np_pass = comb(5, 0) * p**0 * (1 - p)**5 + comb(5, 1) * p**1 * (1 - p)**4\np_fail = 1 - p_pass\nprint(p_fail)\n\n# part b.\np_fail_new = 1 - (1 - p_fail)**(1/5)\nprint(p_fail_new)\n\n\n0.26271999999999984\n0.059136781980261066"
  },
  {
    "objectID": "sample_problems/lecture_2.html#problem-1",
    "href": "sample_problems/lecture_2.html#problem-1",
    "title": "L2 examples",
    "section": "",
    "text": "Commercial airline pilots need to pass four out of five separate tests for certification. Assume that the tests are equally difficult, and that the performance on separate tests are independent.\n\nIf the probability of failing each separate test is \\(p=0.2\\), then what is the probability of failing certification?\nTo improve safety, more stringent regulations require that pilots pass all five tests. To be able to meet the demand, the individual tests are made easier. What should the new individual failure rate be if the overall certification probability is to remain unchanged?\n\n\n\nSolution\n\n\nGiven that each test is independent, the combined probabilities follow a Binomial distribution. A pilot will fail certification if they fail two or more tests, and they will pass if they fail zero or one of the individual tests. Thus, the probability of passing certification is\n\n\\[\n\\large\np_{pass} = \\left(\\begin{array}{c}\n5\\\\\n0\n\\end{array}\\right) p^{0} \\left( 1 - p \\right)^{5} + \\left(\\begin{array}{c}\n5\\\\\n1\n\\end{array}\\right)p^{1} \\left( 1 - p \\right)^{4}\n\\]\nFrom the code snippet below this is roughly 0.737. Thus the combined failure rate is \\(1 - 0.7373 = 0.2627\\).\n\nUnder the new certification protocol, as there is no possibility of failing a test, we have\n\n\\[\n\\large\n\\left(1 - p_{fail, new} \\right)^{5} = 1 - 0.2627 \\Rightarrow p_{fail, new} = 0.06\n\\]\n\n\n\nCode\nfrom scipy.special import comb\nimport numpy as np\n\n# part a.\np = 0.2\np_pass = comb(5, 0) * p**0 * (1 - p)**5 + comb(5, 1) * p**1 * (1 - p)**4\np_fail = 1 - p_pass\nprint(p_fail)\n\n# part b.\np_fail_new = 1 - (1 - p_fail)**(1/5)\nprint(p_fail_new)\n\n\n0.26271999999999984\n0.059136781980261066"
  },
  {
    "objectID": "sample_problems/lecture_4.html",
    "href": "sample_problems/lecture_4.html",
    "title": "L4 examples",
    "section": "",
    "text": "Code\nimport numpy as np \nfrom scipy.stats import bernoulli, binom, expon\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.special import comb\nsns.set(font_scale=1.0)\nsns.set_style(\"white\")\nsns.set_style(\"ticks\")\npalette = sns.color_palette('deep')"
  },
  {
    "objectID": "sample_problems/lecture_4.html#problem-1",
    "href": "sample_problems/lecture_4.html#problem-1",
    "title": "L4 examples",
    "section": "Problem 1",
    "text": "Problem 1\nFind the probability density function of \\(Y = h \\left( X \\right) = X^2\\), for any \\(y &gt; 0\\), where \\(X\\) is a continuous random variable with a known probability density function.\n\n\nSolution\n\nFor \\(y &gt; 0\\) we have\n\\[\nF_Y \\left( y \\right) = p \\left( Y \\leq y \\right) = p \\left( X^2 \\leq y \\right) = p \\left( - \\sqrt{y} \\leq X \\leq \\sqrt{y} \\right)\n\\]\n\\[\n\\Rightarrow F_Y \\left( y \\right) = F_{X} \\left( \\sqrt{y} \\right) - F_{X} \\left( - \\sqrt{y} \\right)\n\\]\nThus, by differentiating and applying the chain rule we have\n\\[\nf_{Y} \\left( y \\right) = \\frac{1}{2\\sqrt{y}} f_{X} \\left( \\sqrt{y} \\right) + \\frac{1}{2 \\sqrt{y}} f_{X} \\left( - \\sqrt{y} \\right), \\; \\; \\; \\; y &gt; 0\n\\]"
  },
  {
    "objectID": "sample_problems/lecture_4.html#problem-2",
    "href": "sample_problems/lecture_4.html#problem-2",
    "title": "L4 examples",
    "section": "Problem 2",
    "text": "Problem 2\nFind the probability density function of \\(Y = exp \\left( X^2 \\right)\\) if \\(X\\) is a non-negative random variable.\n\n\nSolution\n\nNote that \\(F_Y \\left( y \\right) = 0\\) for \\(y &lt; 1\\). For \\(y \\geq 1\\), we have\n\\[\nF_{Y} \\left( y \\right) = p \\left(exp \\left(X^2 \\right) \\leq y \\right) = p \\left(X^2 \\leq log \\left( y \\right) \\right)\n\\]\n\\[\n\\Rightarrow F_{Y} = p \\left( X \\leq \\sqrt{log \\left( y \\right) } \\right).\n\\]\nBy differentiating and using the chain rule, we obtain\n\\[\nf_{Y} \\left( y \\right) = f_{X} \\left( \\sqrt{log \\left( y \\right) } \\right) \\frac{1}{2y \\sqrt{log \\left( y \\right) } }, \\; \\; \\; y &gt; 1.\n\\]\n\n\n\nCode\nlam = 1.\nX = expon.rvs(size=15000, scale=1/lam)\nY = np.sin(X)\n\nfig = plt.figure(figsize=(10,3))\nplt.subplot(121)\nplt.hist(X,40, density=True, color='crimson')\nplt.ylabel(r'$f_{X}(x)$')\nplt.xlabel('x')\nplt.subplot(122)\nplt.hist(Y,40, density=True, color='dodgerblue')\nplt.ylabel(r'$f_{Y}(y)$')\nplt.xlabel('y')\nplt.show()"
  },
  {
    "objectID": "sample_problems/lecture_4.html#problem-3",
    "href": "sample_problems/lecture_4.html#problem-3",
    "title": "L4 examples",
    "section": "Problem 3",
    "text": "Problem 3\nFollowing the bit of code above, let \\(X\\) be an exponential random variable with parameter \\(\\lambda\\), i.e., \\(f_{X} \\left( x \\right) = \\lambda exp \\left( -\\lambda x \\right)\\) and \\(F_{X} \\left( x \\right) = 1 - exp \\left( -\\lambda x \\right)\\). Let \\(Y= sin\\left( X \\right)\\). Determine \\(F_Y\\left( y \\right)\\) and \\(f_{Y} \\left( y \\right)\\).\n\n\nSolution\n\nFrom the event \\(\\left\\{ Y \\leq y \\right\\}\\), we can conclude that for \\(x = sin^{-1} \\left( y \\right)\\) we have\n\\[\nF_{Y} \\left( y \\right) = p \\left( Y \\leq y \\right)\n\\]\n\\[\nF_{Y} \\left( y \\right) = p \\left( X \\leq x \\right) + \\sum_{k=1}^{\\infty} \\left[ F_{X} \\left( 2 k \\pi + x \\right) - F_{X} \\left( \\left(2k - 1 \\right) \\pi - x\\right) \\right]\n\\]\n\\[\nF_{Y} \\left( y \\right) = p \\left( X \\leq x \\right) + \\sum_{k=1}^{\\infty} \\left[  1 - exp\\left( -\\lambda x - 2 \\lambda k \\pi \\right) - 1 + exp \\left( \\lambda x + \\lambda \\pi - 2 \\lambda k \\pi \\right) \\right]\n\\]\n\\[\n= p \\left( X \\leq x \\right) + \\left[ exp\\left( \\lambda x \\right) exp \\left( \\lambda \\pi \\right) - exp \\left( - \\lambda x \\right) \\right] \\sum_{k=1}^{\\infty}  exp \\left( - 2 \\lambda k \\pi \\right)\n\\]\n\\[\n= p \\left( X \\leq sin^{-1} \\left( y \\right) \\right) + \\left[ exp \\left( \\lambda sin^{-1} \\left( y \\right) + \\lambda \\pi \\right) - exp \\left( -\\lambda sin^{-1} \\left( y \\right) \\right) \\right] \\frac{exp \\left( -2 \\lambda \\pi \\right) }{1 - exp \\left( -2 \\lambda \\pi \\right)}\n\\]\nThis expansion uses the sum of a geometric sequence formula. The first term above is zero for negative \\(y \\in [-1, 0)\\) and\n\\[\np \\left( X \\leq sin^{-1} \\left( y \\right) \\right) = F_{X} \\left( sin^{-1} \\left( y \\right) \\right) = 1 - exp(- \\lambda sin^{-1}\\left( y \\right) )\n\\]\nfor non-negative \\(y \\in [0, 1]\\). Since \\(F_{X} \\left(0\\right) = 0\\), the cumulative probability \\(F_Y\\left( y \\right)\\) will remain continuous at \\(y=0\\). However, its derivative is discontinuous and we will be unable to derive an expression for \\(f_{Y} \\left( 0 \\right)\\). Hence, for negative \\(y \\in [-1, 0)\\) we have\n\\[\nf_{Y} \\left( y \\right) = \\frac{d}{dx} F_{X} \\left( x \\right) \\frac{dx}{dy} = \\frac{\\lambda}{\\sqrt{1 - y^2}} \\frac{exp \\left( \\lambda \\left(sin^{-1} \\left( y \\right) + \\pi \\right) \\right) + exp \\left( -\\lambda sin^{-1} \\left( y \\right) \\right) }{exp \\left( 2 \\lambda \\pi -1 \\right) }.\n\\]\nFor positive \\(y \\in (0, 1]\\), we have\n\\[\nf_{Y} \\left( y \\right) = \\frac{d}{dx} F_{X} \\left( x \\right) \\frac{dx}{dy} = \\frac{\\lambda}{\\sqrt{1 - y^2}} \\left[ \\frac{exp \\left( \\lambda \\left(sin^{-1} \\left( y \\right) + \\pi \\right) \\right) + exp \\left( -\\lambda sin^{-1} \\left( y \\right) \\right) }{exp \\left( 2 \\lambda \\pi -1 \\right) } + exp \\left( -\\lambda sin^{-1} \\left( y \\right) \\right) \\right].\n\\]\n\n\n\nCode\nx = np.linspace(-2.5, 2.5, 500)\ny = np.sin(x)\n\ndef f_y(y):\n    f_y = np.zeros((y.shape[0]))\n    for i in range(0, f_y.shape[0]):\n        if y[i] &gt; 0:\n            f_y[i] = lam/np.sqrt(1 - y[i]**2) * (np.exp(-lam * np.arcsin(y[i])) \\\n                        + (np.exp(lam * np.arcsin(y[i]) + lam * np.pi) + \\\n                          np.exp(-lam * np.arcsin(y[i])))/(np.exp(2 * lam * np.pi) - 1))\n        else:\n            f_y[i] = lam/np.sqrt(1 - y[i]**2) * ((np.exp(lam * np.arcsin(y[i]) + lam * np.pi) + \\\n                          np.exp(-lam * np.arcsin(y[i])))/(np.exp(2 * lam * np.pi) - 1))\n    return f_y\n\nfig = plt.figure(figsize=(6,4))\nplt.plot(y, f_y(y), color='navy', lw=3)\nplt.hist(Y,40, density=True, color='dodgerblue')\nplt.ylabel(r'$f_{Y}(y)$')\nplt.xlabel('y')\nplt.ylim([0, 2.7])\nplt.show()"
  },
  {
    "objectID": "sample_problems/lecture_4.html#problem-4",
    "href": "sample_problems/lecture_4.html#problem-4",
    "title": "L4 examples",
    "section": "Problem 4",
    "text": "Problem 4\nLet \\(X\\) and \\(Y\\) be independent and uniform between \\(0\\) and \\(1\\). Compute \\(X + Y\\). To set the stage for the problem, consider the code and plot below.\n\n\nCode\nX = np.random.rand(9000)\nY = np.random.rand(9000)\nS = X + Y \n\nfig = plt.figure(figsize=(6,3))\nplt.hist(X+Y,40, density=True, color='orangered')\nplt.ylabel(r'$f_{S}(s)$')\nplt.xlabel('s')\nplt.show()\n\n\n\n\n\nIt appears we have a triangular distribution. In what follows we shall aim to derive this analytically.\n\n\nSolution\n\nFrom the Lecture notes, we have:\n\\[\nf_{S} \\left( s \\right) = \\int_{0}^{1} f_{X} \\left( x \\right) f_{Y} \\left( s - x \\right) dx = \\int_{0}^{1} f_{Y} \\left( s - x \\right) dx\n\\]\n\\[\n\\Rightarrow f_{S} \\left( s \\right) = \\begin{cases}\n\\begin{array}{c}\n\\int_{0}^{s}1dx=s\\\\\n\\int_{s-1}^{1}1dx=2-s\n\\end{array} & \\begin{array}{c}\n\\textrm{for} \\; \\; s \\in [0, 1] \\\\\n\\textrm{for} \\; \\; s \\in [1, 2]\n\\end{array}\\end{cases}\n\\]"
  }
]