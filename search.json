[
  {
    "objectID": "coding-1/coding-1.html",
    "href": "coding-1/coding-1.html",
    "title": "Build your own GP from scratch for a given dataset",
    "section": "",
    "text": "Assignment 2\nThis assignment requires you to fit a Gaussian process model to the Mauna Loa data set. It is a univariate dataset that comprises the monthly average carbon dioxide concentration, measured in parts per million.\nYou will find the real data at this site. Details about the data can be found in reference 1.\nThe plot shows the data: Date vs. CO2 (ppm). Some rows of the table have -99.99 values; these may be ignored. Note that as there are 12 measurements per year (1 for each month), utilizing just the year as the covariate is not appropriate, and that is why the “Date” or third column must be used.\nYour training data must be limited to all years before 2014, i.e., you may only use CO2 concentrations in the years 1958 to 2013. It is entirely your decision whether you wish to use all this data, or select a subset.\nA plot of all the data is shown below.\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndf = pd.read_csv('data.csv')\ndf2014 = df[df['Date']&lt; 2014]\ndfnew = df[df['Date']&gt;= 2014]\n\nfig = plt.figure(figsize=(10,4))\nplt.plot(df2014['Date'].values, df2014['CO2'].values, 'o', ms=1, color='crimson', label='Pre 2014 (training)')\nplt.plot(dfnew['Date'].values, dfnew['CO2'].values, 'o', ms=1, color='dodgerblue', label='2014 and later')\nplt.legend()\nplt.axvline(x=2014, color=\"grey\")\nplt.xlabel('Year')\nplt.ylabel(r'$CO_2$ emissions (ppm)')\nplt.show()\n\n\n\n\n\nDespite the fact that this is a univariate dataset, it is challenging as it requires multiple kernel functions. Ten minutes on your favorite search browser will give you some clues. Your grade will be determined via the following criterion.\n\nAppropriate importing of the data and filtering of non-relevant rows. I will run your code on the “.csv” file as provided on the Scripps website. You cannot submit your amended version of the data.\nUse of multiple kernel functions, justifying what exactly each kernel is doing.\nA well-documented Jupyter notebook with equations for all the relevant formulas and code. If your code does not run, or produces an error upon running, you will loose a lot of marks.\n\nOne approach for hyperparameter inference (e.g., maximum likelihood, cross validation, Markov chain Monte Carlo, etc.). Please note that the signal noise need not be optimized over (but can be if you wish).\nYou will have to analytically calculate any gradients for hyperparameter inference. To clarify, code that does not use gradients, or code where the gradients are incorrect, will not receive full marks. To check your gradients you can always use finite differences.\n\nYou are restricted to the following libraries: numpy, seaborn, matplotlib, scipy, pandas. Thus, you will have to build a lot of the codebase yourself.\nThe last plot in your submission should have the same data as the plot above (both pre- and post-), along with predictive posterior mean and standard deviation contours.\n\n\nDue date: 15th March 2024 | 21:00 on Canvas.\nGrading rubric [marks in brackets]: - Data importing [5] - GP model architecture (i.e., kernels) [5] - Hyperparameter inference [10] - Clarity of documentation [5]\n\n\n\nReferences\n\nC. D. Keeling, S. C. Piper, R. B. Bacastow, M. Wahlen, T. P. Whorf, M. Heimann, and H. A. Meijer, Exchanges of atmospheric CO2 and 13CO2 with the terrestrial biosphere and oceans from 1978 to 2000. I. Global aspects, SIO Reference Series, No. 01-06, Scripps Institution of Oceanography, San Diego, 88 pages, 2001."
  },
  {
    "objectID": "useful_codes/fourier.html",
    "href": "useful_codes/fourier.html",
    "title": "Fourier analysis of kernels",
    "section": "",
    "text": "This note concerns the Fourier dual of a kernel, i.e., one can think of a kernel has having an associated set of frequencies and amplitudes, much like studying the spectral density or power spectrum of a signal.\n\n\nCode\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nimport plotly.express as px\nimport pandas as pd\nimport plotly.io as pio\nimport numpy as np\nfrom scipy.stats import multivariate_normal\nimport matplotlib.pyplot as plt\npio.renderers.default = 'iframe'\nfrom IPython.display import display, HTML\n\n\n\n\nA stationary function \\(k\\left( \\mathbf{x}, \\mathbf{x}' \\right)=k \\left( \\mathbf{x} - \\mathbf{x}' \\right) = k \\left( \\boldsymbol{\\tau} \\right)\\) can be represented as the Fourier transform of a positive finite measure. The formal statement is given by\nA complex-valued function \\(k\\) on \\(\\mathcal{X}\\) is the covariance function of a weakly stationary mean square continuous complex-valued random process on \\(\\mathcal{X}\\) if and only if it can be represented as\n\\[\nk \\left( \\boldsymbol{\\tau} \\right) = \\int_{\\mathcal{X}} exp \\left( 2 \\pi i \\boldsymbol{\\omega} \\cdot \\boldsymbol{\\tau} \\right) d \\mu \\left( \\boldsymbol{\\omega} \\right)\n\\]\nwhere \\(\\mu\\) is a positive finite measure, and \\(\\boldsymbol{\\omega}\\) are the frequencies. If \\(\\mu\\) has a density \\(S \\left( \\boldsymbol{\\omega} \\right)\\), then \\(S\\) is the spectral density or power spectrum associated with the kernel \\(k\\).\n\n\n\nA direct consequence of Bochner’s theoreom is the Wiener-Khintchine theorem. If the spectral density \\(S \\left( \\boldsymbol{\\omega} \\right)\\) exists, the spectral density and the covariance function are said to be Fourier duals. This leads to the following statement:\n\\[\nk \\left( \\boldsymbol{\\tau} \\right) = \\int S \\left( \\boldsymbol{\\omega} \\right) exp \\left( 2 \\pi i \\boldsymbol{\\omega} \\cdot \\boldsymbol{\\tau} \\right) d \\boldsymbol{\\omega}, \\; \\; \\; \\; S \\left( \\boldsymbol{\\omega}\\right) = \\int k \\left( \\boldsymbol{\\tau} \\right) exp\\left(- 2 \\pi i \\boldsymbol{\\omega} \\cdot \\boldsymbol{\\tau} \\right) d \\boldsymbol{\\tau}\n\\]\nAs noted in RW, \\(S \\left( \\boldsymbol{\\omega} \\right)\\) is essentially the amount of power assigned to the eigenfunction \\(exp \\left( 2 \\pi i \\boldsymbol{\\omega} \\cdot \\mathbf{\\tau} \\right)\\) with frequency \\(\\boldsymbol{\\omega}\\). The amplitude as a function of frequency \\(S\\left( \\boldsymbol{\\omega} \\right)\\) must decay sufficiently fast so that the terms above are integrable.\nThere are some important points to note:\n\nIf we have a stationary kernel, we can resolve what frequencies underscore the model by working out its Fourier transform.\nOn the other hand, if we have a certain spectral density of interest, then its inverse Fourier transform is a kernel.\n\nTo analytically work this out, it may be useful to go through an example (courtsey of Markus Heinonen). The derivation below will require three pieces: - We shall assume a symmetric frequency distribution, i.e., \\(S\\left( \\boldsymbol{\\omega} \\right) = S \\left( -\\boldsymbol{\\omega} \\right)\\). - From Euler’s formula we have \\(cos\\left(x\\right) \\pm i sin\\left(x \\right) = exp \\left(\\pm ix \\right)\\) - The negative sine identity, i.e., \\(sin \\left( -x \\right) = - sin \\left( x \\right)\\)\nStarting with the expression above, we begin wtih\n\\[\n\\begin{aligned}\nk \\left( \\boldsymbol{\\tau} \\right) & = \\int_{-\\infty}^{\\infty} S \\left( \\boldsymbol{\\omega} \\right) exp \\left( 2 \\pi i \\boldsymbol{\\omega} \\cdot \\boldsymbol{\\tau} \\right) d \\boldsymbol{\\omega} \\\\\n& =   \\int_{-\\infty}^{\\infty} S \\left(\\boldsymbol{\\omega} \\right) cos \\left( 2 \\pi\\boldsymbol{\\tau} \\cdot \\boldsymbol{\\omega} \\right) d \\boldsymbol{\\omega} + \\int_{-\\infty}^{\\infty} iS \\left(\\boldsymbol{\\omega} \\right) sin \\left( 2 \\pi \\boldsymbol{\\tau} \\cdot \\boldsymbol{\\omega} \\right) d \\boldsymbol{\\omega} \\\\\n& = \\mathbb{E}\\left[ S \\left(\\omega \\right) \\right] cos \\left( 2 \\pi \\boldsymbol{\\tau} \\cdot \\boldsymbol{\\omega} \\right)  + \\int_{-\\infty}^{0} iS \\left(\\boldsymbol{\\omega} \\right) sin \\left( 2 \\pi \\boldsymbol{\\tau} \\cdot \\boldsymbol{\\omega} \\right) d \\boldsymbol{\\omega} + \\int_{0}^{\\infty} iS \\left(\\boldsymbol{\\omega} \\right) sin \\left( 2 \\pi \\boldsymbol{\\tau} \\cdot \\boldsymbol{\\omega} \\right) d \\boldsymbol{\\omega} \\\\\n& = \\mathbb{E}\\left[ S \\left(\\omega \\right) \\right] cos \\left( 2 \\pi \\boldsymbol{\\tau} \\cdot \\boldsymbol{\\omega} \\right)  + \\int_{0}^{\\infty} iS \\left(-\\boldsymbol{\\omega} \\right) sin \\left( -2 \\pi \\boldsymbol{\\tau} \\cdot \\boldsymbol{\\omega} \\right) d \\boldsymbol{\\omega} + \\int_{0}^{\\infty} iS \\left(\\boldsymbol{\\omega} \\right) sin \\left( 2 \\pi \\boldsymbol{\\tau} \\cdot \\boldsymbol{\\omega} \\right) d \\boldsymbol{\\omega} \\\\\n& = \\mathbb{E}\\left[ S \\left(\\omega \\right) \\right] cos \\left( 2 \\pi \\boldsymbol{\\tau} \\cdot \\boldsymbol{\\omega} \\right)  + \\int_{0}^{\\infty} -iS \\left(\\boldsymbol{\\omega} \\right) sin \\left( 2 \\pi \\boldsymbol{\\tau} \\cdot \\boldsymbol{\\omega} \\right) d \\boldsymbol{\\omega} + \\int_{0}^{\\infty} iS \\left(\\boldsymbol{\\omega} \\right) sin \\left( 2 \\pi \\boldsymbol{\\tau} \\cdot \\boldsymbol{\\omega} \\right) d \\boldsymbol{\\omega} \\\\\n\\end{aligned}\n\\]\nThis leads to\n\\[\n\\begin{aligned}\nk \\left( \\boldsymbol{\\tau} \\right) & = \\mathbb{E}\\left[ S \\left(\\omega \\right) \\right] cos \\left( 2 \\pi \\boldsymbol{\\tau} \\cdot \\boldsymbol{\\omega} \\right)   \n\\end{aligned}\n\\]\nThis demonstrates that all real-valued stationary kernels are \\(S\\left( \\boldsymbol{\\omega} \\right)\\)-weighted combinations of cosine terms. \n\n\n\nOur new general stationary kernel definition is thus:\n\\[\nk \\left( \\boldsymbol{\\tau} \\right)  = \\mathbb{E}\\left[ S \\left(\\omega \\right) \\right] cos \\left( 2 \\pi \\boldsymbol{\\tau} \\cdot \\boldsymbol{\\omega} \\right)   \n\\]\nwhere the frequencies \\(\\boldsymbol{\\omega}\\) are an inverse of the period \\(1/\\boldsymbol{\\omega}\\). Bracewell provides the following expressions for the Wiener-Khintchine result, by integrating out the angular variables (see page 83 of RW):\n\\[\n\\begin{aligned}\nk \\left( \\boldsymbol{\\tau} \\right) & = \\frac{2 \\pi}{\\boldsymbol{\\tau}^{-1/2}} \\int_{0}^{\\infty} S \\left( \\boldsymbol{\\omega} \\right) J_{-1/2} \\left(2 \\pi  \\boldsymbol{\\tau} \\cdot \\boldsymbol{\\omega} \\right) \\boldsymbol{\\omega}^{1/2} d \\boldsymbol{\\omega} \\\\\nS \\left(   \\boldsymbol{\\omega} \\right) & = \\frac{2 \\pi}{\\boldsymbol{\\omega}^{-1/2}} \\int_{0}^{\\infty} k \\left( \\boldsymbol{\\tau} \\right) J_{-1/2} \\left(2 \\pi  \\boldsymbol{\\tau} \\cdot \\boldsymbol{\\omega} \\right) \\boldsymbol{\\tau}^{1/2} d \\boldsymbol{\\tau}\n\\end{aligned}\n\\]\nNote that in RW, the authors use \\(D\\) to denote the dimensionality, which we have assumed to be 1. The function \\(J_{-1/2}\\) is the Bessel function of order \\(-1/2\\). While the expressions above may seem unwiedly, we can work out what these are using a bit of Sympy. Consider the case of a squared exponential kernel of the form\n\\[\nk \\left(\\boldsymbol{\\tau} \\right) = exp \\left(- \\frac{\\boldsymbol{\\tau}^2}{2l^2} \\right).\n\\]\n\n\nCode\nfrom sympy import * \n\nomega = Symbol(\"omega\")\nell = Symbol(\"l\")\ntau = Symbol(\"tau\")\n\nkernel = exp(- tau**2 / (2 * ell**2))\nintegrate(2*pi*omega**(1/2) * kernel * besselj(-1/2, 2*pi*tau*omega)*tau**(1/2), (tau, 0, oo))\n\n\n\\(\\displaystyle \\begin{cases} 1.4142135623731 \\pi^{0.5} l^{1.0} e^{- 2 \\pi^{2} l^{2} \\omega^{2}} & \\text{for}\\: \\left(\\left|{\\arg{\\left(\\omega \\right)}}\\right| = 0 \\wedge \\left|{\\arg{\\left(l \\right)}}\\right| &lt; \\frac{\\pi}{4}\\right) \\vee \\left|{\\arg{\\left(l \\right)}}\\right| &lt; \\frac{\\pi}{4} \\\\\\int\\limits_{0}^{\\infty} 2 \\pi \\omega^{0.5} \\tau^{0.5} e^{- \\frac{\\tau^{2}}{2 l^{2}}} J_{-0.5}\\left(2 \\pi \\omega \\tau\\right)\\, d\\tau & \\text{otherwise} \\end{cases}\\)\n\n\nThe first expression above is the Fourier amplitude of the squared exponential kernel, i.e.,\n\\[\nS \\left(\\boldsymbol{\\omega} \\right) = \\left( 2 \\pi l^2\\right)^{1/2} exp \\left( - 2 \\pi l^2 \\boldsymbol{\\omega}^2 \\right)\n\\]\n\n\nCode\nomega = np.linspace(0, np.pi/4, 50)\nl = 0.5\nS_omega = (2 * np.pi * l**2)**(1/2) * \\\n            np.exp(- 2 * np.pi * l**2 * omega**2)\ntau = np.linspace(0, 10, 200)\n\n\nfig = go.Figure()\nfig.add_scatter(x=omega, y=S_omega, mode='lines')\nfig.update_layout(title='Spectral density', \\\n                  xaxis_title=r'Frequency, $\\omega$',\\\n                  yaxis_title=r'Spectral density, $S\\left( \\omega \\right) $')\nfig.show()\n\n\n\n\n\n\n\nCode\nkernel = tau * 0.\ntrue_kernel = np.exp(-tau**2 / l**2)\ncounter = 0.\n\nfig = go.Figure()\nfor omega_j in omega:\n    counter += 1.\n    label=str(np.around(int(counter), 1))+' terms'\n    S_omega_j = (2 * np.pi * l**2)**(1/2) * \\\n            np.exp(- 2 * np.pi * l**2 * omega_j**2)\n    cos_term = np.cos(2 * np.pi * tau * omega_j)\n    kernel += (S_omega_j * cos_term)\n    fig.add_scatter(x=tau, y=kernel * 1/counter, name=label, mode='lines')\nfig.add_scatter(x=tau, y=true_kernel, name='Kernel', mode='lines', \\\n                line=dict(width=4, color='black'))\nfig.update_layout(title='Sq. exp kernel Fourier representation', \\\n                  xaxis_title=r'Distance, $\\tau$',\\\n                  yaxis_title=r'$k ( \\tau )$')\nfig.show()\n\n\n\n\n\nNotice that the more terms we incorporate, the closer we converge to the true kernel.\n\n\n\nRather than negotiate a kernel approximation with a great many number of terms, it will be more instructive to resort to a few terms. Such is the idea behind Random Fourier Features, where one selects a kernel comprised of random frequencies. For more details, please see the paper by Rahimi and Recht.\n\n\nCode\nR = 500 # random features\nD = 50 # number of data pts.\nx = np.linspace(-2*np.pi, 2*np.pi, D).reshape(D,1) # grid\nX = np.tile(x, [1, D]) - np.tile(x.T, [D, 1])\nW    = np.random.normal(loc=0, scale=0.1, size=(R, D))\nb    = np.random.uniform(0, 2*np.pi, size=R)\nB    = np.repeat(b[:, np.newaxis], D, axis=1)\nnorm = 1./ np.sqrt(R)\nZ    = norm * np.sqrt(2) * np.cos(W @ X.T + B)\nZZ   = Z.T @ Z\n\n\n\n\nCode\nfig = plt.figure(figsize=(14,5))\nplt.subplot(121)\nd = plt.imshow(ZZ)\nplt.colorbar(d, shrink=0.3)\nplt.title('Random Fourier Features')\nnormal = multivariate_normal(np.zeros((D)), ZZ, allow_singular=True)\nplt.subplot(122)\nplt.plot(x, normal.rvs(10).T )\nplt.title('Random samples from prior')\nplt.xlabel('x')\nplt.show()"
  },
  {
    "objectID": "useful_codes/fourier.html#overview",
    "href": "useful_codes/fourier.html#overview",
    "title": "Fourier analysis of kernels",
    "section": "",
    "text": "This note concerns the Fourier dual of a kernel, i.e., one can think of a kernel has having an associated set of frequencies and amplitudes, much like studying the spectral density or power spectrum of a signal.\n\n\nCode\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nimport plotly.express as px\nimport pandas as pd\nimport plotly.io as pio\nimport numpy as np\nfrom scipy.stats import multivariate_normal\nimport matplotlib.pyplot as plt\npio.renderers.default = 'iframe'\nfrom IPython.display import display, HTML\n\n\n\n\nA stationary function \\(k\\left( \\mathbf{x}, \\mathbf{x}' \\right)=k \\left( \\mathbf{x} - \\mathbf{x}' \\right) = k \\left( \\boldsymbol{\\tau} \\right)\\) can be represented as the Fourier transform of a positive finite measure. The formal statement is given by\nA complex-valued function \\(k\\) on \\(\\mathcal{X}\\) is the covariance function of a weakly stationary mean square continuous complex-valued random process on \\(\\mathcal{X}\\) if and only if it can be represented as\n\\[\nk \\left( \\boldsymbol{\\tau} \\right) = \\int_{\\mathcal{X}} exp \\left( 2 \\pi i \\boldsymbol{\\omega} \\cdot \\boldsymbol{\\tau} \\right) d \\mu \\left( \\boldsymbol{\\omega} \\right)\n\\]\nwhere \\(\\mu\\) is a positive finite measure, and \\(\\boldsymbol{\\omega}\\) are the frequencies. If \\(\\mu\\) has a density \\(S \\left( \\boldsymbol{\\omega} \\right)\\), then \\(S\\) is the spectral density or power spectrum associated with the kernel \\(k\\).\n\n\n\nA direct consequence of Bochner’s theoreom is the Wiener-Khintchine theorem. If the spectral density \\(S \\left( \\boldsymbol{\\omega} \\right)\\) exists, the spectral density and the covariance function are said to be Fourier duals. This leads to the following statement:\n\\[\nk \\left( \\boldsymbol{\\tau} \\right) = \\int S \\left( \\boldsymbol{\\omega} \\right) exp \\left( 2 \\pi i \\boldsymbol{\\omega} \\cdot \\boldsymbol{\\tau} \\right) d \\boldsymbol{\\omega}, \\; \\; \\; \\; S \\left( \\boldsymbol{\\omega}\\right) = \\int k \\left( \\boldsymbol{\\tau} \\right) exp\\left(- 2 \\pi i \\boldsymbol{\\omega} \\cdot \\boldsymbol{\\tau} \\right) d \\boldsymbol{\\tau}\n\\]\nAs noted in RW, \\(S \\left( \\boldsymbol{\\omega} \\right)\\) is essentially the amount of power assigned to the eigenfunction \\(exp \\left( 2 \\pi i \\boldsymbol{\\omega} \\cdot \\mathbf{\\tau} \\right)\\) with frequency \\(\\boldsymbol{\\omega}\\). The amplitude as a function of frequency \\(S\\left( \\boldsymbol{\\omega} \\right)\\) must decay sufficiently fast so that the terms above are integrable.\nThere are some important points to note:\n\nIf we have a stationary kernel, we can resolve what frequencies underscore the model by working out its Fourier transform.\nOn the other hand, if we have a certain spectral density of interest, then its inverse Fourier transform is a kernel.\n\nTo analytically work this out, it may be useful to go through an example (courtsey of Markus Heinonen). The derivation below will require three pieces: - We shall assume a symmetric frequency distribution, i.e., \\(S\\left( \\boldsymbol{\\omega} \\right) = S \\left( -\\boldsymbol{\\omega} \\right)\\). - From Euler’s formula we have \\(cos\\left(x\\right) \\pm i sin\\left(x \\right) = exp \\left(\\pm ix \\right)\\) - The negative sine identity, i.e., \\(sin \\left( -x \\right) = - sin \\left( x \\right)\\)\nStarting with the expression above, we begin wtih\n\\[\n\\begin{aligned}\nk \\left( \\boldsymbol{\\tau} \\right) & = \\int_{-\\infty}^{\\infty} S \\left( \\boldsymbol{\\omega} \\right) exp \\left( 2 \\pi i \\boldsymbol{\\omega} \\cdot \\boldsymbol{\\tau} \\right) d \\boldsymbol{\\omega} \\\\\n& =   \\int_{-\\infty}^{\\infty} S \\left(\\boldsymbol{\\omega} \\right) cos \\left( 2 \\pi\\boldsymbol{\\tau} \\cdot \\boldsymbol{\\omega} \\right) d \\boldsymbol{\\omega} + \\int_{-\\infty}^{\\infty} iS \\left(\\boldsymbol{\\omega} \\right) sin \\left( 2 \\pi \\boldsymbol{\\tau} \\cdot \\boldsymbol{\\omega} \\right) d \\boldsymbol{\\omega} \\\\\n& = \\mathbb{E}\\left[ S \\left(\\omega \\right) \\right] cos \\left( 2 \\pi \\boldsymbol{\\tau} \\cdot \\boldsymbol{\\omega} \\right)  + \\int_{-\\infty}^{0} iS \\left(\\boldsymbol{\\omega} \\right) sin \\left( 2 \\pi \\boldsymbol{\\tau} \\cdot \\boldsymbol{\\omega} \\right) d \\boldsymbol{\\omega} + \\int_{0}^{\\infty} iS \\left(\\boldsymbol{\\omega} \\right) sin \\left( 2 \\pi \\boldsymbol{\\tau} \\cdot \\boldsymbol{\\omega} \\right) d \\boldsymbol{\\omega} \\\\\n& = \\mathbb{E}\\left[ S \\left(\\omega \\right) \\right] cos \\left( 2 \\pi \\boldsymbol{\\tau} \\cdot \\boldsymbol{\\omega} \\right)  + \\int_{0}^{\\infty} iS \\left(-\\boldsymbol{\\omega} \\right) sin \\left( -2 \\pi \\boldsymbol{\\tau} \\cdot \\boldsymbol{\\omega} \\right) d \\boldsymbol{\\omega} + \\int_{0}^{\\infty} iS \\left(\\boldsymbol{\\omega} \\right) sin \\left( 2 \\pi \\boldsymbol{\\tau} \\cdot \\boldsymbol{\\omega} \\right) d \\boldsymbol{\\omega} \\\\\n& = \\mathbb{E}\\left[ S \\left(\\omega \\right) \\right] cos \\left( 2 \\pi \\boldsymbol{\\tau} \\cdot \\boldsymbol{\\omega} \\right)  + \\int_{0}^{\\infty} -iS \\left(\\boldsymbol{\\omega} \\right) sin \\left( 2 \\pi \\boldsymbol{\\tau} \\cdot \\boldsymbol{\\omega} \\right) d \\boldsymbol{\\omega} + \\int_{0}^{\\infty} iS \\left(\\boldsymbol{\\omega} \\right) sin \\left( 2 \\pi \\boldsymbol{\\tau} \\cdot \\boldsymbol{\\omega} \\right) d \\boldsymbol{\\omega} \\\\\n\\end{aligned}\n\\]\nThis leads to\n\\[\n\\begin{aligned}\nk \\left( \\boldsymbol{\\tau} \\right) & = \\mathbb{E}\\left[ S \\left(\\omega \\right) \\right] cos \\left( 2 \\pi \\boldsymbol{\\tau} \\cdot \\boldsymbol{\\omega} \\right)   \n\\end{aligned}\n\\]\nThis demonstrates that all real-valued stationary kernels are \\(S\\left( \\boldsymbol{\\omega} \\right)\\)-weighted combinations of cosine terms. \n\n\n\nOur new general stationary kernel definition is thus:\n\\[\nk \\left( \\boldsymbol{\\tau} \\right)  = \\mathbb{E}\\left[ S \\left(\\omega \\right) \\right] cos \\left( 2 \\pi \\boldsymbol{\\tau} \\cdot \\boldsymbol{\\omega} \\right)   \n\\]\nwhere the frequencies \\(\\boldsymbol{\\omega}\\) are an inverse of the period \\(1/\\boldsymbol{\\omega}\\). Bracewell provides the following expressions for the Wiener-Khintchine result, by integrating out the angular variables (see page 83 of RW):\n\\[\n\\begin{aligned}\nk \\left( \\boldsymbol{\\tau} \\right) & = \\frac{2 \\pi}{\\boldsymbol{\\tau}^{-1/2}} \\int_{0}^{\\infty} S \\left( \\boldsymbol{\\omega} \\right) J_{-1/2} \\left(2 \\pi  \\boldsymbol{\\tau} \\cdot \\boldsymbol{\\omega} \\right) \\boldsymbol{\\omega}^{1/2} d \\boldsymbol{\\omega} \\\\\nS \\left(   \\boldsymbol{\\omega} \\right) & = \\frac{2 \\pi}{\\boldsymbol{\\omega}^{-1/2}} \\int_{0}^{\\infty} k \\left( \\boldsymbol{\\tau} \\right) J_{-1/2} \\left(2 \\pi  \\boldsymbol{\\tau} \\cdot \\boldsymbol{\\omega} \\right) \\boldsymbol{\\tau}^{1/2} d \\boldsymbol{\\tau}\n\\end{aligned}\n\\]\nNote that in RW, the authors use \\(D\\) to denote the dimensionality, which we have assumed to be 1. The function \\(J_{-1/2}\\) is the Bessel function of order \\(-1/2\\). While the expressions above may seem unwiedly, we can work out what these are using a bit of Sympy. Consider the case of a squared exponential kernel of the form\n\\[\nk \\left(\\boldsymbol{\\tau} \\right) = exp \\left(- \\frac{\\boldsymbol{\\tau}^2}{2l^2} \\right).\n\\]\n\n\nCode\nfrom sympy import * \n\nomega = Symbol(\"omega\")\nell = Symbol(\"l\")\ntau = Symbol(\"tau\")\n\nkernel = exp(- tau**2 / (2 * ell**2))\nintegrate(2*pi*omega**(1/2) * kernel * besselj(-1/2, 2*pi*tau*omega)*tau**(1/2), (tau, 0, oo))\n\n\n\\(\\displaystyle \\begin{cases} 1.4142135623731 \\pi^{0.5} l^{1.0} e^{- 2 \\pi^{2} l^{2} \\omega^{2}} & \\text{for}\\: \\left(\\left|{\\arg{\\left(\\omega \\right)}}\\right| = 0 \\wedge \\left|{\\arg{\\left(l \\right)}}\\right| &lt; \\frac{\\pi}{4}\\right) \\vee \\left|{\\arg{\\left(l \\right)}}\\right| &lt; \\frac{\\pi}{4} \\\\\\int\\limits_{0}^{\\infty} 2 \\pi \\omega^{0.5} \\tau^{0.5} e^{- \\frac{\\tau^{2}}{2 l^{2}}} J_{-0.5}\\left(2 \\pi \\omega \\tau\\right)\\, d\\tau & \\text{otherwise} \\end{cases}\\)\n\n\nThe first expression above is the Fourier amplitude of the squared exponential kernel, i.e.,\n\\[\nS \\left(\\boldsymbol{\\omega} \\right) = \\left( 2 \\pi l^2\\right)^{1/2} exp \\left( - 2 \\pi l^2 \\boldsymbol{\\omega}^2 \\right)\n\\]\n\n\nCode\nomega = np.linspace(0, np.pi/4, 50)\nl = 0.5\nS_omega = (2 * np.pi * l**2)**(1/2) * \\\n            np.exp(- 2 * np.pi * l**2 * omega**2)\ntau = np.linspace(0, 10, 200)\n\n\nfig = go.Figure()\nfig.add_scatter(x=omega, y=S_omega, mode='lines')\nfig.update_layout(title='Spectral density', \\\n                  xaxis_title=r'Frequency, $\\omega$',\\\n                  yaxis_title=r'Spectral density, $S\\left( \\omega \\right) $')\nfig.show()\n\n\n\n\n\n\n\nCode\nkernel = tau * 0.\ntrue_kernel = np.exp(-tau**2 / l**2)\ncounter = 0.\n\nfig = go.Figure()\nfor omega_j in omega:\n    counter += 1.\n    label=str(np.around(int(counter), 1))+' terms'\n    S_omega_j = (2 * np.pi * l**2)**(1/2) * \\\n            np.exp(- 2 * np.pi * l**2 * omega_j**2)\n    cos_term = np.cos(2 * np.pi * tau * omega_j)\n    kernel += (S_omega_j * cos_term)\n    fig.add_scatter(x=tau, y=kernel * 1/counter, name=label, mode='lines')\nfig.add_scatter(x=tau, y=true_kernel, name='Kernel', mode='lines', \\\n                line=dict(width=4, color='black'))\nfig.update_layout(title='Sq. exp kernel Fourier representation', \\\n                  xaxis_title=r'Distance, $\\tau$',\\\n                  yaxis_title=r'$k ( \\tau )$')\nfig.show()\n\n\n\n\n\nNotice that the more terms we incorporate, the closer we converge to the true kernel.\n\n\n\nRather than negotiate a kernel approximation with a great many number of terms, it will be more instructive to resort to a few terms. Such is the idea behind Random Fourier Features, where one selects a kernel comprised of random frequencies. For more details, please see the paper by Rahimi and Recht.\n\n\nCode\nR = 500 # random features\nD = 50 # number of data pts.\nx = np.linspace(-2*np.pi, 2*np.pi, D).reshape(D,1) # grid\nX = np.tile(x, [1, D]) - np.tile(x.T, [D, 1])\nW    = np.random.normal(loc=0, scale=0.1, size=(R, D))\nb    = np.random.uniform(0, 2*np.pi, size=R)\nB    = np.repeat(b[:, np.newaxis], D, axis=1)\nnorm = 1./ np.sqrt(R)\nZ    = norm * np.sqrt(2) * np.cos(W @ X.T + B)\nZZ   = Z.T @ Z\n\n\n\n\nCode\nfig = plt.figure(figsize=(14,5))\nplt.subplot(121)\nd = plt.imshow(ZZ)\nplt.colorbar(d, shrink=0.3)\nplt.title('Random Fourier Features')\nnormal = multivariate_normal(np.zeros((D)), ZZ, allow_singular=True)\nplt.subplot(122)\nplt.plot(x, normal.rvs(10).T )\nplt.title('Random samples from prior')\nplt.xlabel('x')\nplt.show()"
  },
  {
    "objectID": "useful_codes/kernels.html",
    "href": "useful_codes/kernels.html",
    "title": "Kernel trick and lifting",
    "section": "",
    "text": "This attempts to describe kernels. The hope is after going through this, the reader appreciates just how powerful kernels are, and the role they play in Gaussian process models.\n\n\nCode\n### Data \nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nimport plotly.express as px\nimport numpy as np\nimport pandas as pd\nimport plotly.io as pio\nimport numpy as np\npio.renderers.default = 'iframe'\nfrom IPython.display import display, HTML\n\n\n\n\nOne way to motivate the study of kernels, is to consider a linear regression problem where one has more unknowns than observational data. Let \\(\\mathbf{X} = \\left[\\mathbf{x}_{1}^{T}, \\mathbf{x}_{2}^{T}, \\ldots, \\mathbf{x}_{N}^{T}\\right]\\) be the \\(N \\times d\\) data corresponding to \\(N\\) observations of \\(d\\)-dimensional data. These input observations are accompanied by an output observational vector, \\(\\mathbf{y} \\in \\mathbb{R}^{N}\\). Let \\(\\boldsymbol{\\Phi} \\left( \\mathbf{X} \\right) \\in \\mathbb{R}^{N \\times M}\\) be a parameterized matrix comprising of \\(M\\) basis functions, i.e.,\n\\[\n\\boldsymbol{\\Phi}\\left( \\mathbf{X} \\right) = \\left[\\begin{array}{cccc}\n\\phi_{1}\\left(\\mathbf{X} \\right), & \\phi_{2}\\left(\\mathbf{X} \\right), & \\ldots, & \\phi_{M}\\left(\\mathbf{X} \\right)\\end{array}\\right]\n\\]\nIf we are interested in approximating \\(f \\left( \\mathbf{X} \\right) \\approx \\hat{f} \\left( \\mathbf{X} \\right) = y = \\mathbf{\\Phi} \\left( \\mathbf{X} \\right) \\boldsymbol{\\alpha}\\), we can determine the unknown coefficients via least squares. This leads to the solution via the normal equations\n\\[\n\\boldsymbol{\\alpha} = \\left( \\mathbf{\\Phi}^{T} \\mathbf{\\Phi}\\right)^{-1} \\boldsymbol{\\Phi}^{T} \\mathbf{y}\n\\]\nNow, strictly speaking, one cannot use the normal equations to solve a problem where there are more unknowns than observations because \\(\\left( \\mathbf{\\Phi}^{T} \\mathbf{\\Phi}\\right)\\) is not full rank. Recognizing that in such a situation, there may likely be numerous solutions to \\(\\boldsymbol{\\Phi}\\left( \\mathbf{X} \\right) \\boldsymbol{\\alpha} = \\mathbf{y}\\), we want the solution with the lowest \\(L_2\\) norm. This can be more conveniently formulated as as minimum norm problem, written as\n\\[\n\\begin{aligned}\n\\underset{x}{\\textrm{minimize}} & \\; \\boldsymbol{\\alpha}^{T} \\boldsymbol{\\alpha} \\\\\n\\textrm{subject to} \\; \\; &  \\boldsymbol{\\Phi}\\left( \\mathbf{X} \\right) \\boldsymbol{\\alpha} = \\mathbf{y}.\n\\end{aligned}\n\\]\nThe easiest way to solve this via the method of Lagrange multipliers, i.e., we define the objective function\n\\[\nL \\left( \\boldsymbol{\\alpha}, \\lambda \\right) = \\boldsymbol{\\alpha}^{T} \\boldsymbol{\\alpha}  + \\lambda^{T} \\left( \\boldsymbol{\\Phi}\\left( \\mathbf{X} \\right) \\boldsymbol{\\alpha} - \\mathbf{y}\\right),\n\\]\nwhere \\(\\lambda\\) comprises the Lagrange multipliers. The optimality conditions for this objective are given by\n\\[\n\\begin{aligned}\n\\nabla_{\\boldsymbol{\\alpha}} L & = 2 \\boldsymbol{\\alpha} + \\boldsymbol{\\Phi}^{T} \\lambda = 0, \\\\\n\\nabla_{\\lambda} L & = \\boldsymbol{\\Phi} \\boldsymbol{\\alpha} - \\mathbf{y} = 0.\n\\end{aligned}\n\\]\nThis leads to \\(\\boldsymbol{\\alpha} = - \\boldsymbol{\\Phi}^{T} \\lambda / 2\\). Substituting this into the second expression above yields \\(\\lambda = -2 \\left(\\boldsymbol{\\Phi} \\boldsymbol{\\Phi}^{T} \\right)^{-1} \\mathbf{y}\\). This leads to the minimum norm solution\n\\[\n\\boldsymbol{\\alpha} = \\boldsymbol{\\Phi}^{T}  \\left( \\boldsymbol{\\Phi} \\boldsymbol{\\Phi}^{T}  \\right)^{-1} \\mathbf{y}.\n\\]\nNote that unlike \\(\\left( \\boldsymbol{\\Phi}^{T} \\boldsymbol{\\Phi} \\right)\\), \\(\\left( \\boldsymbol{\\Phi} \\boldsymbol{\\Phi}^{T} \\right)\\) does have full rank. The latter is an inner product between feature vectors. To see this, define the two-point kernel function\n\\[\nk \\left( \\mathbf{x}, \\mathbf{x}' \\right) = \\boldsymbol{\\Phi} \\left( \\mathbf{x} \\right) \\boldsymbol{\\Phi}^{T} \\left( \\mathbf{x} \\right).\n\\]\nand the associated covariance matrix, defined elementwise via\n\\[\n\\left[ \\mathbf{K} \\left(\\mathbf{X}, \\mathbf{X}' \\right)\\right]_{ij} = k \\left( \\mathbf{x}_{i}, \\mathbf{x}_{j} \\right)\n\\]\n\n\n\nFrom the coefficients \\(\\boldsymbol{\\alpha}\\) computed via the minimum norm solution, it should be clear that approximate values of the true function at new locations \\(\\mathbf{X}_{\\ast}\\) can be given via\n\\[\n\\begin{aligned}\n\\hat{f} \\left( \\mathbf{X}_{\\ast} \\right) & = \\Phi \\left( \\mathbf{X}_{\\ast} \\right) \\boldsymbol{\\alpha} \\\\\n& = \\boldsymbol{\\Phi} \\left( \\mathbf{X}_{\\ast} \\right)  \\boldsymbol{\\Phi}^{T} \\left( \\mathbf{X} \\right)  \\left( \\boldsymbol{\\Phi} \\left( \\mathbf{X} \\right)  \\boldsymbol{\\Phi}^{T} \\left( \\mathbf{X} \\right)   \\right)^{-1} \\mathbf{y} \\\\\n& = \\left( \\boldsymbol{\\Phi} \\left( \\mathbf{X}_{\\ast} \\right)  \\boldsymbol{\\Phi}^{T} \\left( \\mathbf{X} \\right)  \\right)  \\left( \\boldsymbol{\\Phi} \\left( \\mathbf{X} \\right)  \\boldsymbol{\\Phi} \\left( \\mathbf{X} \\right) ^{T}  \\right)^{-1} \\mathbf{y} \\\\\n& = \\mathbf{K} \\left( \\mathbf{X}_{\\ast}, \\mathbf{X} \\right) \\mathbf{K}^{-1} \\left( \\mathbf{X}, \\mathbf{X} \\right) \\mathbf{y} \\\\\n\\end{aligned}\n\\]\nThere are two points to note here:\n\nThe form of the expression above is exactly that of the posterior predictive mean of a noise-free Gaussian processes model.\nOne need not compute the full \\(N \\times M\\) feature matrix \\(\\boldsymbol{\\Phi} \\left( \\mathbf{X} \\right)\\) explictly to work out the \\(N \\times N\\) matrix \\(\\mathbf{K}\\left( \\mathbf{X}, \\mathbf{X} \\right)\\).\n\nThis latter point is why this is called the kernel trick, i.e., for a very large number of features \\(M &gt;&gt; N\\) (possibly infinite), it is more computationally efficient to work out \\(\\mathbf{K}\\).\nAnother way to interpret the kernel trick is to consider the example that was discussed in lecture with regards to the data in the plot below.\n\n\nConsider a quadratic kernel in \\(\\mathbb{R}^{2}\\), where \\(\\mathbf{x} = \\left(x_1, x_2 \\right)^{T}\\) and \\(\\mathbf{v} = \\left(v_1, v_2 \\right)^{T}\\). We can express this kernel as \\[\n\\begin{aligned}\nk \\left( \\mathbf{x}, \\mathbf{v} \\right) =  \\left( \\mathbf{x}^{T}  \\mathbf{v} \\right)^2 & = \\left( \\left[\\begin{array}{cc}\nx_{1} & x_{2}\\end{array}\\right]\\left[\\begin{array}{c}\nv_{1}\\\\\nv_{2}\n\\end{array}\\right] \\right)^2  \\\\\n& = \\left( x_1^2 v_1^2 + 2 x_1 x_2 v_1 v_2 + x_2^2 v_2^2\\right) \\\\\n& = \\left[\\begin{array}{ccc}\nx^2_{1} & \\sqrt{2} x_1 x_2 & x_2^2 \\end{array}\\right]\\left[\\begin{array}{c}\nv_{1}^2\\\\\n\\sqrt{2}v_1 v_2 \\\\\nv_{2}^2\n\\end{array}\\right] \\\\\n& = \\phi \\left( \\mathbf{x} \\right)^{T} \\phi \\left( \\mathbf{v}  \\right).\n\\end{aligned}\n\\] where \\(\\phi \\left( \\cdot \\right) \\in \\mathbb{R}^{3}\\).\nNow lets tabulate the number of operations required depending on which route one takes. Computing \\(\\left( \\mathbf{x}^{T} \\mathbf{v} \\right)^2\\) requires two multiplications (i.e., \\(x_1 \\times v_1\\) and \\(x_2 \\times v_2\\)), one sum (i.e., \\(s = x_1 v_1 + x_2 v_2\\)), and one product (i.e., \\(s^2\\)). This leads to a total of four operations.\nNow consider the number of operations required for computing $ ( )^{T} ( )$. Assembling \\(\\phi \\left( \\mathbf{x} \\right)\\) itself requires three products; multiplying by \\(\\phi \\left( \\mathbf{v} \\right)\\) incurs another three products leading to a total of 10 operations (9 multiplications and one sum). Thus, computationally, it is cheaper to use the original form for calculating the product.\nThere is however another perspective to this. Data that is not linearly separable in \\(\\mathbb{R}^{2}\\) can be lifted up to \\(\\mathbb{R}^{3}\\) where a separation may be more easily inferred. In this particular case, \\(\\phi \\left( \\mathbf{x} \\right)\\) takes the form of a polynomial kernel.\nTo visualize this consider a red and green set of random points within a circle. Points that have a relatively greater radius are shown in red, whilst points that are closer to the center are captured in green.\n\n\nCode\nt = np.random.rand(40,1)* 2 * np.pi\nr = np.random.rand(40,1)*0.2 + 2\nu = r * np.cos(t)\nv = r * np.sin(t)\n\nxy = np.vstack([np.random.rand(20,2)*2 - 1])\nxy2 = np.hstack([u, v])\n\n\n\n\nCode\nfig = go.Figure()\nfig.add_scatter(x=xy2[:,0], y=xy2[:,1],  name='Red', mode='markers', marker=dict(\n        size=15, color='red', opacity=0.8, line=dict(color='black', width=1) ))\nfig.add_scatter(x=xy[:,0], y=xy[:,1],  name='Green', mode='markers', marker=dict(\n        size=15, color='green', opacity=0.8, line=dict(color='black', width=1) ))\nfig.update_layout(legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=0.01),\n                  xaxis_title=r'$\\mathbf{x}$',yaxis_title=r'$\\mathbf{v}$')\n\nfig.show()\n\n\n\n\n\nIt is not possible to separate these two sets using a line (or more generally a hyperplane). However, when the same data is lifed to \\(\\mathbb{R}^{3}\\), the two sets are linearly separable.\n\n\nCode\ndef mapup(xy):\n    phi_1 = xy[:,0]**2\n    phi_2 = np.sqrt(2) * xy[:,0] * xy[:,1]\n    phi_3 = xy[:,1]**2\n    return phi_1, phi_2, phi_3\n\nz1, z2, z3 = mapup(xy2)\nw1, w2, w3 = mapup(xy)\n\n\n\n\nCode\nfig = go.Figure()\nfig.add_scatter3d(x=z1, y=z2, z=z3, name='Red', mode='markers', marker=dict(\n        size=10, color='red', opacity=0.8, line=dict(color='black', width=2) ))\nfig.add_scatter3d(x=w1, y=w2, z=w3, name='Green', mode='markers', marker=dict(\n        size=10, color='green', opacity=0.8, line=dict(color='black', width=2) ))\nfig.update_layout(legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=0.01),\n                  scene = dict(\n                    xaxis_title=r'$\\phi_{1}$',\n                    yaxis_title=r'$\\phi_2$',\n                    zaxis_title=r'$\\phi_3$'),\n                    width=700,\n                    margin=dict(r=20, b=10, l=10, t=10))\n\nfig.show()\n\n\n\n\n\n\n\n\nWe shall now briefly consider the case of regression with infinitely many functions. Consider a basis function of the form\n\\[\n\\boldsymbol{\\Phi}\\left( \\mathbf{x} \\right) = \\left[\\begin{array}{cccc}\n\\phi_{1}\\left(\\mathbf{x} \\right), & \\phi_{2}\\left(\\mathbf{x} \\right), & \\ldots, & \\phi_{\\infty}\\left(\\mathbf{x} \\right)\\end{array}\\right]\n\\]\nwhere\n\\[\n\\phi_{j}\\left( \\mathbf{x} \\right) = exp \\left( - \\frac{\\left( \\mathbf{x} - c_j \\right)^2 }{2l^2} \\right)\n\\]\nwhere \\(c_j\\) represents the center of the bell-shaped basis function; we assume that there are infinitely many centers across the domain of interest and thus there exists infinitely many basis terms. To visualize this, see the code below.\n\n\nCode\nx = np.linspace(-5, 5, 150)\ninfty_subtitute = 20\nc_js = np.linspace(-5, 5, infty_subtitute)\nl = 0.5\n\nfig = go.Figure()\nfor j in range(0, infty_subtitute):\n    leg = 'c_j = '+str(np.around(c_js[j], 2))\n    psi_j = np.exp(- (x - c_js[j])**2 * 1./(2*l**2))\n    fig.add_scatter(x=x, y=psi_j, mode='lines', name=leg)\n    fig.update_layout(legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=-0.6),\n                  xaxis_title=r'$x$',yaxis_title=r'$\\phi\\left( x \\right)$')\nfig.show()\n\n\n\n\n\nThe two-point covariance matrix can be written as the sum of outer products of the feature vectors evaluated at all points \\(\\mathbf{X}\\) (which are individually rank-one matrices):\n\\[\n\\begin{aligned}\n\\mathbf{K} & = \\boldsymbol{\\Phi}\\left( \\mathbf{x} \\right)\\boldsymbol{\\Phi}^{T} \\left( \\mathbf{X} \\right) \\\\\n& = \\sum_{j=1}^{\\infty} \\phi_{j}\\left( \\mathbf{X} \\right) \\phi_{j}^{T}\\left( \\mathbf{X}' \\right)\n\\end{aligned}\n\\]\nor if one is considering each kernel entry, one can write\n\\[\n\\begin{aligned}\n\\mathbf{K}\\left[i, j \\right] = \\mathbf{K}_{ij}  & = k \\left( \\mathbf{x}_i, \\mathbf{x}_j \\right)\\\\\n& = \\boldsymbol{\\Phi}\\left( \\mathbf{x}_i \\right)\\boldsymbol{\\Phi}^{T} \\left( \\mathbf{x}_j \\right) \\\\\n& = \\sum_{p=1}^{\\infty} \\phi_{p}\\left( \\mathbf{x}_i \\right) \\phi_{p}\\left( \\mathbf{x}_j \\right)\n\\end{aligned}\n\\]\nThis last expression can be conveniently replaced with an integral (see RW page 84).\n\\[\nk \\left( \\mathbf{x}_i, \\mathbf{x}_j \\right)  = \\int_{\\mathcal{X}} exp \\left( - \\frac{\\left( \\mathbf{x}_i - c \\right)^2 }{2l^2} \\right)exp \\left( - \\frac{\\left( \\mathbf{x}_j - c \\right)^2 }{2l^2} \\right)dc\n\\]\nwhere we will assume that \\(\\mathcal{X} \\subset [-\\infty, \\infty]\\). This leads to\n\\[\n\\begin{aligned}\nk \\left( \\mathbf{x}_i, \\mathbf{x}_j \\right) & = \\int_{-\\infty}^{\\infty} exp \\left( - \\frac{\\left( \\mathbf{x}_i - c \\right)^2 }{2l^2} \\right)exp \\left( - \\frac{\\left( \\mathbf{x}_j - c \\right)^2 }{2l^2} \\right)dc \\\\\n& = \\sqrt{\\pi}l \\; exp \\left( - \\frac{\\left(\\mathbf{x}_i - \\mathbf{x}_j\\right)^2 }{2 \\left( \\sqrt{2} \\right) l^2 } \\right).\n\\end{aligned}\n\\]\nThe last expression is easily recognizable as an RBF kernel with an amplitude of \\(\\sqrt{\\pi}l\\) and a slightly amended length scale of \\(\\sqrt{2}l^2\\). It is straightforward to adapt this to multivariate \\(\\mathbf{x}\\).\nNote the utility of this representation—we essentially have infinitely many basis terms, but the size of our covariance matrix is driven by the number of data points"
  },
  {
    "objectID": "useful_codes/kernels.html#overview",
    "href": "useful_codes/kernels.html#overview",
    "title": "Kernel trick and lifting",
    "section": "",
    "text": "This attempts to describe kernels. The hope is after going through this, the reader appreciates just how powerful kernels are, and the role they play in Gaussian process models.\n\n\nCode\n### Data \nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nimport plotly.express as px\nimport numpy as np\nimport pandas as pd\nimport plotly.io as pio\nimport numpy as np\npio.renderers.default = 'iframe'\nfrom IPython.display import display, HTML\n\n\n\n\nOne way to motivate the study of kernels, is to consider a linear regression problem where one has more unknowns than observational data. Let \\(\\mathbf{X} = \\left[\\mathbf{x}_{1}^{T}, \\mathbf{x}_{2}^{T}, \\ldots, \\mathbf{x}_{N}^{T}\\right]\\) be the \\(N \\times d\\) data corresponding to \\(N\\) observations of \\(d\\)-dimensional data. These input observations are accompanied by an output observational vector, \\(\\mathbf{y} \\in \\mathbb{R}^{N}\\). Let \\(\\boldsymbol{\\Phi} \\left( \\mathbf{X} \\right) \\in \\mathbb{R}^{N \\times M}\\) be a parameterized matrix comprising of \\(M\\) basis functions, i.e.,\n\\[\n\\boldsymbol{\\Phi}\\left( \\mathbf{X} \\right) = \\left[\\begin{array}{cccc}\n\\phi_{1}\\left(\\mathbf{X} \\right), & \\phi_{2}\\left(\\mathbf{X} \\right), & \\ldots, & \\phi_{M}\\left(\\mathbf{X} \\right)\\end{array}\\right]\n\\]\nIf we are interested in approximating \\(f \\left( \\mathbf{X} \\right) \\approx \\hat{f} \\left( \\mathbf{X} \\right) = y = \\mathbf{\\Phi} \\left( \\mathbf{X} \\right) \\boldsymbol{\\alpha}\\), we can determine the unknown coefficients via least squares. This leads to the solution via the normal equations\n\\[\n\\boldsymbol{\\alpha} = \\left( \\mathbf{\\Phi}^{T} \\mathbf{\\Phi}\\right)^{-1} \\boldsymbol{\\Phi}^{T} \\mathbf{y}\n\\]\nNow, strictly speaking, one cannot use the normal equations to solve a problem where there are more unknowns than observations because \\(\\left( \\mathbf{\\Phi}^{T} \\mathbf{\\Phi}\\right)\\) is not full rank. Recognizing that in such a situation, there may likely be numerous solutions to \\(\\boldsymbol{\\Phi}\\left( \\mathbf{X} \\right) \\boldsymbol{\\alpha} = \\mathbf{y}\\), we want the solution with the lowest \\(L_2\\) norm. This can be more conveniently formulated as as minimum norm problem, written as\n\\[\n\\begin{aligned}\n\\underset{x}{\\textrm{minimize}} & \\; \\boldsymbol{\\alpha}^{T} \\boldsymbol{\\alpha} \\\\\n\\textrm{subject to} \\; \\; &  \\boldsymbol{\\Phi}\\left( \\mathbf{X} \\right) \\boldsymbol{\\alpha} = \\mathbf{y}.\n\\end{aligned}\n\\]\nThe easiest way to solve this via the method of Lagrange multipliers, i.e., we define the objective function\n\\[\nL \\left( \\boldsymbol{\\alpha}, \\lambda \\right) = \\boldsymbol{\\alpha}^{T} \\boldsymbol{\\alpha}  + \\lambda^{T} \\left( \\boldsymbol{\\Phi}\\left( \\mathbf{X} \\right) \\boldsymbol{\\alpha} - \\mathbf{y}\\right),\n\\]\nwhere \\(\\lambda\\) comprises the Lagrange multipliers. The optimality conditions for this objective are given by\n\\[\n\\begin{aligned}\n\\nabla_{\\boldsymbol{\\alpha}} L & = 2 \\boldsymbol{\\alpha} + \\boldsymbol{\\Phi}^{T} \\lambda = 0, \\\\\n\\nabla_{\\lambda} L & = \\boldsymbol{\\Phi} \\boldsymbol{\\alpha} - \\mathbf{y} = 0.\n\\end{aligned}\n\\]\nThis leads to \\(\\boldsymbol{\\alpha} = - \\boldsymbol{\\Phi}^{T} \\lambda / 2\\). Substituting this into the second expression above yields \\(\\lambda = -2 \\left(\\boldsymbol{\\Phi} \\boldsymbol{\\Phi}^{T} \\right)^{-1} \\mathbf{y}\\). This leads to the minimum norm solution\n\\[\n\\boldsymbol{\\alpha} = \\boldsymbol{\\Phi}^{T}  \\left( \\boldsymbol{\\Phi} \\boldsymbol{\\Phi}^{T}  \\right)^{-1} \\mathbf{y}.\n\\]\nNote that unlike \\(\\left( \\boldsymbol{\\Phi}^{T} \\boldsymbol{\\Phi} \\right)\\), \\(\\left( \\boldsymbol{\\Phi} \\boldsymbol{\\Phi}^{T} \\right)\\) does have full rank. The latter is an inner product between feature vectors. To see this, define the two-point kernel function\n\\[\nk \\left( \\mathbf{x}, \\mathbf{x}' \\right) = \\boldsymbol{\\Phi} \\left( \\mathbf{x} \\right) \\boldsymbol{\\Phi}^{T} \\left( \\mathbf{x} \\right).\n\\]\nand the associated covariance matrix, defined elementwise via\n\\[\n\\left[ \\mathbf{K} \\left(\\mathbf{X}, \\mathbf{X}' \\right)\\right]_{ij} = k \\left( \\mathbf{x}_{i}, \\mathbf{x}_{j} \\right)\n\\]\n\n\n\nFrom the coefficients \\(\\boldsymbol{\\alpha}\\) computed via the minimum norm solution, it should be clear that approximate values of the true function at new locations \\(\\mathbf{X}_{\\ast}\\) can be given via\n\\[\n\\begin{aligned}\n\\hat{f} \\left( \\mathbf{X}_{\\ast} \\right) & = \\Phi \\left( \\mathbf{X}_{\\ast} \\right) \\boldsymbol{\\alpha} \\\\\n& = \\boldsymbol{\\Phi} \\left( \\mathbf{X}_{\\ast} \\right)  \\boldsymbol{\\Phi}^{T} \\left( \\mathbf{X} \\right)  \\left( \\boldsymbol{\\Phi} \\left( \\mathbf{X} \\right)  \\boldsymbol{\\Phi}^{T} \\left( \\mathbf{X} \\right)   \\right)^{-1} \\mathbf{y} \\\\\n& = \\left( \\boldsymbol{\\Phi} \\left( \\mathbf{X}_{\\ast} \\right)  \\boldsymbol{\\Phi}^{T} \\left( \\mathbf{X} \\right)  \\right)  \\left( \\boldsymbol{\\Phi} \\left( \\mathbf{X} \\right)  \\boldsymbol{\\Phi} \\left( \\mathbf{X} \\right) ^{T}  \\right)^{-1} \\mathbf{y} \\\\\n& = \\mathbf{K} \\left( \\mathbf{X}_{\\ast}, \\mathbf{X} \\right) \\mathbf{K}^{-1} \\left( \\mathbf{X}, \\mathbf{X} \\right) \\mathbf{y} \\\\\n\\end{aligned}\n\\]\nThere are two points to note here:\n\nThe form of the expression above is exactly that of the posterior predictive mean of a noise-free Gaussian processes model.\nOne need not compute the full \\(N \\times M\\) feature matrix \\(\\boldsymbol{\\Phi} \\left( \\mathbf{X} \\right)\\) explictly to work out the \\(N \\times N\\) matrix \\(\\mathbf{K}\\left( \\mathbf{X}, \\mathbf{X} \\right)\\).\n\nThis latter point is why this is called the kernel trick, i.e., for a very large number of features \\(M &gt;&gt; N\\) (possibly infinite), it is more computationally efficient to work out \\(\\mathbf{K}\\).\nAnother way to interpret the kernel trick is to consider the example that was discussed in lecture with regards to the data in the plot below.\n\n\nConsider a quadratic kernel in \\(\\mathbb{R}^{2}\\), where \\(\\mathbf{x} = \\left(x_1, x_2 \\right)^{T}\\) and \\(\\mathbf{v} = \\left(v_1, v_2 \\right)^{T}\\). We can express this kernel as \\[\n\\begin{aligned}\nk \\left( \\mathbf{x}, \\mathbf{v} \\right) =  \\left( \\mathbf{x}^{T}  \\mathbf{v} \\right)^2 & = \\left( \\left[\\begin{array}{cc}\nx_{1} & x_{2}\\end{array}\\right]\\left[\\begin{array}{c}\nv_{1}\\\\\nv_{2}\n\\end{array}\\right] \\right)^2  \\\\\n& = \\left( x_1^2 v_1^2 + 2 x_1 x_2 v_1 v_2 + x_2^2 v_2^2\\right) \\\\\n& = \\left[\\begin{array}{ccc}\nx^2_{1} & \\sqrt{2} x_1 x_2 & x_2^2 \\end{array}\\right]\\left[\\begin{array}{c}\nv_{1}^2\\\\\n\\sqrt{2}v_1 v_2 \\\\\nv_{2}^2\n\\end{array}\\right] \\\\\n& = \\phi \\left( \\mathbf{x} \\right)^{T} \\phi \\left( \\mathbf{v}  \\right).\n\\end{aligned}\n\\] where \\(\\phi \\left( \\cdot \\right) \\in \\mathbb{R}^{3}\\).\nNow lets tabulate the number of operations required depending on which route one takes. Computing \\(\\left( \\mathbf{x}^{T} \\mathbf{v} \\right)^2\\) requires two multiplications (i.e., \\(x_1 \\times v_1\\) and \\(x_2 \\times v_2\\)), one sum (i.e., \\(s = x_1 v_1 + x_2 v_2\\)), and one product (i.e., \\(s^2\\)). This leads to a total of four operations.\nNow consider the number of operations required for computing $ ( )^{T} ( )$. Assembling \\(\\phi \\left( \\mathbf{x} \\right)\\) itself requires three products; multiplying by \\(\\phi \\left( \\mathbf{v} \\right)\\) incurs another three products leading to a total of 10 operations (9 multiplications and one sum). Thus, computationally, it is cheaper to use the original form for calculating the product.\nThere is however another perspective to this. Data that is not linearly separable in \\(\\mathbb{R}^{2}\\) can be lifted up to \\(\\mathbb{R}^{3}\\) where a separation may be more easily inferred. In this particular case, \\(\\phi \\left( \\mathbf{x} \\right)\\) takes the form of a polynomial kernel.\nTo visualize this consider a red and green set of random points within a circle. Points that have a relatively greater radius are shown in red, whilst points that are closer to the center are captured in green.\n\n\nCode\nt = np.random.rand(40,1)* 2 * np.pi\nr = np.random.rand(40,1)*0.2 + 2\nu = r * np.cos(t)\nv = r * np.sin(t)\n\nxy = np.vstack([np.random.rand(20,2)*2 - 1])\nxy2 = np.hstack([u, v])\n\n\n\n\nCode\nfig = go.Figure()\nfig.add_scatter(x=xy2[:,0], y=xy2[:,1],  name='Red', mode='markers', marker=dict(\n        size=15, color='red', opacity=0.8, line=dict(color='black', width=1) ))\nfig.add_scatter(x=xy[:,0], y=xy[:,1],  name='Green', mode='markers', marker=dict(\n        size=15, color='green', opacity=0.8, line=dict(color='black', width=1) ))\nfig.update_layout(legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=0.01),\n                  xaxis_title=r'$\\mathbf{x}$',yaxis_title=r'$\\mathbf{v}$')\n\nfig.show()\n\n\n\n\n\nIt is not possible to separate these two sets using a line (or more generally a hyperplane). However, when the same data is lifed to \\(\\mathbb{R}^{3}\\), the two sets are linearly separable.\n\n\nCode\ndef mapup(xy):\n    phi_1 = xy[:,0]**2\n    phi_2 = np.sqrt(2) * xy[:,0] * xy[:,1]\n    phi_3 = xy[:,1]**2\n    return phi_1, phi_2, phi_3\n\nz1, z2, z3 = mapup(xy2)\nw1, w2, w3 = mapup(xy)\n\n\n\n\nCode\nfig = go.Figure()\nfig.add_scatter3d(x=z1, y=z2, z=z3, name='Red', mode='markers', marker=dict(\n        size=10, color='red', opacity=0.8, line=dict(color='black', width=2) ))\nfig.add_scatter3d(x=w1, y=w2, z=w3, name='Green', mode='markers', marker=dict(\n        size=10, color='green', opacity=0.8, line=dict(color='black', width=2) ))\nfig.update_layout(legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=0.01),\n                  scene = dict(\n                    xaxis_title=r'$\\phi_{1}$',\n                    yaxis_title=r'$\\phi_2$',\n                    zaxis_title=r'$\\phi_3$'),\n                    width=700,\n                    margin=dict(r=20, b=10, l=10, t=10))\n\nfig.show()\n\n\n\n\n\n\n\n\nWe shall now briefly consider the case of regression with infinitely many functions. Consider a basis function of the form\n\\[\n\\boldsymbol{\\Phi}\\left( \\mathbf{x} \\right) = \\left[\\begin{array}{cccc}\n\\phi_{1}\\left(\\mathbf{x} \\right), & \\phi_{2}\\left(\\mathbf{x} \\right), & \\ldots, & \\phi_{\\infty}\\left(\\mathbf{x} \\right)\\end{array}\\right]\n\\]\nwhere\n\\[\n\\phi_{j}\\left( \\mathbf{x} \\right) = exp \\left( - \\frac{\\left( \\mathbf{x} - c_j \\right)^2 }{2l^2} \\right)\n\\]\nwhere \\(c_j\\) represents the center of the bell-shaped basis function; we assume that there are infinitely many centers across the domain of interest and thus there exists infinitely many basis terms. To visualize this, see the code below.\n\n\nCode\nx = np.linspace(-5, 5, 150)\ninfty_subtitute = 20\nc_js = np.linspace(-5, 5, infty_subtitute)\nl = 0.5\n\nfig = go.Figure()\nfor j in range(0, infty_subtitute):\n    leg = 'c_j = '+str(np.around(c_js[j], 2))\n    psi_j = np.exp(- (x - c_js[j])**2 * 1./(2*l**2))\n    fig.add_scatter(x=x, y=psi_j, mode='lines', name=leg)\n    fig.update_layout(legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=-0.6),\n                  xaxis_title=r'$x$',yaxis_title=r'$\\phi\\left( x \\right)$')\nfig.show()\n\n\n\n\n\nThe two-point covariance matrix can be written as the sum of outer products of the feature vectors evaluated at all points \\(\\mathbf{X}\\) (which are individually rank-one matrices):\n\\[\n\\begin{aligned}\n\\mathbf{K} & = \\boldsymbol{\\Phi}\\left( \\mathbf{x} \\right)\\boldsymbol{\\Phi}^{T} \\left( \\mathbf{X} \\right) \\\\\n& = \\sum_{j=1}^{\\infty} \\phi_{j}\\left( \\mathbf{X} \\right) \\phi_{j}^{T}\\left( \\mathbf{X}' \\right)\n\\end{aligned}\n\\]\nor if one is considering each kernel entry, one can write\n\\[\n\\begin{aligned}\n\\mathbf{K}\\left[i, j \\right] = \\mathbf{K}_{ij}  & = k \\left( \\mathbf{x}_i, \\mathbf{x}_j \\right)\\\\\n& = \\boldsymbol{\\Phi}\\left( \\mathbf{x}_i \\right)\\boldsymbol{\\Phi}^{T} \\left( \\mathbf{x}_j \\right) \\\\\n& = \\sum_{p=1}^{\\infty} \\phi_{p}\\left( \\mathbf{x}_i \\right) \\phi_{p}\\left( \\mathbf{x}_j \\right)\n\\end{aligned}\n\\]\nThis last expression can be conveniently replaced with an integral (see RW page 84).\n\\[\nk \\left( \\mathbf{x}_i, \\mathbf{x}_j \\right)  = \\int_{\\mathcal{X}} exp \\left( - \\frac{\\left( \\mathbf{x}_i - c \\right)^2 }{2l^2} \\right)exp \\left( - \\frac{\\left( \\mathbf{x}_j - c \\right)^2 }{2l^2} \\right)dc\n\\]\nwhere we will assume that \\(\\mathcal{X} \\subset [-\\infty, \\infty]\\). This leads to\n\\[\n\\begin{aligned}\nk \\left( \\mathbf{x}_i, \\mathbf{x}_j \\right) & = \\int_{-\\infty}^{\\infty} exp \\left( - \\frac{\\left( \\mathbf{x}_i - c \\right)^2 }{2l^2} \\right)exp \\left( - \\frac{\\left( \\mathbf{x}_j - c \\right)^2 }{2l^2} \\right)dc \\\\\n& = \\sqrt{\\pi}l \\; exp \\left( - \\frac{\\left(\\mathbf{x}_i - \\mathbf{x}_j\\right)^2 }{2 \\left( \\sqrt{2} \\right) l^2 } \\right).\n\\end{aligned}\n\\]\nThe last expression is easily recognizable as an RBF kernel with an amplitude of \\(\\sqrt{\\pi}l\\) and a slightly amended length scale of \\(\\sqrt{2}l^2\\). It is straightforward to adapt this to multivariate \\(\\mathbf{x}\\).\nNote the utility of this representation—we essentially have infinitely many basis terms, but the size of our covariance matrix is driven by the number of data points"
  },
  {
    "objectID": "useful_codes/discrete.html",
    "href": "useful_codes/discrete.html",
    "title": "Discrete distributions",
    "section": "",
    "text": "This notebook is has useful boiler plate code for generating distributions and visualizing them.\n\n\nCode\nimport numpy as np \nfrom scipy.stats import bernoulli, binom\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.special import comb\nsns.set(font_scale=1.0)\nsns.set_style(\"white\")\nsns.set_style(\"ticks\")\npalette = sns.color_palette('deep')\n#plt.style.use('dark_background') # cosmetic!\n\n\n\n\nThe probability mass function for a Bernoulli distribution is given by\n\\[\np \\left( x \\right) = \\begin{cases}\n\\begin{array}{c}\n1 - p  \\; \\; \\; \\textrm{if} \\; x = 0 \\\\\np \\; \\; \\; \\textrm{if} \\; x = 1\n\\end{array}\\end{cases}\n\\]\nfor \\(x \\in \\left\\{0, 1 \\right\\}\\) and where \\(0 \\leq p \\leq 1\\).\n\n\nCode\np = 0.4 # Bernoulli parameter\nx = np.linspace(0, 1, 2)\nprobabilities = bernoulli.pmf(x, p)\n\nfig = plt.figure(figsize=(8,4))\n\nplt.plot(x, probabilities, 'o', ms=8, color='orangered')\nplt.vlines(x, 0, probabilities, colors='orangered', lw=5, alpha=0.5)\nplt.xlabel('x')\nplt.ylabel('Probability')\nplt.savefig('pdf.png', dpi=150, bbox_inches='tight', transparent=True)\n\nplt.show()\n\n\n\n\n\nOne can generate random values from this distribution, i.e.,\n\n\nCode\nX = bernoulli.rvs(p, size=500)\nprint(X)\n\n\n[1 1 0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0\n 1 0 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0\n 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0\n 0 0 0 1 1 0 1 0 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0\n 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 0\n 0 1 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0\n 1 0 1 0 0 1 1 1 0 1 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0\n 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 1\n 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0\n 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 1 1 1 0 1 0\n 0 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1\n 1 0 0 1 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0 0 1\n 0 0 0 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 0 1 1\n 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0]\n\n\nThus, random values from a Bernoulli distribution are inherently binary, and the number of 0s vs 1s will vary depending on the choice of the parameter, \\(p\\). We will see later on (in another notebook) how this relatively simple idea can be used to train a Naive Bayes Classifier. For now, we will plot the expected value of the Bernoulli random variable with increasing number of samples.\n\n\nCode\nnumbers = [10, 50, 100, 200, 300, 500, 1000, 2000, 5000, 10000]\nmeans = []\nstds = []\nfor j in numbers:\n    X_val = []\n    for q in range(0, 10):\n        X = bernoulli.rvs(p, size=j)\n        X_val.append(np.mean(X))\n    means.append(np.mean(X_val))\n    stds.append(np.std(X_val))\n\nmeans = np.array(means)\nstds = np.array(stds)\nnumbers = np.array(numbers)\n\nfig = plt.figure(figsize=(8,4))\nplt.plot(numbers, means, 'ro-', lw=2)\nplt.fill_between(numbers, means + stds, means - stds, color='crimson', alpha=0.3)\nplt.xlabel('Number of random samples')\nplt.ylabel('Expectation')\nplt.savefig('convergence.png', dpi=150, bbox_inches='tight', transparent=True)\nplt.show()\n\n\n\n\n\n\n\n\nNext, we consider the Binomial distribution. It has a probability mass function\n\\[\np \\left( x \\right) = \\left(\\begin{array}{c}\nn\\\\\nx\n\\end{array}\\right)p^{x}\\left(1-p\\right)^{n-x}\n\\]\nfor \\(x \\in \\left\\{0, 1, \\ldots, n \\right\\}\\) and where \\(0 \\leq p \\leq 1\\).\n\n\nCode\np = 0.3 # Bernoulli parameter\nn = 7\nx = np.arange(0, n+1)\nprobabilities = binom(n, p)\n\nfig = plt.figure(figsize=(8,4))\nplt.plot(x, probabilities.pmf(x), 'o', ms=8, color='deeppink')\nplt.vlines(x, 0, probabilities.pmf(x), colors='deeppink', lw=5 )\nplt.xlabel('x')\nplt.ylabel('Probability')\nplt.savefig('pdf_2.png', dpi=150, bbox_inches='tight', transparent=True)\nplt.show()\n\n\n\n\n\nTo work out the probability at \\(x=3\\), we can compute:\n\n\nCode\nprob = comb(N=n, k=3) * p**3 * (1 - p)**(n - 3)\nprint(prob)\n\n\n0.22689449999999992"
  },
  {
    "objectID": "useful_codes/discrete.html#scope",
    "href": "useful_codes/discrete.html#scope",
    "title": "Discrete distributions",
    "section": "",
    "text": "This notebook is has useful boiler plate code for generating distributions and visualizing them.\n\n\nCode\nimport numpy as np \nfrom scipy.stats import bernoulli, binom\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.special import comb\nsns.set(font_scale=1.0)\nsns.set_style(\"white\")\nsns.set_style(\"ticks\")\npalette = sns.color_palette('deep')\n#plt.style.use('dark_background') # cosmetic!\n\n\n\n\nThe probability mass function for a Bernoulli distribution is given by\n\\[\np \\left( x \\right) = \\begin{cases}\n\\begin{array}{c}\n1 - p  \\; \\; \\; \\textrm{if} \\; x = 0 \\\\\np \\; \\; \\; \\textrm{if} \\; x = 1\n\\end{array}\\end{cases}\n\\]\nfor \\(x \\in \\left\\{0, 1 \\right\\}\\) and where \\(0 \\leq p \\leq 1\\).\n\n\nCode\np = 0.4 # Bernoulli parameter\nx = np.linspace(0, 1, 2)\nprobabilities = bernoulli.pmf(x, p)\n\nfig = plt.figure(figsize=(8,4))\n\nplt.plot(x, probabilities, 'o', ms=8, color='orangered')\nplt.vlines(x, 0, probabilities, colors='orangered', lw=5, alpha=0.5)\nplt.xlabel('x')\nplt.ylabel('Probability')\nplt.savefig('pdf.png', dpi=150, bbox_inches='tight', transparent=True)\n\nplt.show()\n\n\n\n\n\nOne can generate random values from this distribution, i.e.,\n\n\nCode\nX = bernoulli.rvs(p, size=500)\nprint(X)\n\n\n[1 1 0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0\n 1 0 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0\n 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0\n 0 0 0 1 1 0 1 0 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0\n 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 0\n 0 1 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0\n 1 0 1 0 0 1 1 1 0 1 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0\n 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 1\n 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0\n 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 1 1 1 0 1 0\n 0 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1\n 1 0 0 1 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0 0 1\n 0 0 0 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 0 1 1\n 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0]\n\n\nThus, random values from a Bernoulli distribution are inherently binary, and the number of 0s vs 1s will vary depending on the choice of the parameter, \\(p\\). We will see later on (in another notebook) how this relatively simple idea can be used to train a Naive Bayes Classifier. For now, we will plot the expected value of the Bernoulli random variable with increasing number of samples.\n\n\nCode\nnumbers = [10, 50, 100, 200, 300, 500, 1000, 2000, 5000, 10000]\nmeans = []\nstds = []\nfor j in numbers:\n    X_val = []\n    for q in range(0, 10):\n        X = bernoulli.rvs(p, size=j)\n        X_val.append(np.mean(X))\n    means.append(np.mean(X_val))\n    stds.append(np.std(X_val))\n\nmeans = np.array(means)\nstds = np.array(stds)\nnumbers = np.array(numbers)\n\nfig = plt.figure(figsize=(8,4))\nplt.plot(numbers, means, 'ro-', lw=2)\nplt.fill_between(numbers, means + stds, means - stds, color='crimson', alpha=0.3)\nplt.xlabel('Number of random samples')\nplt.ylabel('Expectation')\nplt.savefig('convergence.png', dpi=150, bbox_inches='tight', transparent=True)\nplt.show()\n\n\n\n\n\n\n\n\nNext, we consider the Binomial distribution. It has a probability mass function\n\\[\np \\left( x \\right) = \\left(\\begin{array}{c}\nn\\\\\nx\n\\end{array}\\right)p^{x}\\left(1-p\\right)^{n-x}\n\\]\nfor \\(x \\in \\left\\{0, 1, \\ldots, n \\right\\}\\) and where \\(0 \\leq p \\leq 1\\).\n\n\nCode\np = 0.3 # Bernoulli parameter\nn = 7\nx = np.arange(0, n+1)\nprobabilities = binom(n, p)\n\nfig = plt.figure(figsize=(8,4))\nplt.plot(x, probabilities.pmf(x), 'o', ms=8, color='deeppink')\nplt.vlines(x, 0, probabilities.pmf(x), colors='deeppink', lw=5 )\nplt.xlabel('x')\nplt.ylabel('Probability')\nplt.savefig('pdf_2.png', dpi=150, bbox_inches='tight', transparent=True)\nplt.show()\n\n\n\n\n\nTo work out the probability at \\(x=3\\), we can compute:\n\n\nCode\nprob = comb(N=n, k=3) * p**3 * (1 - p)**(n - 3)\nprint(prob)\n\n\n0.22689449999999992"
  },
  {
    "objectID": "sample_problems/lecture_3.html",
    "href": "sample_problems/lecture_3.html",
    "title": "L3 examples",
    "section": "",
    "text": "In going through some historical records, you find that scientists from a lost civilization tried to measure the distance from the ground to some clouds. Based on the data you assume that the distance is a Gaussian random variable with a mean of 1830m and a standard deviation of 460m. What is the probability that the clouds would be at a height above 2750m?\n\n\nSolution\n\nLet \\(X\\) be this Gaussian random variable. This problem essentially requires us to work out \\(p \\left( X &gt; 2750 \\right)\\). This can be expressed as\n\\[\n\\large\np \\left(X &gt; 2750 \\right) = 1 - p \\left( X \\leq 2750 \\right) = 1 - \\Phi \\left( z \\right)\n\\]\nwhere \\(z = (2750 - 1830)/460 = 2\\). Thus we have\n\\[\n\\large\n1 - \\Phi \\left( 2 \\right) = 1 - 0.9772 = 0.0228\n\\]"
  },
  {
    "objectID": "sample_problems/lecture_3.html#problem-1",
    "href": "sample_problems/lecture_3.html#problem-1",
    "title": "L3 examples",
    "section": "",
    "text": "In going through some historical records, you find that scientists from a lost civilization tried to measure the distance from the ground to some clouds. Based on the data you assume that the distance is a Gaussian random variable with a mean of 1830m and a standard deviation of 460m. What is the probability that the clouds would be at a height above 2750m?\n\n\nSolution\n\nLet \\(X\\) be this Gaussian random variable. This problem essentially requires us to work out \\(p \\left( X &gt; 2750 \\right)\\). This can be expressed as\n\\[\n\\large\np \\left(X &gt; 2750 \\right) = 1 - p \\left( X \\leq 2750 \\right) = 1 - \\Phi \\left( z \\right)\n\\]\nwhere \\(z = (2750 - 1830)/460 = 2\\). Thus we have\n\\[\n\\large\n1 - \\Phi \\left( 2 \\right) = 1 - 0.9772 = 0.0228\n\\]"
  },
  {
    "objectID": "sample_problems/lecture_1.html",
    "href": "sample_problems/lecture_1.html",
    "title": "L1 examples",
    "section": "",
    "text": "The probability that a scheduled flight departs on time is 0.83 and the probability that it arrives on time is 0.92. The probability that it both departs and arrives on time is 0.78. Find the probability that\n\nthe plane arrives on time given that it departed on time;\nthe plane did not depart on time given that it did not arrive on time.\n\n\n\nSolution\n\nIt will be useful to consider the Venn diagram shown below.\n\nLet \\(\\require{color}{\\color[rgb]{0.000066,0.001801,0.998229}A}\\) denote the event that the plane arrives on time, while \\({\\color[rgb]{0.986252,0.007236,0.027423}D}\\) denotes th event that the plane departs on time. To construct the Venn diagram above note that \\(P\\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) = 0.78\\). From the sum rule of probabilities, we have:\n\\[\n\\require{color}\n\\large\nP\\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\right) = P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap \\bar{{\\color[rgb]{0.986252,0.007236,0.027423}D}} \\right) + P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right)\n\\]\n\\[\n\\require{color}\n\\large\n0.92= 0.78+ P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right)\n\\]\nwhich implies that \\(\\require{color} P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) = 0.92 - 0.78 = 0.14\\). Similarly, we have:\n\\[\n\\require{color}\n\\large\nP \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) = P \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\cap \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) + P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right)\n\\]\n\\[\n\\require{color}\n\\large\n\\Rightarrow 0.83 = P \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\cap \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) + 0.78\n\\]\nwhich implies that \\(P \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\cap \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) = 0.05\\). With these probabilities, we can now answer the questions.\n\nThe plane arrives on time conditioned that it departed on time:\n\n\\[\n\\large\n\\require{color}\nP \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} | {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) = \\frac{P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) }{P \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) } = \\frac{0.78}{0.83} = 0.94\n\\]\n\nThe plane did not depart on time conditioned on it having not arrived on time:\n\n\\[\n\\large\n\\require{color}\nP \\left( \\bar{{\\color[rgb]{0.986252,0.007236,0.027423}D}} | \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) = \\frac{P \\left( \\bar{{\\color[rgb]{0.986252,0.007236,0.027423}D } }\\cap \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) }{P \\left( \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) }\n\\]\n\\[\n\\large\n\\require{color}\n= \\frac{1 - P\\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\right) - P \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) + P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) }{1 - P\\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\right) } = \\frac{1 - 0.92 - 0.83 + 0.78}{1 - 0.92} = \\frac{0.03}{0.08} = 0.375\n\\]"
  },
  {
    "objectID": "sample_problems/lecture_1.html#problem-1",
    "href": "sample_problems/lecture_1.html#problem-1",
    "title": "L1 examples",
    "section": "",
    "text": "The probability that a scheduled flight departs on time is 0.83 and the probability that it arrives on time is 0.92. The probability that it both departs and arrives on time is 0.78. Find the probability that\n\nthe plane arrives on time given that it departed on time;\nthe plane did not depart on time given that it did not arrive on time.\n\n\n\nSolution\n\nIt will be useful to consider the Venn diagram shown below.\n\nLet \\(\\require{color}{\\color[rgb]{0.000066,0.001801,0.998229}A}\\) denote the event that the plane arrives on time, while \\({\\color[rgb]{0.986252,0.007236,0.027423}D}\\) denotes th event that the plane departs on time. To construct the Venn diagram above note that \\(P\\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) = 0.78\\). From the sum rule of probabilities, we have:\n\\[\n\\require{color}\n\\large\nP\\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\right) = P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap \\bar{{\\color[rgb]{0.986252,0.007236,0.027423}D}} \\right) + P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right)\n\\]\n\\[\n\\require{color}\n\\large\n0.92= 0.78+ P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right)\n\\]\nwhich implies that \\(\\require{color} P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) = 0.92 - 0.78 = 0.14\\). Similarly, we have:\n\\[\n\\require{color}\n\\large\nP \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) = P \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\cap \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) + P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right)\n\\]\n\\[\n\\require{color}\n\\large\n\\Rightarrow 0.83 = P \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\cap \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) + 0.78\n\\]\nwhich implies that \\(P \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\cap \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) = 0.05\\). With these probabilities, we can now answer the questions.\n\nThe plane arrives on time conditioned that it departed on time:\n\n\\[\n\\large\n\\require{color}\nP \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} | {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) = \\frac{P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) }{P \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) } = \\frac{0.78}{0.83} = 0.94\n\\]\n\nThe plane did not depart on time conditioned on it having not arrived on time:\n\n\\[\n\\large\n\\require{color}\nP \\left( \\bar{{\\color[rgb]{0.986252,0.007236,0.027423}D}} | \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) = \\frac{P \\left( \\bar{{\\color[rgb]{0.986252,0.007236,0.027423}D } }\\cap \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) }{P \\left( \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) }\n\\]\n\\[\n\\large\n\\require{color}\n= \\frac{1 - P\\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\right) - P \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) + P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) }{1 - P\\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\right) } = \\frac{1 - 0.92 - 0.83 + 0.78}{1 - 0.92} = \\frac{0.03}{0.08} = 0.375\n\\]"
  },
  {
    "objectID": "sample_problems/lecture_1.html#problem-2",
    "href": "sample_problems/lecture_1.html#problem-2",
    "title": "L1 examples",
    "section": "Problem 2",
    "text": "Problem 2\nToss a coin three times, what is the probability of at least two heads?\n\n\nSolution\n\nThere are 8 possible outcomes which, if the coin is unbiased, should all be equally likely:\n\nHHH\nHHT\nHTH\nHTT\nTHH\nTHT\nTTH\nTTT\n\nTwo or more heads result from 4 outcomes. The probability of two or more heads is therefore \\(4/8=1/2\\)."
  },
  {
    "objectID": "sample_problems/lecture_1.html#problem-3",
    "href": "sample_problems/lecture_1.html#problem-3",
    "title": "L1 examples",
    "section": "Problem 3",
    "text": "Problem 3\nThis problem introduces the idea that whilst it may be tempting to add probabilities, the context is very important.\nAround 0.9% of the population are blue-green color blind and roughly 1 in 5 is left-handed. Assuming these characteristics are inherited independently, calculate the probability that a person, chosen at random will:\n\nbe both color-blind and left-handed\nbe color-blind and not left-handed\nbe color-blind or left-handed\nbe neither color-blind nor left-handed\n\n\n\nSolution\n\nConsider the diagram shown below; given that the characteristics are inherited independently, each sub-branch of the population can be divided into color-blind and non-color-blind groups.\n\n\nthe probability of being both color-blind and left-handed is: \\(0.009 \\times 0.2 = 0.0018\\) or \\(0.18 \\%\\).\nthe probability of being color-blind and right-handed is: \\(0.009 \\times 0.8 = 0.0072\\).\nthis is the sum of all probabilities within the first branch and the probability calculated in the prior step, i.e., \\(0.20 + 0.0072 = 0.2072\\).\nthis is given by the last group, i.e., \\(0.991 \\times 0.8 = 0.7928\\)."
  },
  {
    "objectID": "sample_problems/lecture_1.html#problem-4",
    "href": "sample_problems/lecture_1.html#problem-4",
    "title": "L1 examples",
    "section": "Problem 4",
    "text": "Problem 4\nThis problem has two parts.\n\nDerive Bayes’ rule.\nThe chance of an honest citizen lying is 1 in 1000. Assume that such a citizen is tested with a lie detector which correctly identifies both truth and false statements 95 times out of 100.\n\n\nWhat is the probability that the lie detector indicates falsehood?\nIn this case, what is the probability that the person is actually lying?\n\n\n\nSolution\n\n\nTo derive Bayes’ rule, we will use the definition of the conditional probability, and the fact that \\(p \\left({\\color[rgb]{0.986252,0.007236,0.027423}A} \\cap {\\color[rgb]{0.131302,0.999697,0.023594}B} \\right) = p \\left( {\\color[rgb]{0.131302,0.999697,0.023594}B} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}A} \\right)\\), which leads to\n\n\\[\n\\large\n\\require{color}\np \\left( {\\color[rgb]{0.986252,0.007236,0.027423}A} | {\\color[rgb]{0.131302,0.999697,0.023594}B} \\right) p \\left( {\\color[rgb]{0.131302,0.999697,0.023594}B} \\right) = p \\left( {\\color[rgb]{0.131302,0.999697,0.023594}B}| {\\color[rgb]{0.986252,0.007236,0.027423}A} \\right) p \\left({\\color[rgb]{0.986252,0.007236,0.027423}A} \\right)\n\\]\nFrom this one can write\n\\[\n\\large\n\\require{color}\np \\left( {\\color[rgb]{0.986252,0.007236,0.027423}A} | {\\color[rgb]{0.131302,0.999697,0.023594}B} \\right) = \\frac{p \\left( {\\color[rgb]{0.131302,0.999697,0.023594}B} | {\\color[rgb]{0.986252,0.007236,0.027423}A} \\right) p \\left( {\\color[rgb]{0.986252,0.007236,0.027423}A} \\right) }{p \\left( {\\color[rgb]{0.131302,0.999697,0.023594}B} \\right) }\n\\]\n\nThe probability that the lie detector indicates a falsehood is based on (i) the citizen is lying, and (ii) the citizen is being honest, but the detector makes an error. Let \\(F\\) be the probability that the lie detector indicates a falsehood. Thus\n\n\\[\n\\large\np \\left( F \\right) = \\frac{1}{1000} \\times 0.95 + \\frac{999}{1000} \\times 0.05 = 0.0509.\n\\]\nLet $p ( L ) be the probability that the person is actually lying. Thus, what we want is\n\\[\n\\large\np \\left( L | F \\right) = \\frac{p \\left( F | L \\right) p \\left( L \\right) }{p \\left( F \\right) } = \\frac{0.95 \\times 0.001}{0.0509} = 0.01866.\n\\]"
  },
  {
    "objectID": "slides/lecture-9/index.html#an-introduction-to-gaussian-processes",
    "href": "slides/lecture-9/index.html#an-introduction-to-gaussian-processes",
    "title": "Lecture 9",
    "section": "An introduction to Gaussian processes",
    "text": "An introduction to Gaussian processes\nA few remarks\n\nGaussian process (GP) models assume that the vector of targets (e.g., \\mathbf{t} ) come from a Gaussian distribution.\nRather than opting for a parametric form of the regression function, in GPs a mean vector and a covariance matrix are selected for this Gaussian.\nFollowing Chapter 2 of Rasmussen and Williams, we shall begin with a noise-free case, followed by the noisy case.\nNotionally, with GPs, we assume that our function is an infinite dimensional Gaussian! However, any subset of this infinite dimensional Gaussian is by definition also Gaussian!"
  },
  {
    "objectID": "slides/lecture-9/index.html#an-introduction-to-gaussian-processes-1",
    "href": "slides/lecture-9/index.html#an-introduction-to-gaussian-processes-1",
    "title": "Lecture 9",
    "section": "An introduction to Gaussian processes",
    "text": "An introduction to Gaussian processes\nGP prior\n\nLet \\boldsymbol{x} \\in \\mathbb{R}^{d} denote a point in d-dimensional space, i.e., \\boldsymbol{x} \\in \\mathcal{X}\nAs with any Gaussian, a GP is typically specified through its mean \\mu\\left( \\mathbf{x} \\right) and a two-point covariance function k \\left( \\boldsymbol{x}, \\boldsymbol{x}' \\right).\nA popular choice for the mean function is \\mu\\left( \\boldsymbol{x} \\right) = 0 for all x \\in \\mathcal{X}.\nCovariance functions are typically parameterized, and a popular choice is the radial basis function (also known as the squared exponential) \nk \\left( \\boldsymbol{x}, \\boldsymbol{x}' \\right) = \\alpha \\; exp \\left(- \\frac{1}{2l^2}  \\left\\Vert  \\boldsymbol{x} - \\boldsymbol{x}' \\right\\Vert_{2}^{2}  \\right)\n where l is the length scale and \\alpha is the amplitude.\nThe function k is referred to as the kernel function."
  },
  {
    "objectID": "slides/lecture-9/index.html#an-introduction-to-gaussian-processes-2",
    "href": "slides/lecture-9/index.html#an-introduction-to-gaussian-processes-2",
    "title": "Lecture 9",
    "section": "An introduction to Gaussian processes",
    "text": "An introduction to Gaussian processes\nVisualizing GP priors\n\nPlotCode\n\n\nDefining a grid of points \\mathcal{X} \\equiv \\left[-2, 2 \\right], and choosing values for \\alpha and l, we can sample vectors \\mathbf{t} from the GP prior \\mathcal{N}\\left(\\mathbf{0}, \\mathbf{C} \\right), where \\mathbf{C}_{ij} = k \\left( \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\right).\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal\nfrom scipy.linalg import cholesky, solve_triangular\nimport seaborn as sns\nsns.set(font_scale=1.0)\nsns.set_style(\"white\")\nsns.set_style(\"ticks\")\npalette = sns.color_palette('deep')\nplt.style.use('dark_background')\n\n\ndef kernel(xa, xb, amp, ll):\n    Xa, Xb = get_tiled(xa, xb)\n    return amp**2 * np.exp(-0.5 * 1./ll**2 * (Xa - Xb)**2 )\n\n\ndef get_tiled(xa, xb):\n    m, n = len(xa), len(xb)\n    xa, xb = xa.reshape(m,1) , xb.reshape(n,1)\n    Xa = np.tile(xa, (1, n))\n    Xb = np.tile(xb.T, (m, 1))\n    return Xa, Xb\n\nX = np.linspace(-2, 2, 150)\ncov_1 = kernel(X, X, 1, 0.1) \nmu_1 = np.zeros((150,))\nprior_1 = multivariate_normal(mu_1, cov_1, allow_singular=True)\n\ncov_2 = kernel(X, X, 0.5, 1)\nmu_2 = np.zeros((150,))\nprior_2 = multivariate_normal(mu_2, cov_2, allow_singular=True)\n\nrandom_samples = 50\n\nfig, ax = plt.subplots(2, figsize=(12,4))\nfig.patch.set_facecolor('#6C757D')\nax[0].set_fc('#6C757D')\nplt.subplot(121)\nplt.plot(X, prior_1.rvs(random_samples).T, alpha=0.5)\nplt.title(r'Samples from GP prior with $\\alpha=1$, $l=0.1$')\nplt.xlabel(r'$x$')\nplt.ylabel(r'$f$')\n#plt.ylabel(r'$\\mathbf{w}_1$')\nfig.patch.set_facecolor('#6C757D')\n\nplt.subplot(122)\nplt.rcParams['axes.facecolor']='#6C757D'\nax[1].set_facecolor('#6C757D')\nplt.plot(X, prior_2.rvs(random_samples).T, alpha=0.5)\nplt.title(r'Samples from GP prior with $\\alpha=0.5$, $l=1$')\nplt.xlabel(r'$x$')\nplt.ylabel(r'$f$')\nplt.savefig('prior.png', dpi=150, bbox_inches='tight', facecolor=\"#6C757D\")\nplt.close()"
  },
  {
    "objectID": "slides/lecture-9/index.html#an-introduction-to-gaussian-processes-3",
    "href": "slides/lecture-9/index.html#an-introduction-to-gaussian-processes-3",
    "title": "Lecture 9",
    "section": "An introduction to Gaussian processes",
    "text": "An introduction to Gaussian processes\nVisualizing GP priors\n\nThe prior covariance function depended only on the difference between pairs of points, i.e., \\left\\Vert \\boldsymbol{x} - \\boldsymbol{x}' \\right\\Vert_{2}^{2}.\nSuch kernels are said to be stationary; an RBF kernel can be written as\n\n\nk \\left( \\boldsymbol{r} \\right) =  \\alpha \\; exp \\left(- \\frac{1}{2l^2}  \\boldsymbol{r}^2 \\right).\n\n\nWe will encounter many kernel functions later on, some of them are stationary, whilst others are not."
  },
  {
    "objectID": "slides/lecture-9/index.html#an-introduction-to-gaussian-processes-4",
    "href": "slides/lecture-9/index.html#an-introduction-to-gaussian-processes-4",
    "title": "Lecture 9",
    "section": "An introduction to Gaussian processes",
    "text": "An introduction to Gaussian processes\nGaussian marginals and conditionals\nClick here."
  },
  {
    "objectID": "slides/lecture-9/index.html#an-introduction-to-gaussian-processes-5",
    "href": "slides/lecture-9/index.html#an-introduction-to-gaussian-processes-5",
    "title": "Lecture 9",
    "section": "An introduction to Gaussian processes",
    "text": "An introduction to Gaussian processes\nNoise-free regression\n\nWe will consider the Olympic winning times dataset again, but this time assume there is no observational sensor noise.\nThe markers denote the training \\left(x_i, f_i \\right) pairs, while the dashed lines denote the locations are which we would like to make predictions. we will use the superscript \\ast to denote points at which we would like to infer predictions \n\\underbrace{\\mathbf{x}=\\left[\\begin{array}{c}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_N \\\\\n\\end{array}\\right], \\; \\;\\; \\mathbf{f}=\\left[\\begin{array}{c}\nf_1\\\\\nf_2\\\\\n\\vdots \\\\\nf_N \\\\\n\\end{array}\\right]}_{\\text{training}}, \\; \\; \\; \\; \\; \\;  \\underbrace{\\mathbf{x}^{\\ast}=\\left[\\begin{array}{c}\nx_1^{\\ast} \\\\\nx_2^{\\ast} \\\\\n\\vdots \\\\\nx_L^{\\ast} \\\\\n\\end{array}\\right], \\; \\;\\; \\mathbf{f}^{\\ast}=\\left[\\begin{array}{c}\nf_1^{\\ast}\\\\\nf_2^{\\ast}\\\\\n\\vdots \\\\\nf_L^{\\ast} \\\\\n\\end{array}\\right]}_{\\text{prediction}}\n where N denotes the number of points in the training set, and L denotes the number of points in the prediction set.\nIt will be useful to combine the winning times (both training and prediction) in the same vector, i.e., \\mathbf{\\hat{f}} = \\left[ \\mathbf{f}, \\mathbf{f}^{\\ast} \\right]^{T}."
  },
  {
    "objectID": "slides/lecture-9/index.html#an-introduction-to-gaussian-processes-6",
    "href": "slides/lecture-9/index.html#an-introduction-to-gaussian-processes-6",
    "title": "Lecture 9",
    "section": "An introduction to Gaussian processes",
    "text": "An introduction to Gaussian processes\nNoise-free regression\nIt will be useful to define three covariance matrices:\n\n\n\nA covariance matrix, \\mathbf{C} \\in \\mathbb{R}^{N\\times N}, on the training data: \n\\mathbf{C}=\\left[\\begin{array}{ccc}\nk\\left(x_{1},x_{1}\\right) & \\ldots & k\\left(x_{1},x_{N}\\right)\\\\\n\\vdots & \\ddots & \\vdots\\\\\nk\\left(x_{N},x_{1}\\right) & \\cdots & k\\left(x_{N},x_{N}\\right)\n\\end{array}\\right]\n\nA covariance matrix, \\mathbf{C}^{\\ast} \\in \\mathbb{R}^{L\\times L}, on the prediction (or test) data: \n\\mathbf{C}^{\\ast}=\\left[\\begin{array}{ccc}\nk\\left(x_{1}^{\\ast},x_{1}^{\\ast}\\right) & \\ldots & k\\left(x_{1}^{\\ast},x_{L}^{\\ast}\\right)\\\\\n\\vdots & \\ddots & \\vdots\\\\\nk\\left(x_{L}^{\\ast},x_{1}^{\\ast}\\right) & \\cdots & k\\left(x_{L}^{\\ast},x_{L}^{\\ast}\\right)\n\\end{array}\\right]\n\n\n\n\nA cross-covariance matrix, \\mathbf{R} \\in \\mathbb{R}^{N \\times L} on the training and testing data: \n\\mathbf{R} = \\left[\\begin{array}{ccc}\nk\\left(x_{1},x_{1}^{\\ast}\\right) & \\ldots & k\\left(x_{1},x_{L}^{\\ast}\\right)\\\\\n\\vdots & \\ddots & \\vdots\\\\\nk\\left(x_{N},x_{1}^{\\ast}\\right) & \\cdots & k\\left(x_{N},x_{L}^{\\ast}\\right)\n\\end{array}\\right]"
  },
  {
    "objectID": "slides/lecture-9/index.html#an-introduction-to-gaussian-processes-7",
    "href": "slides/lecture-9/index.html#an-introduction-to-gaussian-processes-7",
    "title": "Lecture 9",
    "section": "An introduction to Gaussian processes",
    "text": "An introduction to Gaussian processes\nNoise-free regression\n\nAssuming a zero mean, the GP prior over \\hat{\\mathbf{t}} is given by \np \\left( \\mathbf{\\hat{f}} \\right) = \\mathcal{N} \\left( \\mathbf{0}, \\left[\\begin{array}{cc}\n\\mathbf{C} & \\mathbf{R} \\\\\n\\mathbf{R}^{T} & \\mathbf{C}^{\\ast}\n\\end{array}\\right]  \\right)\n\nThis distribution is the complete definition of our model. It tells us how the function values at the training and prediction points co-vary.\nMaking predictions amounts to manipulating this distribution to give a distribution over the function values at the prediction points conditioned on the observed training data, i.e., p \\left( \\mathbf{t}^{\\ast} | \\mathbf{t} \\right)\nFrom our prior foray into Gaussian conditionals, we recognize this to be \n\\begin{aligned}\np \\left( \\mathbf{f}^{\\ast} | \\mathbf{f} \\right) & = \\mathcal{N}\\left( \\boldsymbol{\\mu}^{\\ast}, \\boldsymbol{\\Sigma}^{\\ast} \\right), \\; \\; \\; \\; \\text{where} \\\\\n\\boldsymbol{\\mu}^{\\ast} = \\mathbf{R}^{T} \\mathbf{C}^{-1} \\mathbf{f}, \\; \\; & \\; \\; \\boldsymbol{\\Sigma}^{\\ast} = \\mathbf{C}^{\\ast} - \\mathbf{R}^{T} \\mathbf{C}^{-1} \\mathbf{R}\n\\end{aligned}"
  },
  {
    "objectID": "slides/lecture-9/index.html#an-introduction-to-gaussian-processes-8",
    "href": "slides/lecture-9/index.html#an-introduction-to-gaussian-processes-8",
    "title": "Lecture 9",
    "section": "An introduction to Gaussian processes",
    "text": "An introduction to Gaussian processes\nVisualizing GP posteriors\n\nPlotCode\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal\nfrom scipy.linalg import cholesky, solve_triangular\nimport seaborn as sns\nsns.set(font_scale=1.0)\nsns.set_style(\"white\")\nsns.set_style(\"ticks\")\npalette = sns.color_palette('deep')\nplt.style.use('dark_background')\n\n\ndef kernel(xa, xb, amp, ll):\n    Xa, Xb = get_tiled(xa, xb)\n    return amp**2 * np.exp(-0.5 * 1./ll**2 * (Xa - Xb)**2 )\n\ndef get_tiled(xa, xb):\n    m, n = len(xa), len(xb)\n    xa, xb = xa.reshape(m,1) , xb.reshape(n,1)\n    Xa = np.tile(xa, (1, n))\n    Xb = np.tile(xb.T, (m, 1))\n    return Xa, Xb\n\ndef get_posterior(amp, ll, x, x_data, y_data):\n    u = y_data.shape[0]\n    mu_y = np.mean(y_data)\n    y = (y_data - mu_y).reshape(u,1)\n    \n    Kxx = kernel(x_data, x_data, amp, ll)\n    Kxpx = kernel(x, x_data, amp, ll)\n    Kxpxp = kernel(x, x, amp, ll)\n    \n    # Inverse\n    jitter = np.eye(u) * 1e-8\n    L = cholesky(Kxx + jitter)\n    S1 = solve_triangular(L.T, y, lower=True)\n    S2 = solve_triangular(L.T, Kxpx.T, lower=True).T\n    \n    mu = S2 @ S1  + mu_y\n    cov = Kxpxp - S2 @ S2.T\n    return mu, cov\n\nx_data = np.random.rand(10)*4 - 2.\ny_data = np.cos(5*x_data) + x_data**2 + 2*x_data\nX = np.linspace(-2, 2, 150)\nrandom_samples = 50\n\nfig, ax = plt.subplots(2, figsize=(12,4))\nfig.patch.set_facecolor('#6C757D')\nax[0].set_fc('#6C757D')\nplt.subplot(121)\nmu, cov = get_posterior(1, 0.1, X, x_data, y_data)\nposterior = multivariate_normal(mu.flatten(), cov, allow_singular=True)\n#mu = mu.flatten()\n#std = np.sqrt(np.diag(cov)).flatten()\nplt.plot(x_data, y_data, 'o', ms=12, color='dodgerblue', lw=1, markeredgecolor='w', zorder=3)\nplt.plot(X, posterior.rvs(random_samples).T, alpha=0.5, zorder=2)\nplt.title(r'Samples from GP posterior with $\\alpha=1$, $l=0.1$')\nplt.xlabel(r'$x$')\nplt.ylabel(r'$t$')\n#plt.ylabel(r'$\\mathbf{w}_1$')\nfig.patch.set_facecolor('#6C757D')\n\nplt.subplot(122)\nplt.rcParams['axes.facecolor']='#6C757D'\nax[1].set_facecolor('#6C757D')\nmu2, cov2 = get_posterior(0.5, 1, X, x_data, y_data)\nposterior2 = multivariate_normal(mu2.flatten(), cov2, allow_singular=True)\nplt.plot(x_data, y_data, 'o', ms=12, color='dodgerblue', lw=1, markeredgecolor='w', zorder=3)\nplt.plot(X, posterior2.rvs(random_samples).T, alpha=0.5, zorder=2)\nplt.title(r'Samples from GP posterior with $\\alpha=0.5$, $l=1$')\nplt.xlabel(r'$x$')\nplt.ylabel(r'$t$')\nplt.savefig('posterior.png', dpi=150, bbox_inches='tight', facecolor=\"#6C757D\")\nplt.close()"
  },
  {
    "objectID": "slides/lecture-9/index.html#an-introduction-to-gaussian-processes-9",
    "href": "slides/lecture-9/index.html#an-introduction-to-gaussian-processes-9",
    "title": "Lecture 9",
    "section": "An introduction to Gaussian processes",
    "text": "An introduction to Gaussian processes\nKernel functions\nThe RBF covariance function presented in the prior section is not the only candidate for a kernel function. Consider some of the other ones.\n\nLinearPolynomialCosine\n\n\n\nk \\left( \\boldsymbol{x}, \\boldsymbol{x}' \\right) = \\alpha \\; \\boldsymbol{x}^{T} \\boldsymbol{x}'\n\n\n\n\n\nk \\left( \\boldsymbol{x}, \\boldsymbol{x}' \\right) = \\alpha \\; \\left( 1 + \\boldsymbol{x}^{T} \\boldsymbol{x}' \\right)^{\\gamma}\n\n\n\n\n\nk \\left( \\boldsymbol{x}, \\boldsymbol{x}' \\right) = \\alpha^2 \\; cos\\left( \\frac{2 \\pi}{l^2}   \\left\\Vert  \\boldsymbol{x} - \\boldsymbol{x}' \\right\\Vert_{2}^{2} \\right)\n\n\n\n\n\n\n\nAE8803 | Gaussian Processes for Machine Learning"
  },
  {
    "objectID": "slides/lecture-7/index.html#gaussian-noise-model-3",
    "href": "slides/lecture-7/index.html#gaussian-noise-model-3",
    "title": "Lecture 7",
    "section": "Gaussian noise model",
    "text": "Gaussian noise model\nThis yields\n\n\\mathbb{E}_{p \\left( \\mathbf{t} | \\mathbf{X}, \\mathbf{w}, \\sigma^2 \\right)} \\left[ \\hat{\\mathbf{w}} \\right] = \\left( \\mathbf{X}^{T} \\mathbf{X} \\right)^{-1} \\mathbf{X}^{T} \\mathbf{X w} = \\mathbf{w}.\n\nSo, the expected value of our approximation \\mathbf{\\hat{w}} is the true parameter value \\mathbf{w}. This means that our estimator is unbiased.\nAny potential variability in \\mathbf{\\hat{w}} is captured by its covariance.\n\n\\begin{aligned}\nCov \\left[ \\mathbf{\\hat{w}} \\right] & = \\mathbb{E}_{p \\left( \\mathbf{t} | \\mathbf{X}, \\mathbf{w}, \\sigma^2 \\right)} \\left[\\mathbf{\\hat{w}} \\mathbf{\\hat{w}}^{T} \\right] - \\mathbb{E}_{p \\left( \\mathbf{t} | \\mathbf{X}, \\mathbf{w}, \\sigma^2 \\right)} \\left[ \\mathbf{\\hat{w}}  \\right] \\mathbb{E}_{p \\left( \\mathbf{t} | \\mathbf{X}, \\mathbf{w}, \\sigma^2 \\right)} \\left[ \\mathbf{\\hat{w}}  \\right]^{T} \\\\\n& = \\mathbb{E}_{p \\left( \\mathbf{t} | \\mathbf{X}, \\mathbf{w}, \\sigma^2 \\right)} \\left[\\mathbf{\\hat{w}} \\mathbf{\\hat{w}}^{T} \\right] - \\mathbf{ww}^{T}\n\\end{aligned}\n\\tag{1}\nFocusing solely on the first term, we have \n\\begin{aligned}\n\\mathbb{E}_{p \\left( \\mathbf{t} | \\mathbf{X}, \\mathbf{w}, \\sigma^2 \\right)} \\left[\\mathbf{\\hat{w}} \\mathbf{\\hat{w}}^{T} \\right] & = \\mathbb{E}_{p \\left( \\mathbf{t} | \\mathbf{X}, \\mathbf{w}, \\sigma^2 \\right)} \\left[  \\left( \\left(\\mathbf{X}^{T} \\mathbf{X} \\right)^{-1} \\mathbf{X}^{T} \\mathbf{t}  \\right) \\left( \\left(\\mathbf{X}^{T} \\mathbf{X} \\right)^{-1} \\mathbf{X}^{T} \\mathbf{t}  \\right)^{T} \\right] \\\\\n& =  \\left(\\mathbf{X}^{T} \\mathbf{X} \\right)^{-1} \\mathbf{X}^{T}  \\underbrace{\\mathbb{E}_{p \\left( \\mathbf{t} | \\mathbf{X}, \\mathbf{w}, \\sigma^2 \\right)}\\left[ \\mathbf{tt}^{T} \\right]}_{\\text{need to determine}} \\mathbf{X} \\left( \\mathbf{X}^{T} \\mathbf{X} \\right)^{-1}\n\\end{aligned}\n\\tag{2}"
  },
  {
    "objectID": "slides/lecture-5/index.html#multivariate-gaussians",
    "href": "slides/lecture-5/index.html#multivariate-gaussians",
    "title": "Lecture 5",
    "section": "Multivariate Gaussians",
    "text": "Multivariate Gaussians\nAlthough we have introduced joint probabilities and learnt how to manipulate them in the lectures prior, we have thus far only stuied univariate densities.\nIn this lecture, we will focus on one multivariate density that sets the stage for our journey into machine learning: the Gaussian distribution!"
  },
  {
    "objectID": "slides/lecture-5/index.html#multivariate-gaussians-1",
    "href": "slides/lecture-5/index.html#multivariate-gaussians-1",
    "title": "Lecture 5",
    "section": "Multivariate Gaussians",
    "text": "Multivariate Gaussians\nThe random vector, \\mathbf{X} = \\left(X_1, X_2, \\ldots, X_n \\right) is a multivariate Gaussian \\mathbf{X} \\sim \\mathcal{N} \\left( \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma} \\right) if\n\nf_{\\mathbf{X}} \\left( \\mathbf{x} \\right) = \\frac{1}{\\left( 2 \\pi \\right)^{n/2}} \\left|\\boldsymbol{\\Sigma} \\right|^{-\\frac{1}{2}} exp \\left(-\\frac{1}{2}\\left( \\mathbf{x}-\\boldsymbol{\\mu}\\right)^{T} \\boldsymbol{\\Sigma}^{-1} \\left( \\mathbf{x}-\\boldsymbol{\\mu}\\right) \\right)\n\nwhere\n\n\\boldsymbol{\\Sigma} is a n \\times n covariance matrix\n\\boldsymbol{\\mu} = \\left( \\mu_1, \\mu_2, \\ldots, \\mu_{n} \\right)^{T} is a n \\times 1 mean vector."
  },
  {
    "objectID": "slides/lecture-5/index.html#multivariate-gaussians-2",
    "href": "slides/lecture-5/index.html#multivariate-gaussians-2",
    "title": "Lecture 5",
    "section": "Multivariate Gaussians",
    "text": "Multivariate Gaussians\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nHere\n\n\\boldsymbol{\\mu}=\\left(\\begin{array}{c}\n-2\\\\\n1\n\\end{array}\\right),\\Sigma=\\left(\\begin{array}{cc}\n3 & 0\\\\\n0 & 6\n\\end{array}\\right)\n\nTry adding non-zero entries into the off-diagonal. Note the matrix must be symmetric!\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal\nfrom mpl_toolkits.mplot3d import Axes3D\nimport seaborn as sns\nsns.set(font_scale=1.0)\nsns.set_style(\"white\")\nsns.set_style(\"ticks\")\npalette = sns.color_palette('deep')\n\n#Parameters to set\nmu_x = -2\nvariance_x = 3\n\nmu_y = 1\nvariance_y = 6\n\n#Create grid and multivariate normal\nx = np.linspace(-10,10,500)\ny = np.linspace(-10,10,500)\nX, Y = np.meshgrid(x,y)\npos = np.empty(X.shape + (2,))\npos[:, :, 0] = X; pos[:, :, 1] = Y\nrv = multivariate_normal([mu_x, mu_y], [[variance_x, 0], [0, variance_y]])\n\n#Make a 3D plot\nfig, ax = plt.subplots(subplot_kw=dict(projection='3d'), figsize=(7,8))\nax.plot_surface(X, Y, rv.pdf(pos),cmap='viridis',linewidth=0)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.set_title(r'Probability density, $f_{\\mathbf{X}} (\\mathbf{x})$')\nplt.close()"
  },
  {
    "objectID": "slides/lecture-5/index.html#multivariate-gaussians-3",
    "href": "slides/lecture-5/index.html#multivariate-gaussians-3",
    "title": "Lecture 5",
    "section": "Multivariate Gaussians",
    "text": "Multivariate Gaussians\n\nRemember that \\mathbf{X} is a random vector and its possible values \\mathbf{x} are also vectors.\nThe density, f_{\\mathbf{X}} \\left( \\mathbf{x} \\right), is a scalar-valued function.\nThe coefficient \n\\frac{1}{\\left( 2 \\pi \\right)^{n/2}} \\left|\\boldsymbol{\\Sigma} \\right|^{-\\frac{1}{2}}\n acts as a normalizing constant."
  },
  {
    "objectID": "slides/lecture-5/index.html#covariance-matrix",
    "href": "slides/lecture-5/index.html#covariance-matrix",
    "title": "Lecture 5",
    "section": "Covariance matrix",
    "text": "Covariance matrix\n\nElements of the covariance matrix have the following form:\n\n\n\\left[ \\boldsymbol{\\Sigma} \\right]_{ij} = \\mathbb{E} \\left[ \\left( X_i - \\mu_{i} \\right) \\left( X_j - \\mu_{j} \\right) \\right] = \\mathbb{E} \\left[ X_i X_j \\right] - \\mu_{i}\\mu_{j}.\n\\tag{1}\n\nFollowing Equation 1 it is clear that the diagonal elements are simply the individual variances:\n\n\n\\left[ \\boldsymbol{\\Sigma} \\right]_{ii}  = \\mathbb{E} \\left[ X^2_i \\right] - \\mu^2_{i} = Var \\left(X_i \\right).\n\n\nThe matrix is symmetric with off-diagonal terms being zero when two components X_i and X_j are independent, i.e., \\left[ \\boldsymbol{\\Sigma} \\right]_{ij} = \\mathbb{E} \\left[ X_i \\right] \\mathbb{E} \\left[ X_j \\right] - \\mu_{i} \\mu_{j} = 0."
  },
  {
    "objectID": "slides/lecture-5/index.html#covariance-matrix-1",
    "href": "slides/lecture-5/index.html#covariance-matrix-1",
    "title": "Lecture 5",
    "section": "Covariance matrix",
    "text": "Covariance matrix\n\nWhen the off-diagonal elements are not zero, i.e., when two components X_i and X_j are related, we can introduce a measure called the correlation coefficient\n\n\n\\rho_{ij} = \\frac{\\left[ \\boldsymbol{\\Sigma} \\right]_{ij} }{\\left( Var\\left(X_i \\right)Var\\left(X_j \\right) \\right)^{1/2} }.\n\n\nThis values satisfies -1 \\leq \\rho_{ij} \\leq 1, and depending upon the sign it is said to be either negatively correlated or positively correlated.\nWhen \\rho_{ij}=0, i.e., when there is no correlation, \\left[ \\boldsymbol{\\Sigma} \\right]_{ij}= 0."
  },
  {
    "objectID": "slides/lecture-5/index.html#marginal-distribution",
    "href": "slides/lecture-5/index.html#marginal-distribution",
    "title": "Lecture 5",
    "section": "Marginal distribution",
    "text": "Marginal distribution\n\nIt can be shown that the marginal density of any component \\left(X_1, \\ldots, X_n \\right) of a multivariate Gaussian is a univariate Gaussian.\nTo see this, consider that\n\n\nf_{X_k}\\left( x \\right) = \\int_{-\\infty}^{\\infty} \\ldots \\int_{-\\infty}^{\\infty} f_{\\mathbf{X}} \\left(\\mathbf{x} \\right) dx_1 dx_2 \\ldots dx_{k-1} dx_{k+1} \\ldots dx_{n}\n\n\n= \\frac{1}{\\sqrt{2 \\pi \\left[ \\boldsymbol{\\Sigma} \\right]_{kk} } } exp \\left( \\frac{\\left( x - \\mu_{k} \\right)^2}{2 \\left[ \\boldsymbol{\\Sigma} \\right]_{kk} } \\right)\n\n\nIn practice any partial marginalization of a multivariate Gaussian will yield another multivariate Gaussian (but with reduced dimensions)."
  },
  {
    "objectID": "slides/lecture-5/index.html#marginal-and-conditional-distribution",
    "href": "slides/lecture-5/index.html#marginal-and-conditional-distribution",
    "title": "Lecture 5",
    "section": "Marginal and conditional distribution",
    "text": "Marginal and conditional distribution\n\nLet \\mathbf{X} and \\mathbf{Y} be jointly Gaussian random vectors with marginals \n\\mathbf{X} \\sim \\mathcal{N}\\left(\\boldsymbol{\\mu}_{x}, \\mathbf{A} \\right), \\; \\; \\; \\text{and} \\; \\; \\; \\mathbf{Y} \\sim \\mathcal{N}\\left(\\boldsymbol{\\mu}_{y}, \\mathbf{B} \\right).\n\nWe can write the joint distribution as shown below\n\n\n\\left[\\begin{array}{c}\n\\mathbf{X}\\\\\n\\mathbf{Y}\n\\end{array}\\right]\\sim\\mathcal{N}\\left( \\underbrace{\\left[\\begin{array}{c}\n\\boldsymbol{\\mu}_{x}\\\\\n\\boldsymbol{\\mu}_{y}\n\\end{array}\\right]}_{\\boldsymbol{\\mu}}, \\underbrace{\\left[\\begin{array}{cc}\n\\mathbf{A} & \\mathbf{C}\\\\\n\\mathbf{C}^{T} & \\mathbf{B}\n\\end{array}\\right]}_{\\boldsymbol{\\Sigma}}\\right)\n\n\nThe conditional distribution of \\mathbf{X} given \\mathbf{Y} is\n\n\nf_{\\mathbf{X} | \\mathbf{Y}} \\left( \\mathbf{x}, \\mathbf{y} \\right) = \\mathcal{N} \\left( \\boldsymbol{\\mu}_{x} + \\mathbf{CB}^{-1} \\left(\\mathbf{y} - \\boldsymbol{\\mu}_{y} \\right), \\mathbf{A} - \\mathbf{CB}^{-1} \\mathbf{C}^{T} \\right)\n\n\nAlgebraically, this uses the Schur complement. To explore this further, consider the following schematic."
  },
  {
    "objectID": "slides/lecture-5/index.html#marginal-and-conditional-distribution-1",
    "href": "slides/lecture-5/index.html#marginal-and-conditional-distribution-1",
    "title": "Lecture 5",
    "section": "Marginal and conditional distribution",
    "text": "Marginal and conditional distribution\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nThe joint multivariate Gaussian distribution to the left has mean and covariance:\n\n\\boldsymbol{\\mu}=\\left(\\begin{array}{c}\n0.5\\\\\n0.2\n\\end{array}\\right),\\Sigma=\\left(\\begin{array}{cc}\n1.5 & -1.27\\\\\n-1.27 & 3\n\\end{array}\\right)\n\nAs an example, we wish to work out what f_{X| Y} \\left( x, y=3.7 \\right) is (see code).\nThe conditional is Gaussian!\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal, norm\nimport pandas as pd\nimport seaborn as sns\nsns.set(font_scale=1.0)\nsns.set_style(\"white\")\nsns.set_style(\"ticks\")\npalette = sns.color_palette('deep')\n\nvar_1 = 1.5\nvar_2 = 3.0\nrho = -0.6\noff_diag = np.sqrt(var_1 * var_2) * rho\n\n\nmu = np.array([0.5, 0.2])\ncov = np.array([[var_1, off_diag], \\\n       [off_diag, var_2]])\n\nrv = multivariate_normal(mu, cov)\n\n# Generate random samples from this multivariate normal (largely for plotting!)\ndata = rv.rvs(8500)\ndf = pd.DataFrame({'$x$': data[:,0].flatten(), '$y$': data[:,1].flatten()})\n\n# Now, to plot the conditional distribution of $X_1$ at $X_2=5.0$, we would have\ndef calculate_conditional(mu, cov, yy):\n    new_mu = mu[0] + cov[0,1] * (cov[1,1])**(-1) * (yy - mu[1])\n    new_var =  cov[0,0] - cov[0,1] * (cov[1,1])**(-1) * cov[0,1]\n    return new_mu, new_var\n\ny_new = 3.7\ncond_mu, cond_var = calculate_conditional(mu, cov, y_new)\n\n# Now, to plot the conditional distribution of $X_1$ at $X_2=5.0$, we would have\ndef calculate_conditional(mu, cov, yy):\n    new_mu = mu[0] + cov[0,1] * (cov[1,1])**(-1) * (yy - mu[1])\n    new_var =  cov[0,0] - cov[0,1] * (cov[1,1])**(-1) * cov[0,1]\n    return new_mu, new_var\n\ny_new = 3.7\ncond_mu, cond_var = calculate_conditional(mu, cov, y_new)\n\nX_samples = np.tile( np.linspace(-10, 10, 200).reshape(200,1) , (1, 2))\nX_samples[:,1] = X_samples[:,1]* 0 + y_new\n\nf_X = rv.pdf(X_samples)\nrv2 = multivariate_normal(cond_mu, cond_var)\nf_X1 = rv2.pdf(X_samples[:,0])\n\n# Plot!\ng = sns.JointGrid(data=df, x=\"$x$\", y=\"$y$\", space=0)\ng.plot_joint(sns.kdeplot, fill=True,  cmap=\"turbo\", thresh=0, levels=100)\ng.plot_marginals(sns.kdeplot, color=\"grey\", gridsize=100)\nplt.close()\n\nfig = plt.figure(figsize=(8,3))\nplt.plot(X_samples[:,0], f_X1, 'r-')\nplt.xlabel('$x$')\nplt.title('Conditional distribution of $x$ at $y=3.7$')\nplt.close()"
  },
  {
    "objectID": "slides/lecture-5/index.html#generating-samples",
    "href": "slides/lecture-5/index.html#generating-samples",
    "title": "Lecture 5",
    "section": "Generating samples",
    "text": "Generating samples\nIt will be useful to generate samples from a multivariate Gaussian. To understand how to do this, consider the following setup.\n\nLet \\mathbf{X} \\sim \\mathcal{N} \\left(\\mathbf{0}, \\mathbf{I}\\right). Thus, \\mathbb{E} \\left[ \\mathbf{X} \\right] = \\mathbf{0}, and Cov\\left[ \\mathbf{X} \\right] = \\mathbf{I}.\nNow consider the map given by \\tilde{\\mathbf{x}} = \\mathbf{S} \\mathbf{x} + \\mathbf{b}, where \\mathbf{x} is a particular value from the random variable \\mathbf{X}, where\n\n\\mathbf{S} \\in \\mathbb{R}^{n \\times n} is a matrix;\n\\mathbf{b} \\in \\mathbb{R}^{n} is a vector.\n\nBy linearity of the expectation we can show that\n\n\n\\mathbb{E} \\left[ \\tilde{\\mathbf{X}} \\right] = \\mathbf{b}, \\; \\; \\; \\text{and} \\; \\; \\; Cov   \\left[ \\tilde{\\mathbf{X}} \\right] = \\mathbf{SS}^{T}.\n\n\nThe distribution \\mathcal{N}\\left( \\mathbf{b}, \\mathbf{SS}^{T} \\right) is valid (i.e., it is Gaussian), only if \\mathbf{S} is non-singular, i.e., \\mathbf{SS}^{T} is positive definite.\nIn practice, if we need to generate samples from \\mathcal{N}\\left( \\mathbf{b}, \\mathbf{B} \\right), we would compute the Cholesky decomposition of \\mathbf{B}= \\mathbf{LL}^{T}, and then use \\tilde{\\mathbf{x}} = \\mathbf{b} + \\mathbf{L} \\mathbf{x}.\n\n\n\nAE8803 | Gaussian Processes for Machine Learning"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Overview",
    "section": "",
    "text": "This graduate-level course offers a practical approach to probabilistic learning with Gaussian processes (GPs). GPs represent a powerful set of methods for modeling and predicting a wide variety of spatio-temporal phenomena. Today, they are used for problems that span both regression and classification, with theoretical foundations in Bayesian inference, reproducing kernel Hilbert spaces, eigenvalue problems, and numerical integration. Rather than focus solely on these theoretical foundations, this course balances theory with practical probabilistic programming, using a variety of python-based packages. Moreover, practical engineering problems will also be discussed that see GP models that cut across other areas of machine learning including transfer learning, convolutional networks, and normalizing flows."
  },
  {
    "objectID": "index.html#grading",
    "href": "index.html#grading",
    "title": "Overview",
    "section": "Grading",
    "text": "Grading\nThis course has four assignments; the grades are given below:\n\n\n\n\n\n\n\nAssignment\nGrade percentage (%)\n\n\n\n\nAssignment 1: Take-home mid-term (covering fundamentals) \n20\n\n\nAssignment 2: Build your own GP from scratch for a given dataset\n20\n\n\nAssignment 3: Proposal (data and literature review)\n20\n\n\nAssignment 4: Final project (presentation and notebook)\n40\n\n\n\n\nPre-requisites:\n\nCS1371, MATH2551, MATH2552 (or equivalent)\nWorking knowledge of python including familiarity with numpy and matplotlib libraries.\nWorking local version of python and Jupyter."
  },
  {
    "objectID": "index.html#lectures",
    "href": "index.html#lectures",
    "title": "Overview",
    "section": "Lectures",
    "text": "Lectures\nBelow you will find a list of the lectures that form the backbone of this course. Sub-topics for each lecture will be updated in due course.\n01.08: L1. Introduction & probability fundamentals | Slides | Examples\n\n\n\nContents\n\n\nCourse overview.\nProbability fundamentals (and Bayes’ theorem).\nRandom variables.\n\n\n01.10: L2. Discrete probability distributions | Slides | Examples | Notebook\n\n\nContents\n\n\nExpectation and variance.\nIndependence.\nBernoulli and Binomial distributions.\n\n\n01.15: No Class (Institute Holiday)\n01.17: L3. Continuous distributions | Slides | Examples\n\n\n\nContents\n\n\nFundamentals of continuous random variables.\nProbability density function.\nGaussian and Beta distributions.\n\n\n01.22: L4. Manipulating and combining distributions | Slides | Examples\n\n\nContents\n\n\nFunctions of random variables.\nSums of random variables.\n\n\n01.24: No Class\n01.29: L5. Multivariate Gaussian distributions | Slides\n\n\nContents\n\n\nMarginal distributions.\nConditional distributions.\nJoint distribution and Schur complement.\n\n\n01.31: L6. Linear modelling | Slides\n\n\nContents\n\n\nLeast squares.\nRegularization.\nGaussian noise model.\n\n\n\n02.05: L7. Towards Bayesian Inference | Slides\n\n\nContents\n\n\nPosterior mean and covariance for a linear model.\nFisher information matrix.\nBayesian model introduction.\nPosterior definition.\n\n\n02.07: L8. Bayesian inference in action | Slides\n\n\nContents\n\n\nAnalytical calculation of the posterior\nConjugacy in Bayesian inference\nA function-space perspective\n\n\n02.12:  Fundamentals Mid-term (take-home)\n02.12: L9. An introduction to Gaussian Processes | Slides | Notebook\n\n\nContents\n\n\nGaussian process prior\nNoise-free regression\nKernel functions\nMidterm overview\n\n\n02.14: L10. More on Gaussian Processes and Kernels | Slides | Notebook\n\n\nContents\n\n\nNoisy regression\nMore about kernels\nKernel trick\n\n\n02.19: L10. Open-source resources\n\n\nContents\n\n\nIntroduction to pymc, gpytorch, gpflow, GPjax.\nOverview of pandas\n\n\n02.21: L11. Hyperparameters\n\n\nContents\n\n\nHyperparameter optimization 2. Distance metrics 3. Markov chain Monte Carlo and variational inference\n\n\n02.26: L12. Gaussian process classification\n\n\nContents\n\n\nBernoulli prior\nSoftmax for multi-class classification\n\n\n02.28: L13. Scaling up Gausssian processes I\n\n\nContents\n\n\nReview of matrix inverse via Cholesky.\nSubset of data approaches\nNystrom approximation\nInducing points\nKronecker product kernels.\n\n\n03.04: L14. Scaling up Gausssian processes II\n\n\nContents\n\n\nVariational inference\nELBO derivation\nMinimizing the KL-divergence practically using Adam.\n\n\n03.06: Coding assignment due\n03.06: L15. Sparse (and subspace-based) Gaussian processes\n\n\nContents\n\n\nBrief introduction to matrix manifolds.\nSubspace-based projections.\nActive subspaces.\nSparsity promoting priors.\n\n\n03.11: L16. Reproducing Kernel Hilbert Spaces\n\n\nContents\n\n\nProject overview\nHilbert space\nUnderstanding a kernel.\nReproducing kernel Hilbert spaces.\nRepresenter theoreom.\n\n\n03.13: L17. Multi-output and deep Gaussian processes\n\n\nContents\n\n\nCoregional models.\nTransfer learning across covariance blocks.\nDerivative (or gradient) enhancement.\nDepth in Gaussian processes.\nPosterior inference and stochastic variational inference\n\n\n03.13: Withdrawal Deadline\n03.18-03.22: Spring Break\n03.25: Project proposals due\n03.25: L19. Convolutional Gaussian processes\n\n\nContents\n\n\nConvolution as a linear operator.\nDeep convolutional Gaussian processes.\n\n\n03.27: L20. Latent models and unsupervised learning\n\n\nContents\n\n\nContrast standard regression with latent variable model.\nGaussian process latent variable model.\nCoding demo.\n\n\n04.01: L21. State-space Gaussian processes\n\n\nContents\n\n\nApplication: time series models.\nGaussian state space model.\nParallels with Kalman filtering and smoothing.\nCreating custom state-space kernels.\n\n\n04.03: L22. Bayesian optimization\n\n\nContents\n\n\nGaussian process surrogate.\nAcquisition function.\nThompson’s sampling.\nGaussian process dynamic model.\n\n\n04.08: L23. Guest Lecture\n04.22: L24. Project presentations"
  },
  {
    "objectID": "index.html#office-hours",
    "href": "index.html#office-hours",
    "title": "Overview",
    "section": "Office hours",
    "text": "Office hours\nProfessor Seshadri’s office hours:\n\n\n\nLocation\nTime\n\n\n\n\nMK 421\nFridays 14:30 to 15:30"
  },
  {
    "objectID": "index.html#textbooks",
    "href": "index.html#textbooks",
    "title": "Overview",
    "section": "Textbooks",
    "text": "Textbooks\nThis course will make heavy use of the following texts:\n\nRasmussen, C. E., Williams, C. K. Gaussian Processes for Machine Learning, The MIT Press, 2006.\nMurphy, K. P., Probabilistic Machine Learning: Advanced Topics, The MIT Press, 2023.\n\nBoth these texts have been made freely available by the authors."
  },
  {
    "objectID": "index.html#important-papers",
    "href": "index.html#important-papers",
    "title": "Overview",
    "section": "Important papers",
    "text": "Important papers\nStudents are encouraged to read through the following papers:\n\nRoberts, S., Osborne, M., Ebden, M., Reece, S., Gibson, N., Aigrain, S., (2013) Gaussian processes for time-series modelling, Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences.\nDunlop, M., Girolami, M., Stuart, A., Teckentrup, A., (2018) How Deep Are Deep Gaussian Processes?, Journal of Machine Learning Research 19, 1-46\nAlvarez, M., Lawrence, N., (2011) Computationally Efficient Convolved Multiple Output Gaussian Processes, Journal of Machine Learning Research 12, 1459-1500\nVan der Wilk, M., Rasmussen, C., Hensman, J., (2017) Convolutional Gaussian Processes, 31st Conference on Neural Information Processing Systems"
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Overview",
    "section": "References",
    "text": "References\nMaterial used in this course has been adapted from\n\nCUED Part IB probability course notes\nAlto University’s module on Gaussian Processes\nSlides from the Gaussian Process Summer Schools"
  },
  {
    "objectID": "slides/lecture-6/index.html#the-three-model-levels",
    "href": "slides/lecture-6/index.html#the-three-model-levels",
    "title": "Lecture 6",
    "section": "The three model levels",
    "text": "The three model levels\nIn this lecture, we will explore three distinct but related flavors of modelling.\n\nLinear least squares model\nGaussian noise model (introducing the idea of likelihood)\nFull Bayesian treatment (next time!)\n\n\nMuch of the exposition shown here is based on the first three chapters of Rogers and Girolami’s, A First Course in Machine Learning."
  },
  {
    "objectID": "slides/lecture-6/index.html#linear-least-squares",
    "href": "slides/lecture-6/index.html#linear-least-squares",
    "title": "Lecture 6",
    "section": "Linear least squares",
    "text": "Linear least squares\n\nConsider the data shown in the plot below. It shows the winning times for the men’s 100 meter race at the Summer Olympics for many years.\n\n\n\n\n\n\nOur goal will be to fit a model to this data. To begin, we will consider a linear model, i.e., \nt = f \\left( x; {\\color{blue}{w_0}}, {\\color{blue}{w_1}} \\right) = {\\color{blue}{w_0}} + {\\color{blue}{w_1}} x\n where x is the year and t is the winning time.\n{\\color{blue}{w_0}} and {\\color{blue}{w_1}} are unknown model parameters that we need to ascertain.\nGood sense would suggest that the best line passes as closely as possible through all the data points on the left."
  },
  {
    "objectID": "slides/lecture-6/index.html#linear-least-squares-1",
    "href": "slides/lecture-6/index.html#linear-least-squares-1",
    "title": "Lecture 6",
    "section": "Linear least squares",
    "text": "Linear least squares\nDefining a good model\n\nOne common strategy for defining this is based on the squared distance between the truth and the model. Thus, for a given year, t_i, this is written as: \n\\mathcal{L}_{i} = \\left( t_i - f \\left( x_i; {\\color{blue}{w_0}}, {\\color{blue}{w_1}} \\right) \\right)^2.\n\nHowever, as we want a model that fits well across all the data, we may consider the average across the entire data set, i.e., all N data points. This is given by: \n\\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^{N} \\mathcal{L}_{i} = \\frac{1}{N} \\sum_{i=1}^{N} \\left( t_i - f \\left( x_i; {\\color{blue}{w_0}}, {\\color{blue}{w_1}} \\right) \\right)^2\n\nNote that this loss function is always positive, and the lower it is the better! Finding optimal values for {\\color{blue}{w_0}}, {\\color{blue}{w_1}} can be expressed as \n\\underset{{\\color{blue}{w_0}}, {\\color{blue}{w_1}}}{argmin} \\; \\; \\frac{1}{N} \\sum_{i=1}^{N} \\left( t_i - f \\left( x_i; {\\color{blue}{w_0}}, {\\color{blue}{w_1}} \\right) \\right)^2\n\n\n\nNote that other loss functions can be considered. A common example is the absolute loss, i.e., \\mathcal{L}_{i} = | t_i - f \\left( x_i; {\\color{blue}{w_0}}, {\\color{blue}{w_1}} \\right)|"
  },
  {
    "objectID": "slides/lecture-6/index.html#linear-least-squares-2",
    "href": "slides/lecture-6/index.html#linear-least-squares-2",
    "title": "Lecture 6",
    "section": "Linear least squares",
    "text": "Linear least squares\nMatrix-vector notation\n\nIt will be very useful to work with vectors and matrices. For convenience, we define: \n\\mathbf{X}=\\left[\\begin{array}{cc}\n1 & x_{1}\\\\\n1 & x_{2}\\\\\n\\vdots & \\vdots\\\\\n1 & x_{N}\n\\end{array}\\right] = \\left[\\begin{array}{c}\n\\mathbf{x}_{1}^{T}\\\\\n\\mathbf{x}_{2}^{T}\\\\\n\\vdots \\\\\n\\mathbf{x}_{N}^{T}\n\\end{array}\\right], \\; \\; \\; \\; \\; \\mathbf{t} =\\left[\\begin{array}{c}\nt_{1}\\\\\nt_{2}\\\\\n\\vdots\\\\\nt_{N}\n\\end{array}\\right], \\; \\; \\; \\; \\; \\mathbf{{\\color{blue}{w}}} = \\left[\\begin{array}{c}\n{\\color{blue}{w_0}}\\\\\n{\\color{blue}{w_1}}\n\\end{array}\\right]\n\nThe loss function from the prior slide is equivalent to writing \n\\mathcal{L} = \\frac{1}{N} \\left( \\mathbf{t} - \\mathbf{X} \\mathbf{{\\color{blue}{w}}} \\right)^{T} \\left( \\mathbf{t} - \\mathbf{X} \\mathbf{{\\color{blue}{w}}} \\right).\n\nThis can be expanded to yield \n\\mathcal{L} = \\frac{1}{N} \\left( \\mathbf{t}^{T} - \\left( \\mathbf{X} \\mathbf{{\\color{blue}{w}}} \\right)^T  \\right)\\left( \\mathbf{t} - \\mathbf{X} \\mathbf{{\\color{blue}{w}}} \\right) = \\frac{1}{N} \\left[ \\mathbf{t}^{T} \\mathbf{t} - 2 \\mathbf{t}^{T} \\mathbf{X}  \\mathbf{{\\color{blue}{w}}} +   \\mathbf{{\\color{blue}{w}}}^T \\mathbf{X}^{T} \\mathbf{X} \\mathbf{{\\color{blue}{w}}} \\right]"
  },
  {
    "objectID": "slides/lecture-6/index.html#linear-least-squares-3",
    "href": "slides/lecture-6/index.html#linear-least-squares-3",
    "title": "Lecture 6",
    "section": "Linear least squares",
    "text": "Linear least squares\nMinimizing the loss\n\nAs our objective is to minimize the loss, the obvious idea is to find out for which \\mathbf{{\\color{blue}{w}}}, the derivative of the loss function, \\partial \\mathcal{L} / \\partial \\mathbf{{\\color{blue}{w}}}, goes to zero.\nNote that in practice, we refer to these points as turning points as they may equally correspond to maxima, minima, or saddle points. A positive second derivative is a sure sign of a minima.\nPrior to working out the derivatives, it will be useful to take note of the following identities on the left below.\n\n\n\n\n\n\n\n\n\n\ng \\left( \\mathbf{{\\color{red}{v}}} \\right)\n\\partial g / \\partial \\mathbf{{\\color{red}{v}}}\n\n\n\n\n\\mathbf{{\\color{red}{v}}}^{T}\\mathbf{x}\n\\mathbf{x}\n\n\n\\mathbf{x}^{T} \\mathbf{{\\color{red}{v}}}\n\\mathbf{x}\n\n\n\\mathbf{{\\color{red}{v}}}^{T} \\mathbf{{\\color{red}{v}}}\n2\\mathbf{{\\color{red}{v}}}\n\n\n\\mathbf{{\\color{red}{v}}}^{T} \\mathbf{C} \\mathbf{{\\color{red}{v}}}\n2\\mathbf{C} \\mathbf{{\\color{red}{v}}}\n\n\n\n\n\nThe derivative of the loss function is given by \n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{{\\color{blue}{w}}}} = - \\frac{2}{N} \\mathbf{X}^{T} \\mathbf{t} + \\frac{2}{N} \\mathbf{X}^{T} \\mathbf{X} \\mathbf{{\\color{blue}{w}}}\n\nSetting the derivative to zero, we have \n\\mathbf{X}^{T} \\mathbf{X} \\mathbf{{\\color{blue}{w}}} = \\mathbf{X}^{T} \\mathbf{t} \\; \\; \\; \\Rightarrow \\; \\; \\; \\hat{\\mathbf{{\\color{blue}{w}}}} = \\left( \\mathbf{X}^{T} \\mathbf{X} \\right)^{-1} \\mathbf{X}^{T} \\mathbf{t}\n where \\hat{\\mathbf{{\\color{blue}{w}}}} represents the value of \\mathbf{{\\color{blue}{w}}} that minimizes the loss."
  },
  {
    "objectID": "slides/lecture-6/index.html#linear-least-squares-4",
    "href": "slides/lecture-6/index.html#linear-least-squares-4",
    "title": "Lecture 6",
    "section": "Linear least squares",
    "text": "Linear least squares\n\n\n\nPlotCode\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(font_scale=1.0)\nsns.set_style(\"white\")\nsns.set_style(\"ticks\")\npalette = sns.color_palette('deep')\n\ndf = pd.read_csv('notebooks/data/data100m.csv')\ndf.columns=['Year', 'Time']\nN = df.shape[0]\n\nmax_year, min_year = df['Year'].values.max() , df['Year'].values.min()\n\nx = (df['Year'].values.reshape(N,1) - min_year)/(max_year - min_year)\nt = df['Time'].values.reshape(N,1)\nX_func = lambda u : np.hstack([np.ones((u.shape[0],1)), u])\nX = X_func(x)\nw_hat = np.linalg.inv(X.T @ X) @ X.T @ t\nloss_func = 1./N * (t - X @ w_hat).T @ (t - X @ w_hat)\n\nxgrid = np.linspace(0, 1, 100).reshape(100,1)\nXg = X_func(xgrid)\ntime_grid = Xg @ w_hat\n\nfig = plt.figure(figsize=(6,4))\nplt.plot(df['Year'].values, df['Time'].values, 'o', color='crimson', label='Data')\nplt.plot(xgrid*(max_year - min_year) + min_year, time_grid, '-', color='dodgerblue', label='Model')\nplt.xlabel('Year')\nplt.ylabel('Time (seconds)')\nloss_title = r'Loss function, $\\mathcal{L}=$'+str(np.around(float(loss_func), 5))+'; \\t norm of $\\hat{\\mathbf{w}}$='+str(np.around(float(np.linalg.norm(w_hat,2)), 3))\nplt.title(loss_title)\nplt.legend()\nplt.savefig('olympics_0.png', dpi=150, bbox_inches='tight')\nplt.close()\n\n\n\n\n\n\nFor this result we set \n\\mathbf{X}=\\left[\\begin{array}{cc}\n1 & x_{1}\\\\\n1 & x_{2}\\\\\n\\vdots & \\vdots\\\\\n1 & x_{N}\n\\end{array}\\right]\n and solve for \\hat{\\mathbf{{\\color{blue}{w}}}} = \\left( \\mathbf{X}^{T} \\mathbf{X} \\right)^{-1} \\mathbf{X}^{T} \\mathbf{t}.\n\n\n\nOnce these weights are obtained, we can extrapolate (blue line) over the years.\n\n\n\nNote the graph title shows the loss function value and the L_2 norm, \\left\\Vert\\hat{\\mathbf{{\\color{blue}{w}}}}\\right\\Vert_{2}."
  },
  {
    "objectID": "slides/lecture-6/index.html#linear-least-squares-5",
    "href": "slides/lecture-6/index.html#linear-least-squares-5",
    "title": "Lecture 6",
    "section": "Linear least squares",
    "text": "Linear least squares\n\n\n\nPlotCode\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(font_scale=1.0)\nsns.set_style(\"white\")\nsns.set_style(\"ticks\")\npalette = sns.color_palette('deep')\n\ndf = pd.read_csv('notebooks/data/data100m.csv')\ndf.columns=['Year', 'Time']\nN = df.shape[0]\n\nmax_year, min_year = df['Year'].values.max() , df['Year'].values.min()\n\nx = (df['Year'].values.reshape(N,1) - min_year)/(max_year - min_year)\nt = df['Time'].values.reshape(N,1)\nX_func = lambda u : np.hstack([np.ones((u.shape[0],1)), u , u**2, u**3])\nX = X_func(x)\nw_hat = np.linalg.inv(X.T @ X) @ X.T @ t\nloss_func = 1./N * (t - X @ w_hat).T @ (t - X @ w_hat)\n\nxgrid = np.linspace(0, 1, 100).reshape(100,1)\nXg = X_func(xgrid)\ntime_grid = Xg @ w_hat\n\nfig = plt.figure(figsize=(6,4))\nplt.plot(df['Year'].values, df['Time'].values, 'o', color='crimson', label='Data')\nplt.plot(xgrid*(max_year - min_year) + min_year, time_grid, '-', color='dodgerblue', label='Model')\nplt.xlabel('Year')\nplt.ylabel('Time (seconds)')\nloss_title = r'Loss function, $\\mathcal{L}=$'+str(np.around(float(loss_func), 5))+'; \\t norm of $\\hat{\\mathbf{w}}$='+str(np.around(float(np.linalg.norm(w_hat,2)), 3))\nplt.title(loss_title)\nplt.legend()\nplt.savefig('olympics_3.png', dpi=150, bbox_inches='tight')\nplt.close()\n\n\n\n\n\n\nFor this result we set \n\\mathbf{X}=\\left[\\begin{array}{cccc}\n1 & x_{1} & x_{1}^2 & x_{1}^3\\\\\n1 & x_{2} & x_{2}^2 & x_{2}^3\\\\\n\\vdots & \\vdots & \\vdots & \\vdots\\\\\n1 & x_{N} & x_{N}^2 & x_{N}^3\n\\end{array}\\right]\n and solve for \\hat{\\mathbf{{\\color{blue}{w}}}} = \\left( \\mathbf{X}^{T} \\mathbf{X} \\right)^{-1} \\mathbf{X}^{T} \\mathbf{t}.\n\n\n\nOnce these weights are obtained, we can extrapolate (blue line) over the years.\n\n\n\nNote the graph title shows the loss function value and the L_2 norm, \\left\\Vert\\hat{\\mathbf{{\\color{blue}{w}}}}\\right\\Vert_{2}."
  },
  {
    "objectID": "slides/lecture-6/index.html#linear-least-squares-6",
    "href": "slides/lecture-6/index.html#linear-least-squares-6",
    "title": "Lecture 6",
    "section": "Linear least squares",
    "text": "Linear least squares\n\n\n\nPlotCode\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(font_scale=1.0)\nsns.set_style(\"white\")\nsns.set_style(\"ticks\")\npalette = sns.color_palette('deep')\n\ndf = pd.read_csv('notebooks/data/data100m.csv')\ndf.columns=['Year', 'Time']\nN = df.shape[0]\n\nmax_year, min_year = df['Year'].values.max() , df['Year'].values.min()\n\nx = (df['Year'].values.reshape(N,1) - min_year)/(max_year - min_year)\nt = df['Time'].values.reshape(N,1)\nX_func = lambda u : np.hstack([np.ones((u.shape[0],1)), u , u**2, u**3])\nX = X_func(x)\nw_hat = np.linalg.inv(X.T @ X) @ X.T @ t\nloss_func = 1./N * (t - X @ w_hat).T @ (t - X @ w_hat)\n\nxgrid = np.linspace(0, 1, 100).reshape(100,1)\nXg = X_func(xgrid)\ntime_grid = Xg @ w_hat\n\nfig = plt.figure(figsize=(6,4))\nplt.plot(df['Year'].values, df['Time'].values, 'o', color='crimson', label='Data')\nplt.plot(xgrid*(max_year - min_year) + min_year, time_grid, '-', color='dodgerblue', label='Model')\nplt.xlabel('Year')\nplt.ylabel('Time (seconds)')\nloss_title = r'Loss function, $\\mathcal{L}=$'+str(np.around(float(loss_func), 5))+'; \\t norm of $\\hat{\\mathbf{w}}$='+str(np.around(float(np.linalg.norm(w_hat,2)), 3))\nplt.title(loss_title)\nplt.legend()\nplt.savefig('olympics_8.png', dpi=150, bbox_inches='tight')\nplt.close()\n\n\n\n\n\n\nFor this result we set \n\\mathbf{X}=\\left[\\begin{array}{cccccc}\n1 & x_{1} & x_{1}^2 & x_{1}^3 & \\ldots & x_{1}^{8} \\\\\n1 & x_{2} & x_{2}^2 & x_{2}^3 & \\ldots & x_{2}^{8} \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\ldots & \\vdots \\\\\n1 & x_{N} & x_{N}^2 & x_{N}^3 & \\ldots & x_{N}^{8} \\\\\n\\end{array}\\right]\n and solve for \\hat{\\mathbf{{\\color{blue}{w}}}} = \\left( \\mathbf{X}^{T} \\mathbf{X} \\right)^{-1} \\mathbf{X}^{T} \\mathbf{t}.\n\n\n\nOnce these weights are obtained, we can extrapolate (blue line) over the years.\n\n\n\nNote the graph title shows the loss function value and the L_2 norm, \\left\\Vert\\hat{\\mathbf{{\\color{blue}{w}}}}\\right\\Vert_{2}."
  },
  {
    "objectID": "slides/lecture-6/index.html#linear-least-squares-7",
    "href": "slides/lecture-6/index.html#linear-least-squares-7",
    "title": "Lecture 6",
    "section": "Linear least squares",
    "text": "Linear least squares\nWith regularization\n\nThere is clearly a trade-off between the:\n\nComplexity of the model in terms of the number of weights, and\nthe value of the loss function.\n\nThere is also the risk of over-fitting to the data. For instance, if we had only 9 data points, then the last model would have interpolated each point, at the risk of not being generalizable.\nAs we do not want our model to be too complex, there are two relatively simple recipes:\n\nSplit the data into test and train (your homework!)\nAdd a regularization term, i.e., \n\\mathcal{L} = \\mathcal{L} + \\lambda \\mathbf{{\\color{blue}{w}}}^{T} \\mathbf{{\\color{blue}{w}}}\n where \\lambda is a constant."
  },
  {
    "objectID": "slides/lecture-6/index.html#maximum-likelihood",
    "href": "slides/lecture-6/index.html#maximum-likelihood",
    "title": "Lecture 6",
    "section": "Maximum likelihood",
    "text": "Maximum likelihood\n\nThe linear model from before is unable to capture each data point, and there are errors between the true data and the model.\nNow we will consider a paradigm where these errors are explicitly modelled.\nWe consider a model of the form \nt_j = f \\left( \\mathbf{x}_{n}; \\mathbf{{\\color{blue}{w}}} \\right) + \\epsilon_{n} \\; \\; \\; \\epsilon_{n} \\sim \\mathcal{N}\\left(0, \\sigma^2 \\right), \\; \\; \\; \\; \\text{where} \\; j \\in \\left[1, N \\right]\n\\tag{1}\nRecall, we had previously learnt that adding a constant to a Gaussian random variable alters its mean. Thus, the random variable t_j has a probability density function \np \\left( t_j | \\mathbf{x}_{j}, \\mathbf{{\\color{blue}{w}}}, \\sigma^2 \\right) = \\mathcal{N} \\left( \\mathbf{{\\color{blue}{w}}}^{T} \\mathbf{x}_{j} , \\sigma^2\\right)\n\nCarefully note the conditioning: the probability density function for t_j depends on particular values of \\mathbf{x}_{j} and \\mathbf{{\\color{blue}{w}}}."
  },
  {
    "objectID": "slides/lecture-6/index.html#maximum-likelihood-1",
    "href": "slides/lecture-6/index.html#maximum-likelihood-1",
    "title": "Lecture 6",
    "section": "Maximum likelihood",
    "text": "Maximum likelihood\nDefining the likelihood\nIf we evaluate the linear model from before, and assume that in Equation 1 \\sigma^2 = 0.05, we would find that \np \\left( t_j | \\mathbf{x}_{j} = \\left[ 1, 1980 \\right]^{T} ,\\mathbf{{\\color{blue}{w}}} = \\left[10.964, -1.31 \\right]^{T}, \\sigma^2 = 0.05 \\right) = \\mathcal{N} \\left( 10.03, 0.05 \\right)  \n This quantity is known as the likelihood of the n-th data point.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nNote that for a continuous random variable t, p\\left( t \\right) cannot be interpreted as a probability.\nThe height of the curve to the left tells us how likely it is that we observe a particular t for x=1980.\nThe most likely is B, followed by C and then A. Note the actual winning time is t_{n}=10.25.\nWhile we obviously cannot change the actual winning time, we can change \\mathbf{{\\color{blue}{w}}} and \\sigma^2 to move the density to make it as high as possible at t=10.25.\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom scipy.stats import multivariate_normal\n\n# Get the data \ndf = pd.read_csv('notebooks/data/data100m.csv')\ndf.columns=['Year', 'Time']\nN = df.shape[0]\nmax_year, min_year = df['Year'].values.max() , df['Year'].values.min()\nx = (df['Year'].values.reshape(N,1) - min_year)/(max_year - min_year)\nt = df['Time'].values.reshape(N,1)\nX_func = lambda u : np.hstack([np.ones((u.shape[0],1)), u])\nX = X_func(x)\nw_hat = np.linalg.inv(X.T @ X) @ X.T @ t\n\n# Specific year!\nyear_j = 1980\nX_j = X_func(np.array( [ (year_j - min_year) / (max_year - min_year) ] ).reshape(1,1) )\ntime_j = float(X_j @ w_hat)\n\nT_1980 = multivariate_normal(time_j, 0.05)\nti = np.linspace(9, 11, 100)\npt_x = T_1980.pdf(ti)\n\nfig = plt.figure(figsize=(7,3))\nplt.plot(ti, pt_x, '-', color='orangered', lw=3, label='From linear model')\nplt.axvline(9.53, linestyle='-.', color='dodgerblue', label='A')\nplt.axvline(10.08, linestyle='-', color='green', label='B')\nplt.axvline(10.40, linestyle='--', color='navy', label='C')\nplt.xlabel('Time (seconds)')\nplt.ylabel(r'$p \\left( t | x \\right)$')\nplt.title(r'For $x=1980$')\nplt.legend()\nplt.close()"
  },
  {
    "objectID": "slides/lecture-6/index.html#maximum-likelihood-2",
    "href": "slides/lecture-6/index.html#maximum-likelihood-2",
    "title": "Lecture 6",
    "section": "Maximum likelihood",
    "text": "Maximum likelihood\nDefining the likelihood\n\nThis idea of finding parameters that can maximize the likelihood is very important in machine learning.\nHowever, in general, we are seldom interested in the likelihood of an isolated data point – we are interested in the likelihood across all the data.\nThis leads to the conditional distribution across all N data points \np \\left( t_1, \\ldots, t_N | \\mathbf{x}_1, \\ldots, \\mathbf{x}_{N}, \\mathbf{{\\color{blue}{w}}}, \\sigma^2 \\right)\n\nIf we assume the noise at each data point is independent, this conditional density can be factorized into N separate terms \n\\mathcal{L} = p  \\left( \\mathbf{t} | \\mathbf{X}, \\mathbf{{\\color{blue}{w}}}, \\sigma^2 \\right) = \\prod_{j=1}^{N} p \\left( t_j | \\mathbf{x}_{j}, \\mathbf{{\\color{blue}{w}}}, \\sigma^2 \\right) = \\prod_{j=1}^{N} \\mathcal{N} \\left(\\mathbf{{\\color{blue}{w}}}^{T} \\mathbf{x}_{n} \\right).\n\\tag{2}\nNote that the t_j values are not completely independent—times have clearly decreased over the years! They are conditionally independent. For a given value of \\mathbf{{\\color{blue}{w}}} the t_j are independent; otherwise they are not.\nWe will now maximize the likelihood (see Equation 2 )."
  },
  {
    "objectID": "slides/lecture-6/index.html#maximum-likelihood-3",
    "href": "slides/lecture-6/index.html#maximum-likelihood-3",
    "title": "Lecture 6",
    "section": "Maximum likelihood",
    "text": "Maximum likelihood\nMaximizing the logarithm of the likelihood\n\nPlugging in the definition of a Gaussian probability density function into Equation 2 we arrive at \n\\mathcal{L} = \\prod_{j=1}^{N} \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} exp \\left( -\\frac{1}{2 \\sigma^2} \\left(t_j - f \\left( \\mathbf{x}_{j}; \\mathbf{{\\color{blue}{w}}} \\right) \\right)^2  \\right)\n\nTaking the logarithm on both sides and simplifying: \nlog \\left( \\mathcal{L} \\right)  = \\sum_{j=1}^{N} log \\left( \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} exp \\left( -\\frac{1}{2 \\sigma^2} \\left(t_j - f \\left( \\mathbf{x}_{j}; \\mathbf{{\\color{blue}{w}}} \\right) \\right)^2  \\right) \\right)\n \n= \\sum_{j=1}^{N}  \\left( -\\frac{1}{2} log \\left( 2 \\pi \\right) - log \\left(\\sigma \\right) - \\frac{1}{2\\sigma^2} \\left( t_j - f \\left(\\mathbf{x}_{j}; \\mathbf{{\\color{blue}{w}}} \\right) \\right)^2 \\right)\n \n= -\\frac{N}{2} log \\left( 2 \\pi \\right) - N \\; log \\left( \\sigma \\right) - \\frac{1}{2 \\sigma^2} \\sum_{j=1}^{N} \\left(t_j - f \\left(\\mathbf{x}_{j}; \\mathbf{{\\color{blue}{w}}} \\right) \\right)^2"
  },
  {
    "objectID": "slides/lecture-6/index.html#maximum-likelihood-4",
    "href": "slides/lecture-6/index.html#maximum-likelihood-4",
    "title": "Lecture 6",
    "section": "Maximum likelihood",
    "text": "Maximum likelihood\nMaximizing the logarithm of the likelihood\n\nJust as we did earlier, with the least squares solution, we can set the derivative of the logarithm of the loss function to be zero. \n\\frac{\\partial \\; log \\left( \\mathcal{L} \\right) }{\\partial \\mathbf{{\\color{blue}{w}}}} = \\frac{1}{\\sigma^2} \\sum_{j=1}^{N} \\mathbf{x}_{j} \\left( t_{j} - \\mathbf{x}_{j}^{T} \\mathbf{{\\color{blue}{w}}} \\right) = \\frac{1}{\\sigma^2} \\sum_{j=1}^{N} \\mathbf{x}_{j} t_j - \\mathbf{x}_{j} \\mathbf{x}_{j}^{T} \\mathbf{{\\color{blue}{w}}} \\equiv 0\n\nJust as we did before, we can use matrix vector notation to write this out as\n\n\n\\frac{\\partial \\; log \\left( \\mathcal{L} \\right) }{\\partial \\mathbf{{\\color{blue}{w}}} } = \\frac{1}{\\sigma^2} \\left( \\mathbf{X}^{T} \\mathbf{t} - \\mathbf{X}^{T} \\mathbf{X} \\mathbf{{\\color{blue}{w}}}\\right) = 0\n\n\nSolving this expression leads to\n\n\\mathbf{X}^{T} \\mathbf{t} - \\mathbf{X}^{T} \\mathbf{X} \\mathbf{{\\color{blue}{w}}} = 0 \\Rightarrow \\hat{\\mathbf{{\\color{blue}{w}}}} = \\left( \\mathbf{X}^{T} \\mathbf{X} \\right)^{-1} \\mathbf{X}^{T} \\mathbf{t}.\n\n Thus, the maximum likelihood solution for \\mathbf{{\\color{blue}{w}}} is exactly the solution for the least squares problem!  Minimizing the squared loss is equivalent to the maximum likelihood solution if the noise is assumed Gaussian."
  },
  {
    "objectID": "slides/lecture-6/index.html#maximum-likelihood-5",
    "href": "slides/lecture-6/index.html#maximum-likelihood-5",
    "title": "Lecture 6",
    "section": "Maximum likelihood",
    "text": "Maximum likelihood\nMaximizing the logarithm of the likelihood\n\nWhat remains now is to compute the maximum likelihood estimate of the noise, \\sigma. Assuming that \\hat{\\mathbf{{\\color{blue}{w}}}} = \\mathbf{{\\color{blue}{w}}} we can write \n\\frac{\\partial \\; log \\left( \\mathcal{L} \\right) }{\\partial \\sigma }  = - \\frac{N}{\\sigma} + \\frac{1}{\\sigma^3} \\sum_{j=1}^{N} \\left( t_j - \\mathbf{x}^{T} \\hat{\\mathbf{{\\color{blue}{w}}} } \\right)^2 \\equiv 0.\n\nRearranging, this yields \\hat{\\sigma^2} = 1/N \\sum_{j=1}^{N} \\left( t_j - \\mathbf{x}^{T} \\hat{\\mathbf{{\\color{blue}{w}}} }\\right).\nThis expression states that the variance is the averaged squared error, which intuitively makes sense. Re-writing this using matrix notation, we have \n\\hat{\\sigma^2} = \\frac{1}{N} \\left( \\mathbf{t} - \\mathbf{X} \\hat{\\mathbf{{\\color{blue}{w}}} } \\right)^{T}  \\left( \\mathbf{t} - \\mathbf{X} \\hat{\\mathbf{{\\color{blue}{w}}} } \\right) = \\frac{1}{N} \\left(  \\mathbf{t}^{T}  \\mathbf{t} - 2  \\mathbf{t}^{T} \\mathbf{X} \\hat{\\mathbf{{\\color{blue}{w}}} } + \\hat{\\mathbf{{\\color{blue}{w}}} }^{T} \\mathbf{X}^{T} \\mathbf{X} \\hat{\\mathbf{{\\color{blue}{w}}} } \\right)\n\n\nPlugging in \\hat{\\mathbf{{\\color{blue}{w}}}} = \\left( \\mathbf{X}^{T} \\mathbf{X} \\right)^{-1} \\mathbf{X}^{T} \\mathbf{t}, we arrive at \n\\hat{\\sigma^2} = \\frac{1}{N} \\left(   \\mathbf{t}^{T}   \\mathbf{t} -  \\mathbf{t}^{T}  \\mathbf{X} \\left( \\mathbf{X}^{T} \\mathbf{X} \\right)^{-1} \\mathbf{X}^{T} \\mathbf{t}\\right)"
  },
  {
    "objectID": "slides/lecture-6/index.html#maximum-likelihood-6",
    "href": "slides/lecture-6/index.html#maximum-likelihood-6",
    "title": "Lecture 6",
    "section": "Maximum likelihood",
    "text": "Maximum likelihood\nVisualizing the Gaussian noise model\n\n\n\nPlotCode\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(font_scale=1.0)\nsns.set_style(\"white\")\nsns.set_style(\"ticks\")\npalette = sns.color_palette('deep')\n\ndf = pd.read_csv('notebooks/data/data100m.csv')\ndf.columns=['Year', 'Time']\nN = df.shape[0]\n\nmax_year, min_year = df['Year'].values.max() , df['Year'].values.min()\n\nx = (df['Year'].values.reshape(N,1) - min_year)/(max_year - min_year)\nt = df['Time'].values.reshape(N,1)\nX_func = lambda u : np.hstack([np.ones((u.shape[0],1)), u ])\nX = X_func(x)\nw_hat = np.linalg.inv(X.T @ X) @ X.T @ t\nloss_func = 1./N * (t - X @ w_hat).T @ (t - X @ w_hat)\n\nxgrid = np.linspace(0, 1, 100).reshape(100,1)\nXg = X_func(xgrid)\ntime_grid = Xg @ w_hat\n\n\nxi = xgrid*(max_year - min_year) + min_year\nxi = xi.flatten()\nsigma_hat_squared = float( (1. / N) * (t.T @ t - t.T @ X @ w_hat) )\nsigma_hat = np.sqrt(sigma_hat_squared)\nyi = xi* 0 + sigma_hat\n\nloss_func =  -N/2 * np.log(np.pi * 2)  - N * np.log(sigma_hat) - \\\n                   1./(2 * sigma_hat_squared) * np.sum((X @ w_hat - t)**2) \n\nfig = plt.figure(figsize=(6,4))\na, = plt.plot(df['Year'].values, df['Time'].values, 'o', color='crimson', label='Data')\nplt.plot(xi, time_grid, '-', color='r')\nc = plt.fill_between(xi, time_grid.flatten()-yi, time_grid.flatten()+yi, color='red', alpha=0.2, label='Model')\nplt.xlabel('Year')\nplt.ylabel('Time (seconds)')\nloss_title = r'Logarithm of loss function, $log \\left( \\mathcal{L} \\right)=$'+str(np.around(float(loss_func), 5))\nplt.title(loss_title)\nplt.legend([a,c], ['Data', 'Model'])\nplt.savefig('olympics_last.png', dpi=150, bbox_inches='tight')\nplt.close()\n\n\n\n\n\nThe graph on the left is the final model.\n\n\n\n\nAE8803 | Gaussian Processes for Machine Learning"
  },
  {
    "objectID": "slides/lecture-8/index.html#bayesian-model",
    "href": "slides/lecture-8/index.html#bayesian-model",
    "title": "Lecture 8",
    "section": "Bayesian model",
    "text": "Bayesian model\nDeriving the posterior\n\nRecall, from Lecture 8\n\n\np \\left( \\mathbf{w} | \\mathbf{t}, \\mathbf{X}, \\sigma^2 \\right) \\propto p \\left( \\mathbf{t} | \\mathbf{w}, \\mathbf{X}, \\sigma^2 \\right) p \\left( \\mathbf{w}   \\right)\n \n\\begin{aligned}\n= \\frac{1}{\\left( 2 \\pi \\right)^{N/2} | \\sigma^2 \\mathbf{I} |^{1/2} }  exp \\left( - \\frac{1}{2} \\left( \\mathbf{t} - \\mathbf{Xw} \\right)^{T} \\left( \\sigma^2 \\mathbf{I} \\right)^{-1} \\left( \\mathbf{t} - \\mathbf{Xw} \\right)  \\right)   \\\\\n\\times \\frac{1}{\\left( 2 \\pi \\right)^{N/2} | \\boldsymbol{\\Sigma}_{0} |^{1/2} } exp \\left( - \\frac{1}{2} \\left( \\mathbf{w} - \\boldsymbol{\\mu}_{0} \\right)^{T} \\boldsymbol{\\Sigma}_{0}^{-1} \\left( \\mathbf{w} - \\boldsymbol{\\mu}_{0} \\right)  \\right)\n\\end{aligned}\n \n\\propto exp \\left( - \\frac{1}{2} \\left( \\mathbf{t} - \\mathbf{Xw} \\right)^{T} \\left( \\sigma^2 \\mathbf{I} \\right)^{-1} \\left( \\mathbf{t} - \\mathbf{Xw} \\right)  \\right)  \\times exp \\left( - \\frac{1}{2} \\left( \\mathbf{w} - \\boldsymbol{\\mu}_{0} \\right)^{T} \\boldsymbol{\\Sigma}_{0}^{-1} \\left( \\mathbf{w} - \\boldsymbol{\\mu}_{0} \\right)  \\right)\n\n\nWe shall now continue this derivation."
  },
  {
    "objectID": "slides/lecture-8/index.html#bayesian-model-1",
    "href": "slides/lecture-8/index.html#bayesian-model-1",
    "title": "Lecture 8",
    "section": "Bayesian model",
    "text": "Bayesian model\nDeriving the posterior\n\np \\left( \\mathbf{w} | \\mathbf{t}, \\mathbf{X}, \\sigma^2 \\right) \\propto exp \\left\\{ - \\frac{1}{2} \\left( \\frac{1}{\\sigma^2} \\left( \\mathbf{t} - \\mathbf{Xw} \\right)^{T}   \\left( \\mathbf{t} - \\mathbf{Xw} \\right) + \\left(   \\mathbf{w} - \\boldsymbol{\\mu}_{0}  \\right)^{T} \\boldsymbol{\\Sigma\n}_{0}^{-1} \\left(   \\mathbf{w} - \\boldsymbol{\\mu}_{0}  \\right)  \\right) \\right\\}\n\nMultiplying the terms in the bracket out, and removing any terms that do not involve a \\mathbf{w} yields\n\n\\begin{aligned}\np \\left( \\mathbf{w} | \\mathbf{t}, \\mathbf{X}, \\sigma^2 \\right) & \\propto \\\\\n& exp \\left\\{ - \\frac{1}{2} \\left(  - \\frac{2}{\\sigma^2} \\mathbf{t}^{T} \\mathbf{Xw} + \\frac{1}{\\sigma^2} \\mathbf{w}^{T} \\mathbf{X}^{T}\\mathbf{X} \\mathbf{w} + \\mathbf{w}^{T} \\boldsymbol{\\Sigma}_{0}^{-1} \\mathbf{w} - 2 \\boldsymbol{\\mu}_{0}^{T} \\boldsymbol{\\Sigma}_{0}^{-1}\\mathbf{w}   \\right)   \\right\\}\n\\end{aligned}\n\\tag{1}\nThe trick is to recognize that since our posterior is Gaussian, it must have the form\n\n\\begin{aligned}\np \\left( \\mathbf{w} | \\mathbf{t}, \\mathbf{X}, \\sigma^2 \\right) = \\mathcal{N}\\left(\\boldsymbol{\\mu}_{\\mathbf{w}}, \\boldsymbol{\\Sigma}_{\\mathbf{w}} \\right)\n\\end{aligned}\n\\tag{2}"
  },
  {
    "objectID": "slides/lecture-8/index.html#bayesian-model-2",
    "href": "slides/lecture-8/index.html#bayesian-model-2",
    "title": "Lecture 8",
    "section": "Bayesian model",
    "text": "Bayesian model\nDeriving the posterior\nIf we expand Equation 2, we arrive at\n\n\\begin{aligned}\np \\left( \\mathbf{w} | \\mathbf{t}, \\mathbf{X}, \\sigma^2 \\right) & \\propto exp \\left\\{ - \\frac{1}{2} \\left( \\mathbf{w} - \\boldsymbol{\\mu}_{\\mathbf{w}} \\right)^{T} \\boldsymbol{\\Sigma}_{\\mathbf{w}}^{-1} \\left( \\mathbf{w} - \\boldsymbol{\\mu}_{\\mathbf{w}} \\right)  \\right\\} \\\\\n& \\propto exp \\left\\{ - \\frac{1}{2} \\left( \\mathbf{w}^{T} \\boldsymbol{\\Sigma}^{-1}_{\\mathbf{w}} \\mathbf{w} - 2 \\boldsymbol{\\mu}_{\\mathbf{w}}^{T} \\boldsymbol{\\Sigma}_{\\mathbf{w}}^{-1} \\mathbf{w} \\right)  \\right\\}\n\\end{aligned}\n\\tag{3}\nThe quadratic terms in \\mathbf{w} in Equation 3 must match those we had in Equation 1. Thus we can write \n\\begin{aligned}\n\\mathbf{w}^{T} \\boldsymbol{\\Sigma}_{\\mathbf{w}}^{-1} \\mathbf{w} & = \\frac{1}{\\sigma^2} \\mathbf{w}^{T} \\mathbf{X}^{T} \\mathbf{X} \\mathbf{w} + \\mathbf{w}^{T} \\boldsymbol{\\Sigma}_{0}^{-1} \\mathbf{w} \\\\\n& = \\mathbf{w}^{T} \\left( \\frac{1}{\\sigma^2} \\mathbf{X}^{T} \\mathbf{X} + \\boldsymbol{\\Sigma}_{0}^{-1} \\right) \\mathbf{w}\n\\end{aligned}\n\nAs for the expectation, we equate the linear terms in Equation 1 with those in Equation 3 \n\\begin{aligned}\n-2 \\boldsymbol{\\mu}_{\\mathbf{w}}^{T} \\boldsymbol{\\Sigma}_{\\mathbf{w}}^{-1}\\mathbf{w} = - \\frac{2}{\\sigma^2} \\mathbf{t}^{T} \\mathbf{Xw} - 2 \\boldsymbol{\\mu}_{0}^{T} \\boldsymbol{\\Sigma}_{0}^{-1} \\mathbf{w}\n\\end{aligned}"
  },
  {
    "objectID": "slides/lecture-8/index.html#bayesian-model-3",
    "href": "slides/lecture-8/index.html#bayesian-model-3",
    "title": "Lecture 8",
    "section": "Bayesian model",
    "text": "Bayesian model\nDeriving the posterior\nContinuing this expansion\n\n\\begin{aligned}\n\\boldsymbol{\\mu}_{\\mathbf{w}}^{T} \\boldsymbol{\\Sigma}_{\\mathbf{w}}^{-1} & = \\frac{1}{\\sigma^2} \\mathbf{t}^{T} \\mathbf{X} + \\boldsymbol{\\mu}_{0}^{T} \\boldsymbol{\\Sigma}_{0}^{-1} \\\\\n\\boldsymbol{\\mu}_{\\mathbf{w}}^{T} & = \\left( \\frac{1}{\\sigma^2} \\mathbf{t}^{T} \\mathbf{X} + \\boldsymbol{\\mu}_{0}^{T} \\boldsymbol{\\Sigma}_{0}^{-1} \\right) \\boldsymbol{\\Sigma}_{\\mathbf{w}} \\\\\n\\Rightarrow \\boldsymbol{\\mu}_{\\mathbf{w}} & = \\mathbf{\\Sigma}_{\\mathbf{w}} \\left( \\frac{1}{\\sigma^2} \\mathbf{X}^{T} \\mathbf{t} + \\boldsymbol{\\Sigma}^{-1}_{0} \\boldsymbol{\\mu}_{0} \\right)\n\\end{aligned}\n\nwhere the last statement is due to \\boldsymbol{\\Sigma}_{\\mathbf{w}} = \\boldsymbol{\\Sigma}_{\\mathbf{w}}^{T}, i.e., the covariance matrix is symmetric.\nTo summarize, we have now worked out our posterior\n\n\np \\left( \\mathbf{w} | \\mathbf{t}, \\mathbf{X}, \\sigma^2 \\right) = \\mathcal{N}\\left( \\boldsymbol{\\mu}_{\\mathbf{w}}, \\boldsymbol{\\Sigma}_{\\mathbf{w}} \\right)\n where \n\\boldsymbol{\\mu}_{\\mathbf{w}} = \\mathbf{\\Sigma}_{\\mathbf{w}} \\left( \\frac{1}{\\sigma^2} \\mathbf{X}^{T} \\mathbf{t} + \\boldsymbol{\\Sigma}^{-1}_{0} \\boldsymbol{\\mu}_{0} \\right) \\; \\; \\text{and} \\; \\; \\boldsymbol{\\Sigma}_{\\mathbf{w}} = \\left( \\frac{1}{\\sigma^2} \\mathbf{X}^{T} \\mathbf{X} + \\boldsymbol{\\Sigma}_{0}^{-1} \\right)^{-1}"
  },
  {
    "objectID": "slides/lecture-8/index.html#bayesian-model-4",
    "href": "slides/lecture-8/index.html#bayesian-model-4",
    "title": "Lecture 8",
    "section": "Bayesian model",
    "text": "Bayesian model\nMaximum a posteriori estimate\n\nIf we set the prior mean to be zero, i.e., \\boldsymbol{\\mu}_{0} = \\left[ 0, 0, \\ldots, 0 \\right]^{T}, the resulting value of \\boldsymbol{\\mu}_{\\mathbf{w}} looks very similar to the maximum likelihood solution from before.\nNote, that because the posterior p \\left( \\mathbf{w} | \\mathbf{t}, \\mathbf{X}, \\sigma^2 \\right) is Gaussian, the most likely value of \\mathbf{w} is the mean of the posterior, \\boldsymbol{\\mu}_{\\mathbf{w}}. .\nThis is known as the maximum a posteriori (MAP) estimate of \\mathbf{w} and can also be thought of as the maximum value of the joint density p \\left( \\mathbf{w}, \\mathbf{t} | \\mathbf{X} \\sigma^2, \\boldsymbol{\\mu}_{0}, \\boldsymbol{\\Sigma}_{0} \\right) (which is the likelihood \\times prior)."
  },
  {
    "objectID": "slides/lecture-8/index.html#bayesian-model-5",
    "href": "slides/lecture-8/index.html#bayesian-model-5",
    "title": "Lecture 8",
    "section": "Bayesian model",
    "text": "Bayesian model\nPredictive density\nJust as we did last time, it will be useful to make predictions. To do this, we once again use \\mathbf{X}_{new} \\in \\mathbb{R}^{S \\times 2} where S is the number of new x locations (in this case time) at which our model needs to be evaluated, i.e., \n\\mathbf{X}_{new}=\\left[\\begin{array}{cc}\n1 & x^{new}_{1}\\\\\n1 & x^{new}_{2}\\\\\n\\vdots & \\vdots\\\\\n1 & x^{new}_{S}\n\\end{array}\\right]\n\nWe are interested in the predictive density\n\np \\left( \\mathbf{t}_{new} | \\mathbf{X}_{new}, \\mathbf{X},  \\mathbf{t}, \\sigma^2 \\right)\n\nNotice, that this density is not conditioned on \\mathbf{w}. We are going to integrate out \\mathbf{w} by taking an expectation with respect to the posterior p \\left( \\mathbf{w} | \\mathbf{t}, \\mathbf{X}, \\sigma^2 \\right), i.e.,\n\n\\begin{aligned}\np \\left( \\mathbf{t}_{new} | \\mathbf{X}_{new}, \\mathbf{X},  \\mathbf{t}, \\sigma^2 \\right) & = \\mathbb{E}_{p \\left( \\mathbf{w} | \\mathbf{t}, \\mathbf{X}, \\sigma^2 \\right)} \\left[  p \\left( \\mathbf{t}_{new} | \\mathbf{w},  \\mathbf{X}_{new}, \\sigma^2 \\right)\\right] \\\\\n& = \\int  p \\left( \\mathbf{t}_{new} | \\mathbf{w}, \\mathbf{X}_{new}, \\sigma^2 \\right) p \\left( \\mathbf{w} | \\mathbf{t}, \\mathbf{X}, \\sigma^2 \\right) d\\mathbf{w}\n\\end{aligned}"
  },
  {
    "objectID": "slides/lecture-8/index.html#bayesian-model-6",
    "href": "slides/lecture-8/index.html#bayesian-model-6",
    "title": "Lecture 8",
    "section": "Bayesian model",
    "text": "Bayesian model\nPredictive density\n\nThis leads to the predictive density form given by \np \\left( \\mathbf{t}_{new} | \\mathbf{X}_{new}, \\mathbf{X},  \\mathbf{t}, \\sigma^2 \\right)  = \\mathcal{N} \\left( \\mathbf{X}_{new}  \\boldsymbol{\\mu}_{\\mathbf{w}} , \\sigma^2 \\mathbf{I} + \\mathbf{X}_{new}^{T} \\boldsymbol{\\Sigma}_{\\mathbf{w}} \\mathbf{X}_{new} \\right)\n\\tag{4}\nTry proving this yourself! You may find section 2.3.2 of Bishop useful\nThe inclusion of the \\sigma^2 \\mathbf{I} term is optional, i.e., if we wish to replicate the true signal (without noise) this can be negated. However, if we wish to model the realistic data generation process then the noise should be incorporated."
  },
  {
    "objectID": "slides/lecture-8/index.html#bayesian-model-7",
    "href": "slides/lecture-8/index.html#bayesian-model-7",
    "title": "Lecture 8",
    "section": "Bayesian model",
    "text": "Bayesian model\nVisualizing the prior\n\nPlotCode\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal\nimport seaborn as sns\nsns.set(font_scale=1.0)\nsns.set_style(\"white\")\nsns.set_style(\"ticks\")\npalette = sns.color_palette('deep')\nplt.style.use('dark_background')\ndf = pd.read_csv('notebook/data/data100m.csv')\ndf.columns=['Year', 'Time']\nN = df.shape[0]\n\n# Data & basis\nmax_year, min_year = df['Year'].values.max() , df['Year'].values.min()\nx = (df['Year'].values.reshape(N,1) - min_year)/(max_year - min_year)\nt = df['Time'].values.reshape(N,1)\nX_func = lambda u : np.hstack([np.ones((u.shape[0],1)), u ])\nX = X_func(x)\n\n# For prediction / plotting\nxgrid = np.linspace(0, 1, 100).reshape(100,1)\nXg = X_func(xgrid)\nxi = xgrid*(max_year - min_year) + min_year\nxi = xi.flatten()\nsigma_hat = 0.2\nsigma_w = 2.0\n\n# Prior\nmu_0 = np.array([[7],\n                 [0]])\nSigma_0 = np.array([[5.0, -0.8],\n                    [-0.8, 0.5]])\ninv_Sigma_0 = np.eye(X.shape[1]) * 1/(sigma_w**2)\nprior = multivariate_normal(mu_0.flatten(), Sigma_0)\n\n# Posterior\nSigma_w = np.linalg.inv(1./(sigma_hat**2) * (X.T @ X) + inv_Sigma_0)\nmu_w = Sigma_w @ (1./(sigma_hat**2) * (X.T @ t) + (inv_Sigma_0 @ mu_0) )\nposterior = multivariate_normal(mu_w.flatten(), Sigma_w)\n\n# Plotting support\nww1, ww2 = np.mgrid[2:12:.05, -2:1.5:.05]\npos = np.dstack((ww1, ww2))\n\nfig, ax = plt.subplots(2, figsize=(12,4))\nfig.patch.set_facecolor('#6C757D')\nax[0].set_fc('#6C757D')\nplt.subplot(121)\nplt.contourf(ww1, ww2, prior.pdf(pos), 30, cmap=plt.cm.Oranges)\nplt.title(r'Prior $\\mathcal{N}(\\mu_0, \\Sigma_0)$')\nplt.xlabel(r'$\\mathbf{w}_0$')\nplt.ylabel(r'$\\mathbf{w}_1$')\nfig.patch.set_facecolor('#6C757D')\n\nplt.subplot(122)\nplt.rcParams['axes.facecolor']='#6C757D'\nax[1].set_facecolor('#6C757D')\nrandom_samples = 200\nplt.plot(xi, Xg @ prior.rvs(random_samples).T, alpha=0.7)\nplt.xlabel('Year')\nplt.title('model using prior samples')\nplt.ylabel('Time (seconds)')\nplt.savefig('prior.png', dpi=150, bbox_inches='tight', facecolor=\"#6C757D\")\nplt.close()"
  },
  {
    "objectID": "slides/lecture-8/index.html#bayesian-model-8",
    "href": "slides/lecture-8/index.html#bayesian-model-8",
    "title": "Lecture 8",
    "section": "Bayesian model",
    "text": "Bayesian model\nVisualizing the posterior\n\nPlotCode\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal\nimport seaborn as sns\nsns.set(font_scale=1.0)\nsns.set_style(\"white\")\nsns.set_style(\"ticks\")\npalette = sns.color_palette('deep')\nplt.style.use('dark_background')\ndf = pd.read_csv('notebook/data/data100m.csv')\ndf.columns=['Year', 'Time']\nN = df.shape[0]\n\n# Data & basis\nmax_year, min_year = df['Year'].values.max() , df['Year'].values.min()\nx = (df['Year'].values.reshape(N,1) - min_year)/(max_year - min_year)\nt = df['Time'].values.reshape(N,1)\nX_func = lambda u : np.hstack([np.ones((u.shape[0],1)), u ])\nX = X_func(x)\n\n# For prediction / plotting\nxgrid = np.linspace(0, 1, 100).reshape(100,1)\nXg = X_func(xgrid)\nxi = xgrid*(max_year - min_year) + min_year\nxi = xi.flatten()\nsigma_hat = 0.2\nsigma_w = 2.0\n\n# Prior\nmu_0 = np.array([[7],\n                 [0]])\nSigma_0 = np.array([[5.0, -0.8],\n                    [-0.8, 0.5]])\ninv_Sigma_0 = np.eye(X.shape[1]) * 1/(sigma_w**2)\nprior = multivariate_normal(mu_0.flatten(), Sigma_0)\n\n# Posterior\nSigma_w = np.linalg.inv(1./(sigma_hat**2) * (X.T @ X) + inv_Sigma_0)\nmu_w = Sigma_w @ (1./(sigma_hat**2) * (X.T @ t) + (inv_Sigma_0 @ mu_0) )\nposterior = multivariate_normal(mu_w.flatten(), Sigma_w)\n\n# Plotting support\nww1, ww2 = np.mgrid[2:12:.05, -2:1.5:.05]\npos = np.dstack((ww1, ww2))\n\nfig, ax = plt.subplots(2, figsize=(12,4))\nfig.patch.set_facecolor('#6C757D')\nax[0].set_fc('#6C757D')\nplt.subplot(121)\nplt.contourf(ww1, ww2, posterior.pdf(pos), 30, cmap=plt.cm.Oranges)\nplt.title(r'Posterior $\\mathcal{N}(\\mu_w, \\Sigma_w)$')\nplt.xlabel(r'$\\mathbf{w}_0$')\nplt.ylabel(r'$\\mathbf{w}_1$')\nfig.patch.set_facecolor('#6C757D')\n\nplt.subplot(122)\nplt.rcParams['axes.facecolor']='#6C757D'\nax[1].set_facecolor('#6C757D')\nplt.plot(xi, Xg @ posterior.rvs(random_samples).T, zorder=-1, alpha=0.8)\na, = plt.plot(df['Year'].values, df['Time'].values, 'o', color='dodgerblue', \\\n              label='Data', markeredgecolor='k', lw=1, ms=10, zorder=1)\nplt.xlabel('Year')\nplt.title('model using posterior samples')\nplt.legend([a ], ['Data'], framealpha=0.2)\nplt.ylabel('Time (seconds)')\nplt.savefig('posterior.png', dpi=150, bbox_inches='tight', facecolor=\"#6C757D\")\nplt.show()\nplt.close()"
  },
  {
    "objectID": "slides/lecture-8/index.html#bayesian-model-9",
    "href": "slides/lecture-8/index.html#bayesian-model-9",
    "title": "Lecture 8",
    "section": "Bayesian model",
    "text": "Bayesian model\nConjugacy\n\nA likelihood-prior pair is said to be conjugate if they result in a posterior which is of the same form as the prior.\nThis enables us to compute the posterior density analytically (as we have done) without having to worry about the marginal likelihood (the denominator in Bayes’ rule).\nSome commonly used prior and likelihood distributions are given below.\n\n\n\n\nPrior\nLikelihood\n\n\n\n\nGaussian\nGaussian\n\n\nBeta\nBinomial\n\n\nGamma\nGaussian\n\n\nDirichlet\nMultinomial"
  },
  {
    "objectID": "slides/lecture-8/index.html#bayesian-model-10",
    "href": "slides/lecture-8/index.html#bayesian-model-10",
    "title": "Lecture 8",
    "section": "Bayesian model",
    "text": "Bayesian model\nMarginal likelihood re-visited\n\nThe marginal likelihood for a Bayesian model is given by \n\\text{marginal likelihood} = \\int \\text{likelihood} \\times \\text{prior} \\; d \\mathbf{w}\n\nThe marginal likelihood for our Gaussian prior and Gaussian likelihood model is given by \n\\begin{aligned}\np \\left( \\mathbf{t} | \\mathbf{X}, \\boldsymbol{\\mu}_{0}, \\boldsymbol{\\Sigma}_{0} \\right) & = \\int p \\left( \\mathbf{t} | \\mathbf{X}, \\mathbf{w}, \\sigma^2 \\right) p \\left( \\mathbf{w} | \\boldsymbol{\\mu}_{0} , \\boldsymbol{\\Sigma}_{0} \\right) d \\mathbf{w} \\\\\n& = \\mathcal{N} \\left( \\mathbf{X} \\boldsymbol{\\mu}_{0}, \\sigma^2 \\mathbf{I}_{N} + \\mathbf{X} \\boldsymbol{\\Sigma}_{0} \\mathbf{X}^{T} \\right)\n\\end{aligned}\n\nNote that it has the same form as Equation 4, and it is evaluated at \\mathbf{t}, i.e., the observed winning times."
  },
  {
    "objectID": "slides/lecture-8/index.html#bayesian-model-11",
    "href": "slides/lecture-8/index.html#bayesian-model-11",
    "title": "Lecture 8",
    "section": "Bayesian model",
    "text": "Bayesian model\nMarginal likelihood re-visited\n\nPlotCode\n\n\n\n\n\nThe plot on the right shows the logarithm of the marginal likelihood.\nNote that where the contours peak corresponds to values of \\mathbf{w} that better explain the data \\mathbf{t}, given its uncertainty \\sigma^2, and the model as encoded into \\mathbf{X}.\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal\nimport seaborn as sns\nsns.set(font_scale=1.0)\nsns.set_style(\"white\")\nsns.set_style(\"ticks\")\npalette = sns.color_palette('deep')\nplt.style.use('dark_background')\ndf = pd.read_csv('notebook/data/data100m.csv')\ndf.columns=['Year', 'Time']\nN = df.shape[0]\n\n# Data & basis\nmax_year, min_year = df['Year'].values.max() , df['Year'].values.min()\nx = (df['Year'].values.reshape(N,1) - min_year)/(max_year - min_year)\nt = df['Time'].values.reshape(N,1)\nX_func = lambda u : np.hstack([np.ones((u.shape[0],1)), u ])\nX = X_func(x)\n\n# For prediction / plotting\nxgrid = np.linspace(0, 1, 100).reshape(100,1)\nXg = X_func(xgrid)\nxi = xgrid*(max_year - min_year) + min_year\nxi = xi.flatten()\nsigma_hat = 0.2\nsigma_w = 2.0\n\n# Prior\nmu_0 = np.array([[7],\n                 [0]])\nSigma_0 = np.array([[5.0, -0.8],\n                    [-0.8, 0.5]])\ninv_Sigma_0 = np.eye(X.shape[1]) * 1/(sigma_w**2)\nprior = multivariate_normal(mu_0.flatten(), Sigma_0)\n\n# Posterior\nSigma_w = np.linalg.inv(1./(sigma_hat**2) * (X.T @ X) + inv_Sigma_0)\nmu_w = Sigma_w @ (1./(sigma_hat**2) * (X.T @ t) + (inv_Sigma_0 @ mu_0) )\nposterior = multivariate_normal(mu_w.flatten(), Sigma_w)\n\n# Plotting support\nww1, ww2 = np.mgrid[2:12:.05, -2:1.5:.05]\npos = np.dstack((ww1, ww2))\n\nmarginal_like_val = np.ones((ww1.shape[0], ww1.shape[1]))\n\nfor i in range(0, ww1.shape[0]):\n    for j in range(0, ww1.shape[1]):\n        mu_0 = np.array([ ww1[i,j], ww2[i,j] ]).reshape(2, 1)\n        marginal_likelihood_dist = multivariate_normal( (X @ mu_0).flatten(), \\\n                                               sigma_hat**2 * np.eye(t.shape[0]) + X @ X.T)\n        marginal_like_val[i, j] = marginal_likelihood_dist.pdf(t.flatten())\n\nfig, ax = plt.subplots(figsize=(6,4))\nfig.patch.set_facecolor('#6C757D')\nax.set_fc('#6C757D')\nc = plt.contourf(ww1, ww2, np.log10(marginal_like_val), 60, cmap=plt.cm.Oranges)\nplt.colorbar(c)\nplt.title(r'Log_{10} of marginal likelihood evaluated at $t$')\nplt.xlabel(r'$\\mathbf{w}_0$')\nplt.ylabel(r'$\\mathbf{w}_1$')\nplt.savefig('marginal_log.png', dpi=150, bbox_inches='tight', facecolor=\"#6C757D\")\nplt.close()"
  },
  {
    "objectID": "slides/lecture-8/index.html#bayesian-model-12",
    "href": "slides/lecture-8/index.html#bayesian-model-12",
    "title": "Lecture 8",
    "section": "Bayesian model",
    "text": "Bayesian model\nA function space perspective\n\nSampling \\mathbf{t} conditioned on \\mathbf{w} vs. sampling \\mathbf{t} directly from the marginal likelihood are statistically equivalent.\nRemoving \\mathbf{w} from our model (effectively making it non-parametric) opens the door to thinking of priors in the space of functions, and not just in the space of model parameters.\nThis is arguably one of the main ideas in Gaussian processes, that I will introduce next time we meet.\n\n\n\nAE8803 | Gaussian Processes for Machine Learning"
  },
  {
    "objectID": "sample_problems/lecture_2.html",
    "href": "sample_problems/lecture_2.html",
    "title": "L2 examples",
    "section": "",
    "text": "Commercial airline pilots need to pass four out of five separate tests for certification. Assume that the tests are equally difficult, and that the performance on separate tests are independent.\n\nIf the probability of failing each separate test is \\(p=0.2\\), then what is the probability of failing certification?\nTo improve safety, more stringent regulations require that pilots pass all five tests. To be able to meet the demand, the individual tests are made easier. What should the new individual failure rate be if the overall certification probability is to remain unchanged?\n\n\n\nSolution\n\n\nGiven that each test is independent, the combined probabilities follow a Binomial distribution. A pilot will fail certification if they fail two or more tests, and they will pass if they fail zero or one of the individual tests. Thus, the probability of passing certification is\n\n\\[\n\\large\np_{pass} = \\left(\\begin{array}{c}\n5\\\\\n0\n\\end{array}\\right) p^{0} \\left( 1 - p \\right)^{5} + \\left(\\begin{array}{c}\n5\\\\\n1\n\\end{array}\\right)p^{1} \\left( 1 - p \\right)^{4}\n\\]\nFrom the code snippet below this is roughly 0.737. Thus the combined failure rate is \\(1 - 0.7373 = 0.2627\\).\n\nUnder the new certification protocol, as there is no possibility of failing a test, we have\n\n\\[\n\\large\n\\left(1 - p_{fail, new} \\right)^{5} = 1 - 0.2627 \\Rightarrow p_{fail, new} = 0.06\n\\]\n\n\n\nCode\nfrom scipy.special import comb\nimport numpy as np\n\n# part a.\np = 0.2\np_pass = comb(5, 0) * p**0 * (1 - p)**5 + comb(5, 1) * p**1 * (1 - p)**4\np_fail = 1 - p_pass\nprint(p_fail)\n\n# part b.\np_fail_new = 1 - (1 - p_fail)**(1/5)\nprint(p_fail_new)\n\n\n0.26271999999999984\n0.059136781980261066"
  },
  {
    "objectID": "sample_problems/lecture_2.html#problem-1",
    "href": "sample_problems/lecture_2.html#problem-1",
    "title": "L2 examples",
    "section": "",
    "text": "Commercial airline pilots need to pass four out of five separate tests for certification. Assume that the tests are equally difficult, and that the performance on separate tests are independent.\n\nIf the probability of failing each separate test is \\(p=0.2\\), then what is the probability of failing certification?\nTo improve safety, more stringent regulations require that pilots pass all five tests. To be able to meet the demand, the individual tests are made easier. What should the new individual failure rate be if the overall certification probability is to remain unchanged?\n\n\n\nSolution\n\n\nGiven that each test is independent, the combined probabilities follow a Binomial distribution. A pilot will fail certification if they fail two or more tests, and they will pass if they fail zero or one of the individual tests. Thus, the probability of passing certification is\n\n\\[\n\\large\np_{pass} = \\left(\\begin{array}{c}\n5\\\\\n0\n\\end{array}\\right) p^{0} \\left( 1 - p \\right)^{5} + \\left(\\begin{array}{c}\n5\\\\\n1\n\\end{array}\\right)p^{1} \\left( 1 - p \\right)^{4}\n\\]\nFrom the code snippet below this is roughly 0.737. Thus the combined failure rate is \\(1 - 0.7373 = 0.2627\\).\n\nUnder the new certification protocol, as there is no possibility of failing a test, we have\n\n\\[\n\\large\n\\left(1 - p_{fail, new} \\right)^{5} = 1 - 0.2627 \\Rightarrow p_{fail, new} = 0.06\n\\]\n\n\n\nCode\nfrom scipy.special import comb\nimport numpy as np\n\n# part a.\np = 0.2\np_pass = comb(5, 0) * p**0 * (1 - p)**5 + comb(5, 1) * p**1 * (1 - p)**4\np_fail = 1 - p_pass\nprint(p_fail)\n\n# part b.\np_fail_new = 1 - (1 - p_fail)**(1/5)\nprint(p_fail_new)\n\n\n0.26271999999999984\n0.059136781980261066"
  },
  {
    "objectID": "sample_problems/lecture_4.html",
    "href": "sample_problems/lecture_4.html",
    "title": "L4 examples",
    "section": "",
    "text": "Code\nimport numpy as np \nfrom scipy.stats import bernoulli, binom, expon\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.special import comb\nsns.set(font_scale=1.0)\nsns.set_style(\"white\")\nsns.set_style(\"ticks\")\npalette = sns.color_palette('deep')"
  },
  {
    "objectID": "sample_problems/lecture_4.html#problem-1",
    "href": "sample_problems/lecture_4.html#problem-1",
    "title": "L4 examples",
    "section": "Problem 1",
    "text": "Problem 1\nFind the probability density function of \\(Y = h \\left( X \\right) = X^2\\), for any \\(y &gt; 0\\), where \\(X\\) is a continuous random variable with a known probability density function.\n\n\nSolution\n\nFor \\(y &gt; 0\\) we have\n\\[\nF_Y \\left( y \\right) = p \\left( Y \\leq y \\right) = p \\left( X^2 \\leq y \\right) = p \\left( - \\sqrt{y} \\leq X \\leq \\sqrt{y} \\right)\n\\]\n\\[\n\\Rightarrow F_Y \\left( y \\right) = F_{X} \\left( \\sqrt{y} \\right) - F_{X} \\left( - \\sqrt{y} \\right)\n\\]\nThus, by differentiating and applying the chain rule we have\n\\[\nf_{Y} \\left( y \\right) = \\frac{1}{2\\sqrt{y}} f_{X} \\left( \\sqrt{y} \\right) + \\frac{1}{2 \\sqrt{y}} f_{X} \\left( - \\sqrt{y} \\right), \\; \\; \\; \\; y &gt; 0\n\\]"
  },
  {
    "objectID": "sample_problems/lecture_4.html#problem-2",
    "href": "sample_problems/lecture_4.html#problem-2",
    "title": "L4 examples",
    "section": "Problem 2",
    "text": "Problem 2\nFind the probability density function of \\(Y = exp \\left( X^2 \\right)\\) if \\(X\\) is a non-negative random variable.\n\n\nSolution\n\nNote that \\(F_Y \\left( y \\right) = 0\\) for \\(y &lt; 1\\). For \\(y \\geq 1\\), we have\n\\[\nF_{Y} \\left( y \\right) = p \\left(exp \\left(X^2 \\right) \\leq y \\right) = p \\left(X^2 \\leq log \\left( y \\right) \\right)\n\\]\n\\[\n\\Rightarrow F_{Y} = p \\left( X \\leq \\sqrt{log \\left( y \\right) } \\right).\n\\]\nBy differentiating and using the chain rule, we obtain\n\\[\nf_{Y} \\left( y \\right) = f_{X} \\left( \\sqrt{log \\left( y \\right) } \\right) \\frac{1}{2y \\sqrt{log \\left( y \\right) } }, \\; \\; \\; y &gt; 1.\n\\]\n\n\n\nCode\nlam = 1.\nX = expon.rvs(size=15000, scale=1/lam)\nY = np.sin(X)\n\nfig = plt.figure(figsize=(10,3))\nplt.subplot(121)\nplt.hist(X,40, density=True, color='crimson')\nplt.ylabel(r'$f_{X}(x)$')\nplt.xlabel('x')\nplt.subplot(122)\nplt.hist(Y,40, density=True, color='dodgerblue')\nplt.ylabel(r'$f_{Y}(y)$')\nplt.xlabel('y')\nplt.show()"
  },
  {
    "objectID": "sample_problems/lecture_4.html#problem-3",
    "href": "sample_problems/lecture_4.html#problem-3",
    "title": "L4 examples",
    "section": "Problem 3",
    "text": "Problem 3\nFollowing the bit of code above, let \\(X\\) be an exponential random variable with parameter \\(\\lambda\\), i.e., \\(f_{X} \\left( x \\right) = \\lambda exp \\left( -\\lambda x \\right)\\) and \\(F_{X} \\left( x \\right) = 1 - exp \\left( -\\lambda x \\right)\\). Let \\(Y= sin\\left( X \\right)\\). Determine \\(F_Y\\left( y \\right)\\) and \\(f_{Y} \\left( y \\right)\\).\n\n\nSolution\n\nFrom the event \\(\\left\\{ Y \\leq y \\right\\}\\), we can conclude that for \\(x = sin^{-1} \\left( y \\right)\\) we have\n\\[\nF_{Y} \\left( y \\right) = p \\left( Y \\leq y \\right)\n\\]\n\\[\nF_{Y} \\left( y \\right) = p \\left( X \\leq x \\right) + \\sum_{k=1}^{\\infty} \\left[ F_{X} \\left( 2 k \\pi + x \\right) - F_{X} \\left( \\left(2k - 1 \\right) \\pi - x\\right) \\right]\n\\]\n\\[\nF_{Y} \\left( y \\right) = p \\left( X \\leq x \\right) + \\sum_{k=1}^{\\infty} \\left[  1 - exp\\left( -\\lambda x - 2 \\lambda k \\pi \\right) - 1 + exp \\left( \\lambda x + \\lambda \\pi - 2 \\lambda k \\pi \\right) \\right]\n\\]\n\\[\n= p \\left( X \\leq x \\right) + \\left[ exp\\left( \\lambda x \\right) exp \\left( \\lambda \\pi \\right) - exp \\left( - \\lambda x \\right) \\right] \\sum_{k=1}^{\\infty}  exp \\left( - 2 \\lambda k \\pi \\right)\n\\]\n\\[\n= p \\left( X \\leq sin^{-1} \\left( y \\right) \\right) + \\left[ exp \\left( \\lambda sin^{-1} \\left( y \\right) + \\lambda \\pi \\right) - exp \\left( -\\lambda sin^{-1} \\left( y \\right) \\right) \\right] \\frac{exp \\left( -2 \\lambda \\pi \\right) }{1 - exp \\left( -2 \\lambda \\pi \\right)}\n\\]\nThis expansion uses the sum of a geometric sequence formula. The first term above is zero for negative \\(y \\in [-1, 0)\\) and\n\\[\np \\left( X \\leq sin^{-1} \\left( y \\right) \\right) = F_{X} \\left( sin^{-1} \\left( y \\right) \\right) = 1 - exp(- \\lambda sin^{-1}\\left( y \\right) )\n\\]\nfor non-negative \\(y \\in [0, 1]\\). Since \\(F_{X} \\left(0\\right) = 0\\), the cumulative probability \\(F_Y\\left( y \\right)\\) will remain continuous at \\(y=0\\). However, its derivative is discontinuous and we will be unable to derive an expression for \\(f_{Y} \\left( 0 \\right)\\). Hence, for negative \\(y \\in [-1, 0)\\) we have\n\\[\nf_{Y} \\left( y \\right) = \\frac{d}{dx} F_{X} \\left( x \\right) \\frac{dx}{dy} = \\frac{\\lambda}{\\sqrt{1 - y^2}} \\frac{exp \\left( \\lambda \\left(sin^{-1} \\left( y \\right) + \\pi \\right) \\right) + exp \\left( -\\lambda sin^{-1} \\left( y \\right) \\right) }{exp \\left( 2 \\lambda \\pi -1 \\right) }.\n\\]\nFor positive \\(y \\in (0, 1]\\), we have\n\\[\nf_{Y} \\left( y \\right) = \\frac{d}{dx} F_{X} \\left( x \\right) \\frac{dx}{dy} = \\frac{\\lambda}{\\sqrt{1 - y^2}} \\left[ \\frac{exp \\left( \\lambda \\left(sin^{-1} \\left( y \\right) + \\pi \\right) \\right) + exp \\left( -\\lambda sin^{-1} \\left( y \\right) \\right) }{exp \\left( 2 \\lambda \\pi -1 \\right) } + exp \\left( -\\lambda sin^{-1} \\left( y \\right) \\right) \\right].\n\\]\n\n\n\nCode\nx = np.linspace(-2.5, 2.5, 500)\ny = np.sin(x)\n\ndef f_y(y):\n    f_y = np.zeros((y.shape[0]))\n    for i in range(0, f_y.shape[0]):\n        if y[i] &gt; 0:\n            f_y[i] = lam/np.sqrt(1 - y[i]**2) * (np.exp(-lam * np.arcsin(y[i])) \\\n                        + (np.exp(lam * np.arcsin(y[i]) + lam * np.pi) + \\\n                          np.exp(-lam * np.arcsin(y[i])))/(np.exp(2 * lam * np.pi) - 1))\n        else:\n            f_y[i] = lam/np.sqrt(1 - y[i]**2) * ((np.exp(lam * np.arcsin(y[i]) + lam * np.pi) + \\\n                          np.exp(-lam * np.arcsin(y[i])))/(np.exp(2 * lam * np.pi) - 1))\n    return f_y\n\nfig = plt.figure(figsize=(6,4))\nplt.plot(y, f_y(y), color='navy', lw=3)\nplt.hist(Y,40, density=True, color='dodgerblue')\nplt.ylabel(r'$f_{Y}(y)$')\nplt.xlabel('y')\nplt.ylim([0, 2.7])\nplt.show()"
  },
  {
    "objectID": "sample_problems/lecture_4.html#problem-4",
    "href": "sample_problems/lecture_4.html#problem-4",
    "title": "L4 examples",
    "section": "Problem 4",
    "text": "Problem 4\nLet \\(X\\) and \\(Y\\) be independent and uniform between \\(0\\) and \\(1\\). Compute \\(X + Y\\). To set the stage for the problem, consider the code and plot below.\n\n\nCode\nX = np.random.rand(9000)\nY = np.random.rand(9000)\nS = X + Y \n\nfig = plt.figure(figsize=(6,3))\nplt.hist(X+Y,40, density=True, color='orangered')\nplt.ylabel(r'$f_{S}(s)$')\nplt.xlabel('s')\nplt.show()\n\n\n\n\n\nIt appears we have a triangular distribution. In what follows we shall aim to derive this analytically.\n\n\nSolution\n\nFrom the Lecture notes, we have:\n\\[\nf_{S} \\left( s \\right) = \\int_{0}^{1} f_{X} \\left( x \\right) f_{Y} \\left( s - x \\right) dx = \\int_{0}^{1} f_{Y} \\left( s - x \\right) dx\n\\]\n\\[\n\\Rightarrow f_{S} \\left( s \\right) = \\begin{cases}\n\\begin{array}{c}\n\\int_{0}^{s}1dx=s\\\\\n\\int_{s-1}^{1}1dx=2-s\n\\end{array} & \\begin{array}{c}\n\\textrm{for} \\; \\; s \\in [0, 1] \\\\\n\\textrm{for} \\; \\; s \\in [1, 2]\n\\end{array}\\end{cases}\n\\]"
  },
  {
    "objectID": "useful_codes/gaussians.html",
    "href": "useful_codes/gaussians.html",
    "title": "Gaussian marginals and conditionals",
    "section": "",
    "text": "This notebook covers a few basic ideas with regards to Gaussian marginals and conditionals.\n\n\nConsider a random vector \\(\\mathbf{u} = \\left[u_1, u_2, u_3, u_4 \\right]^{T}\\) following a multivariate Gaussian distribution a mean vector \\(\\boldsymbol{\\mu}\\) and a covariance matrix \\(\\mathbf{K}\\). These are given by\n\\[\n\\boldsymbol{\\mu}=\\left[\\begin{array}{c}\n\\mu_{1}\\\\\n\\mu_{2}\\\\\n\\mu_{3}\\\\\n\\mu_{4}\n\\end{array}\\right], \\; \\; \\; \\; \\mathbf{K}=\\left[\\begin{array}{cccc}\nk_{11} & k_{12} & k_{13} & k_{14}\\\\\nk_{21} & k_{22} & k_{23} & k_{24}\\\\\nk_{31} & k_{32} & k_{33} & k_{34}\\\\\nk_{41} & k_{42} & k_{43} & k_{44}\n\\end{array}\\right]\n\\]\nThe marginal distribution of any subset of these four variables is obtained by integrating over the remaining ones. This can be trivially done by simply extracting the relevant elements of \\(\\boldsymbol{\\mu}\\) and \\(\\mathbf{K}\\). For instance the joint distribution given by \\(p\\left(u_2, u_3 \\right)\\) is a Gaussian\n\\[\np\\left( u_2, u_3 \\right) = \\mathcal{N} \\left( \\boldsymbol{\\mu}_{\\left(2,3\\right)}, \\boldsymbol{\\Sigma}_{\\left( 2,3 \\right)} \\right)\n\\]\nwhere\n\\[\n\\boldsymbol{\\mu}_{\\left(2,3\\right)}=\\left[\\begin{array}{c}\n\\mu_{2}\\\\\n\\mu_{3}\n\\end{array}\\right], \\; \\; \\; \\; \\boldsymbol{\\Sigma}_{\\left(2,3\\right)} = \\left[\\begin{array}{cc}\nk_{22} & k_{23} \\\\\nk_{32} & k_{33}\n\\end{array}\\right].\n\\]\nSimilarly, the marginal distribution of \\(p\\left( u_1 \\right)\\) is a Gaussian with a mean of \\(\\mu_1\\) and a variance of \\(k_{11}\\).\n\n\n\nConsider a random vector \\(\\mathbf{u}\\), composed of two sets \\(\\mathbf{u}_{1}\\) and \\(\\mathbf{u}_{2}\\). Assume, as before, that \\(\\mathbf{u} = p \\left( \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma} \\right)\\), where\n\\[\n\\boldsymbol{\\mu} =\\left[\\begin{array}{c}\n\\boldsymbol{\\mu}_{1}\\\\\n\\boldsymbol{\\mu}_{2}\n\\end{array}\\right], \\; \\; \\; \\; \\boldsymbol{\\Sigma} = \\left[\\begin{array}{cc}\n\\boldsymbol{\\Sigma}_{11} & \\boldsymbol{\\Sigma}_{12} \\\\\n\\boldsymbol{\\Sigma}_{21} & \\boldsymbol{\\Sigma}_{22}\n\\end{array}\\right].\n\\]\nIf we observe one of these sets, say \\(\\mathbf{u}_{1}\\), then the conditional density of the other set \\(\\mathbf{u}_{2}\\) is a Gaussian of the form\n\\[\np \\left(\\mathbf{u}_{2} | \\mathbf{u}_{1} \\right) = \\mathcal{N} \\left( \\mathbf{d}, \\mathbf{D} \\right)\n\\]\nwhere\n\\[\n\\mathbf{d} = \\boldsymbol{\\mu}_{2} + \\boldsymbol{\\Sigma}_{12}^{T} \\boldsymbol{\\Sigma}_{11}^{-1} \\left( \\mathbf{u}_1 - \\boldsymbol{\\mu}_{1} \\right)\n\\]\nand\n\\[\n\\mathbf{D} = \\boldsymbol{\\Sigma}_{22} - \\boldsymbol{\\Sigma}_{12}^{T} \\boldsymbol{\\Sigma}_{11}^{-1} \\boldsymbol{\\Sigma}_{12}\n\\]\n\n\nThe proof for the result above begins with the definition of the conditional distribution, i.e.,\n\\[\np \\left( \\mathbf{u}_2 | \\mathbf{u}_1 \\right) = \\frac{p\\left( \\mathbf{u}_2 , \\mathbf{u}_1 \\right) }{ p \\left( \\mathbf{u}_1 \\right) }\n\\]\nPlugging in the definition of a multivariate normal distribution, we have:\n\\[\np\\left( \\mathbf{u}_2 , \\mathbf{u}_1 \\right) = p \\left( \\mathbf{u} \\right) = \\frac{1}{\\sqrt{\\left( 2 \\pi \\right)^{N} |\\boldsymbol{\\Sigma} | }} exp \\left[ -\\frac{1}{2} \\left(\\mathbf{u} - \\boldsymbol{\\mu} \\right)^{T} \\boldsymbol{\\Sigma}^{-1}  \\left(\\mathbf{u} - \\boldsymbol{\\mu} \\right) \\right]\n\\]\nand\n\\[\np\\left( \\mathbf{u}_1 \\right) =  \\frac{1}{\\sqrt{\\left( 2 \\pi \\right)^{N_1} |\\boldsymbol{\\Sigma}_{11} | }} exp \\left[ -\\frac{1}{2} \\left(\\mathbf{u}_1 - \\boldsymbol{\\mu}_1 \\right)^{T} \\boldsymbol{\\Sigma}_{11}^{-1}  \\left(\\mathbf{u}_{1} - \\boldsymbol{\\mu}_{1} \\right) \\right]\n\\]\nThus,\n\\[\np \\left( \\mathbf{u}_2 | \\mathbf{u}_1 \\right) =  \\frac{\\sqrt{|\\boldsymbol{\\Sigma}_{11} |}}{\\sqrt{\\left( 2 \\pi \\right)^{N - N_1} |\\boldsymbol{\\Sigma} | }} exp \\left[ -\\frac{1}{2} \\left(\\mathbf{u} - \\boldsymbol{\\mu} \\right)^{T} \\boldsymbol{\\Sigma}^{-1}  \\left(\\mathbf{u} - \\boldsymbol{\\mu} \\right) + \\frac{1}{2} \\left(\\mathbf{u}_1 - \\boldsymbol{\\mu}_1 \\right)^{T} \\boldsymbol{\\Sigma}_{11}^{-1}  \\left(\\mathbf{u}_{1} - \\boldsymbol{\\mu}_{1} \\right) \\right]\n\\]\nExpanding the above, we arrive at\n\\[\np \\left( \\mathbf{u}_2 | \\mathbf{u}_1 \\right) = \\frac{\\sqrt{|\\boldsymbol{\\Sigma}_{11} |}}{\\sqrt{\\left( 2 \\pi \\right)^{N - N_1} |\\boldsymbol{\\Sigma} | }}exp \\left[ -\\frac{1}{2} \\left(\\mathbf{u}_1 - \\boldsymbol{\\mu}_1 \\right)^{T} \\boldsymbol{\\Gamma}_{11}  \\left(\\mathbf{u}_1 - \\boldsymbol{\\mu}_1 \\right) +\n\\left(\\mathbf{u}_1 - \\boldsymbol{\\mu}_1 \\right)^{T} \\boldsymbol{\\Gamma}_{12}  \\left(\\mathbf{u}_2 - \\boldsymbol{\\mu}_2 \\right) -\n\\frac{1}{2}\\left(\\mathbf{u}_2 - \\boldsymbol{\\mu}_2 \\right)^{T} \\boldsymbol{\\Gamma}_{22}  \\left(\\mathbf{u}_2 - \\boldsymbol{\\mu}_2 \\right)\n-\\frac{1}{2}  \\left(\\mathbf{u}_1 - \\boldsymbol{\\mu}_1 \\right)^{T} \\boldsymbol{\\Sigma}_{11}^{-1}  \\left(\\mathbf{u}_1 - \\boldsymbol{\\mu}_1 \\right)\n\\right]\n\\]\nwhere\n\\[\n\\boldsymbol{\\Sigma}^{-1} = \\left[\\begin{array}{cc}\n\\boldsymbol{\\Gamma}_{11} & \\boldsymbol{\\Gamma}_{12} \\\\\n\\boldsymbol{\\Gamma}_{21} & \\boldsymbol{\\Gamma}_{22}\n\\end{array}\\right].\n\\]\nUsing the block matrix inverse and Woodbury matrix identity, we have\n\\[\n\\boldsymbol{\\Gamma}_{11} = \\boldsymbol{\\Sigma}_{11}^{-1} \\boldsymbol{\\Sigma}_{12}\\left( \\boldsymbol{\\Sigma}_{22} - \\boldsymbol{\\Sigma}_{21} \\boldsymbol{\\Sigma}_{11}^{-1} \\boldsymbol{\\Sigma}_{12} \\right)^{-1} \\boldsymbol{\\Sigma}_{21} \\boldsymbol{\\Sigma}_{11}^{-1}\n\\]\n\\[\n\\boldsymbol{\\Gamma}_{12} = - \\boldsymbol{\\Sigma}_{11}^{-1} \\boldsymbol{\\Sigma}_{12} \\left( \\boldsymbol{\\Sigma}_{22} - \\boldsymbol{\\Sigma}_{21} \\boldsymbol{\\Sigma}_{11}^{-1} \\boldsymbol{\\Sigma}_{12} \\right)^{-1}\n\\]\n\\[\n\\boldsymbol{\\Gamma}_{22} = \\left( \\boldsymbol{\\Sigma}_{22} - \\boldsymbol{\\Sigma}_{21} \\boldsymbol{\\Sigma}_{11}^{-1} \\boldsymbol{\\Sigma}_{12} \\right)^{-1}\n\\]\nThe prior expansion then yields\n\\[\np \\left( \\mathbf{u}_2 | \\mathbf{u}_1 \\right) = \\frac{\\sqrt{|\\boldsymbol{\\Sigma}_{11} |}}{\\sqrt{\\left( 2 \\pi \\right)^{N - N_1} |\\boldsymbol{\\Sigma} | }}exp \\left[ -\\frac{1}{2} \\left[\n\\left(\\mathbf{u}_1 - \\boldsymbol{\\mu}_1 \\right)^{T} \\left( \\boldsymbol{\\Gamma}_{11} -  \\boldsymbol{\\Sigma}_{11}^{-1} \\right) \\left(\\mathbf{u}_1 - \\boldsymbol{\\mu}_1 \\right) -  \n2\\left(\\mathbf{u}_1 - \\boldsymbol{\\mu}_1 \\right)^{T} \\boldsymbol{\\Gamma}_{12}  \\left(\\mathbf{u}_2 - \\boldsymbol{\\mu}_2 \\right) +\n\\left(\\mathbf{u}_2 - \\boldsymbol{\\mu}_2 \\right)^{T} \\boldsymbol{\\Gamma}_{22}  \\left(\\mathbf{u}_2 - \\boldsymbol{\\mu}_2 \\right) \\right] \\right]\n\\]\nExpanding this out and grouping similar terms leads to\n\\[\np \\left( \\mathbf{u}_2 | \\mathbf{u}_1 \\right) = \\frac{\\sqrt{|\\boldsymbol{\\Sigma}_{11} |}}{\\sqrt{\\left( 2 \\pi \\right)^{N - N_1} |\\boldsymbol{\\Sigma} | }}exp \\left[ -\\frac{1}{2} \\left[ \\left(\\mathbf{u}_2 - \\boldsymbol{\\mu}_2 - \\boldsymbol{\\Sigma}_{21}\\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{u}_1 - \\boldsymbol{\\mu}_{1} \\right) \\right)^{T} \\left( \\boldsymbol{\\Sigma}_{22} - \\boldsymbol{\\Sigma}_{21} \\boldsymbol{\\Sigma}_{11}^{-1} \\boldsymbol{\\Sigma}_{12} \\right)^{-1} \\left(u_2 - \\mu_2 - \\boldsymbol{\\Sigma}_{21}\\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{u}_1 - \\boldsymbol{\\mu}_{1} \\right) \\right) \\right] \\right]\n\\]\nFrom this it is readily apparent that the mean and covariance of the new density is given by\n\\[\n\\mathbb{E}\\left[ p \\left( \\mathbf{u}_2 | \\mathbf{u}_1 \\right) \\right ] =    \\boldsymbol{\\mu}_2 + \\boldsymbol{\\Sigma}_{21}\\boldsymbol{\\Sigma}_{11}^{-1}\\left(\\mathbf{u}_1 - \\boldsymbol{\\mu}_{1} \\right)\n\\]\n\\[\nCov \\left[ p \\left( \\mathbf{u}_2 | \\mathbf{u}_1 \\right) \\right ] =  \\boldsymbol{\\Sigma}_{22} - \\boldsymbol{\\Sigma}_{21} \\boldsymbol{\\Sigma}_{11}^{-1} \\boldsymbol{\\Sigma}_{12}\n\\]\nThe scaling constant in the equation prior can be expanded using the determinant of a block matrix identity."
  },
  {
    "objectID": "useful_codes/gaussians.html#overview",
    "href": "useful_codes/gaussians.html#overview",
    "title": "Gaussian marginals and conditionals",
    "section": "",
    "text": "This notebook covers a few basic ideas with regards to Gaussian marginals and conditionals.\n\n\nConsider a random vector \\(\\mathbf{u} = \\left[u_1, u_2, u_3, u_4 \\right]^{T}\\) following a multivariate Gaussian distribution a mean vector \\(\\boldsymbol{\\mu}\\) and a covariance matrix \\(\\mathbf{K}\\). These are given by\n\\[\n\\boldsymbol{\\mu}=\\left[\\begin{array}{c}\n\\mu_{1}\\\\\n\\mu_{2}\\\\\n\\mu_{3}\\\\\n\\mu_{4}\n\\end{array}\\right], \\; \\; \\; \\; \\mathbf{K}=\\left[\\begin{array}{cccc}\nk_{11} & k_{12} & k_{13} & k_{14}\\\\\nk_{21} & k_{22} & k_{23} & k_{24}\\\\\nk_{31} & k_{32} & k_{33} & k_{34}\\\\\nk_{41} & k_{42} & k_{43} & k_{44}\n\\end{array}\\right]\n\\]\nThe marginal distribution of any subset of these four variables is obtained by integrating over the remaining ones. This can be trivially done by simply extracting the relevant elements of \\(\\boldsymbol{\\mu}\\) and \\(\\mathbf{K}\\). For instance the joint distribution given by \\(p\\left(u_2, u_3 \\right)\\) is a Gaussian\n\\[\np\\left( u_2, u_3 \\right) = \\mathcal{N} \\left( \\boldsymbol{\\mu}_{\\left(2,3\\right)}, \\boldsymbol{\\Sigma}_{\\left( 2,3 \\right)} \\right)\n\\]\nwhere\n\\[\n\\boldsymbol{\\mu}_{\\left(2,3\\right)}=\\left[\\begin{array}{c}\n\\mu_{2}\\\\\n\\mu_{3}\n\\end{array}\\right], \\; \\; \\; \\; \\boldsymbol{\\Sigma}_{\\left(2,3\\right)} = \\left[\\begin{array}{cc}\nk_{22} & k_{23} \\\\\nk_{32} & k_{33}\n\\end{array}\\right].\n\\]\nSimilarly, the marginal distribution of \\(p\\left( u_1 \\right)\\) is a Gaussian with a mean of \\(\\mu_1\\) and a variance of \\(k_{11}\\).\n\n\n\nConsider a random vector \\(\\mathbf{u}\\), composed of two sets \\(\\mathbf{u}_{1}\\) and \\(\\mathbf{u}_{2}\\). Assume, as before, that \\(\\mathbf{u} = p \\left( \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma} \\right)\\), where\n\\[\n\\boldsymbol{\\mu} =\\left[\\begin{array}{c}\n\\boldsymbol{\\mu}_{1}\\\\\n\\boldsymbol{\\mu}_{2}\n\\end{array}\\right], \\; \\; \\; \\; \\boldsymbol{\\Sigma} = \\left[\\begin{array}{cc}\n\\boldsymbol{\\Sigma}_{11} & \\boldsymbol{\\Sigma}_{12} \\\\\n\\boldsymbol{\\Sigma}_{21} & \\boldsymbol{\\Sigma}_{22}\n\\end{array}\\right].\n\\]\nIf we observe one of these sets, say \\(\\mathbf{u}_{1}\\), then the conditional density of the other set \\(\\mathbf{u}_{2}\\) is a Gaussian of the form\n\\[\np \\left(\\mathbf{u}_{2} | \\mathbf{u}_{1} \\right) = \\mathcal{N} \\left( \\mathbf{d}, \\mathbf{D} \\right)\n\\]\nwhere\n\\[\n\\mathbf{d} = \\boldsymbol{\\mu}_{2} + \\boldsymbol{\\Sigma}_{12}^{T} \\boldsymbol{\\Sigma}_{11}^{-1} \\left( \\mathbf{u}_1 - \\boldsymbol{\\mu}_{1} \\right)\n\\]\nand\n\\[\n\\mathbf{D} = \\boldsymbol{\\Sigma}_{22} - \\boldsymbol{\\Sigma}_{12}^{T} \\boldsymbol{\\Sigma}_{11}^{-1} \\boldsymbol{\\Sigma}_{12}\n\\]\n\n\nThe proof for the result above begins with the definition of the conditional distribution, i.e.,\n\\[\np \\left( \\mathbf{u}_2 | \\mathbf{u}_1 \\right) = \\frac{p\\left( \\mathbf{u}_2 , \\mathbf{u}_1 \\right) }{ p \\left( \\mathbf{u}_1 \\right) }\n\\]\nPlugging in the definition of a multivariate normal distribution, we have:\n\\[\np\\left( \\mathbf{u}_2 , \\mathbf{u}_1 \\right) = p \\left( \\mathbf{u} \\right) = \\frac{1}{\\sqrt{\\left( 2 \\pi \\right)^{N} |\\boldsymbol{\\Sigma} | }} exp \\left[ -\\frac{1}{2} \\left(\\mathbf{u} - \\boldsymbol{\\mu} \\right)^{T} \\boldsymbol{\\Sigma}^{-1}  \\left(\\mathbf{u} - \\boldsymbol{\\mu} \\right) \\right]\n\\]\nand\n\\[\np\\left( \\mathbf{u}_1 \\right) =  \\frac{1}{\\sqrt{\\left( 2 \\pi \\right)^{N_1} |\\boldsymbol{\\Sigma}_{11} | }} exp \\left[ -\\frac{1}{2} \\left(\\mathbf{u}_1 - \\boldsymbol{\\mu}_1 \\right)^{T} \\boldsymbol{\\Sigma}_{11}^{-1}  \\left(\\mathbf{u}_{1} - \\boldsymbol{\\mu}_{1} \\right) \\right]\n\\]\nThus,\n\\[\np \\left( \\mathbf{u}_2 | \\mathbf{u}_1 \\right) =  \\frac{\\sqrt{|\\boldsymbol{\\Sigma}_{11} |}}{\\sqrt{\\left( 2 \\pi \\right)^{N - N_1} |\\boldsymbol{\\Sigma} | }} exp \\left[ -\\frac{1}{2} \\left(\\mathbf{u} - \\boldsymbol{\\mu} \\right)^{T} \\boldsymbol{\\Sigma}^{-1}  \\left(\\mathbf{u} - \\boldsymbol{\\mu} \\right) + \\frac{1}{2} \\left(\\mathbf{u}_1 - \\boldsymbol{\\mu}_1 \\right)^{T} \\boldsymbol{\\Sigma}_{11}^{-1}  \\left(\\mathbf{u}_{1} - \\boldsymbol{\\mu}_{1} \\right) \\right]\n\\]\nExpanding the above, we arrive at\n\\[\np \\left( \\mathbf{u}_2 | \\mathbf{u}_1 \\right) = \\frac{\\sqrt{|\\boldsymbol{\\Sigma}_{11} |}}{\\sqrt{\\left( 2 \\pi \\right)^{N - N_1} |\\boldsymbol{\\Sigma} | }}exp \\left[ -\\frac{1}{2} \\left(\\mathbf{u}_1 - \\boldsymbol{\\mu}_1 \\right)^{T} \\boldsymbol{\\Gamma}_{11}  \\left(\\mathbf{u}_1 - \\boldsymbol{\\mu}_1 \\right) +\n\\left(\\mathbf{u}_1 - \\boldsymbol{\\mu}_1 \\right)^{T} \\boldsymbol{\\Gamma}_{12}  \\left(\\mathbf{u}_2 - \\boldsymbol{\\mu}_2 \\right) -\n\\frac{1}{2}\\left(\\mathbf{u}_2 - \\boldsymbol{\\mu}_2 \\right)^{T} \\boldsymbol{\\Gamma}_{22}  \\left(\\mathbf{u}_2 - \\boldsymbol{\\mu}_2 \\right)\n-\\frac{1}{2}  \\left(\\mathbf{u}_1 - \\boldsymbol{\\mu}_1 \\right)^{T} \\boldsymbol{\\Sigma}_{11}^{-1}  \\left(\\mathbf{u}_1 - \\boldsymbol{\\mu}_1 \\right)\n\\right]\n\\]\nwhere\n\\[\n\\boldsymbol{\\Sigma}^{-1} = \\left[\\begin{array}{cc}\n\\boldsymbol{\\Gamma}_{11} & \\boldsymbol{\\Gamma}_{12} \\\\\n\\boldsymbol{\\Gamma}_{21} & \\boldsymbol{\\Gamma}_{22}\n\\end{array}\\right].\n\\]\nUsing the block matrix inverse and Woodbury matrix identity, we have\n\\[\n\\boldsymbol{\\Gamma}_{11} = \\boldsymbol{\\Sigma}_{11}^{-1} \\boldsymbol{\\Sigma}_{12}\\left( \\boldsymbol{\\Sigma}_{22} - \\boldsymbol{\\Sigma}_{21} \\boldsymbol{\\Sigma}_{11}^{-1} \\boldsymbol{\\Sigma}_{12} \\right)^{-1} \\boldsymbol{\\Sigma}_{21} \\boldsymbol{\\Sigma}_{11}^{-1}\n\\]\n\\[\n\\boldsymbol{\\Gamma}_{12} = - \\boldsymbol{\\Sigma}_{11}^{-1} \\boldsymbol{\\Sigma}_{12} \\left( \\boldsymbol{\\Sigma}_{22} - \\boldsymbol{\\Sigma}_{21} \\boldsymbol{\\Sigma}_{11}^{-1} \\boldsymbol{\\Sigma}_{12} \\right)^{-1}\n\\]\n\\[\n\\boldsymbol{\\Gamma}_{22} = \\left( \\boldsymbol{\\Sigma}_{22} - \\boldsymbol{\\Sigma}_{21} \\boldsymbol{\\Sigma}_{11}^{-1} \\boldsymbol{\\Sigma}_{12} \\right)^{-1}\n\\]\nThe prior expansion then yields\n\\[\np \\left( \\mathbf{u}_2 | \\mathbf{u}_1 \\right) = \\frac{\\sqrt{|\\boldsymbol{\\Sigma}_{11} |}}{\\sqrt{\\left( 2 \\pi \\right)^{N - N_1} |\\boldsymbol{\\Sigma} | }}exp \\left[ -\\frac{1}{2} \\left[\n\\left(\\mathbf{u}_1 - \\boldsymbol{\\mu}_1 \\right)^{T} \\left( \\boldsymbol{\\Gamma}_{11} -  \\boldsymbol{\\Sigma}_{11}^{-1} \\right) \\left(\\mathbf{u}_1 - \\boldsymbol{\\mu}_1 \\right) -  \n2\\left(\\mathbf{u}_1 - \\boldsymbol{\\mu}_1 \\right)^{T} \\boldsymbol{\\Gamma}_{12}  \\left(\\mathbf{u}_2 - \\boldsymbol{\\mu}_2 \\right) +\n\\left(\\mathbf{u}_2 - \\boldsymbol{\\mu}_2 \\right)^{T} \\boldsymbol{\\Gamma}_{22}  \\left(\\mathbf{u}_2 - \\boldsymbol{\\mu}_2 \\right) \\right] \\right]\n\\]\nExpanding this out and grouping similar terms leads to\n\\[\np \\left( \\mathbf{u}_2 | \\mathbf{u}_1 \\right) = \\frac{\\sqrt{|\\boldsymbol{\\Sigma}_{11} |}}{\\sqrt{\\left( 2 \\pi \\right)^{N - N_1} |\\boldsymbol{\\Sigma} | }}exp \\left[ -\\frac{1}{2} \\left[ \\left(\\mathbf{u}_2 - \\boldsymbol{\\mu}_2 - \\boldsymbol{\\Sigma}_{21}\\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{u}_1 - \\boldsymbol{\\mu}_{1} \\right) \\right)^{T} \\left( \\boldsymbol{\\Sigma}_{22} - \\boldsymbol{\\Sigma}_{21} \\boldsymbol{\\Sigma}_{11}^{-1} \\boldsymbol{\\Sigma}_{12} \\right)^{-1} \\left(u_2 - \\mu_2 - \\boldsymbol{\\Sigma}_{21}\\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{u}_1 - \\boldsymbol{\\mu}_{1} \\right) \\right) \\right] \\right]\n\\]\nFrom this it is readily apparent that the mean and covariance of the new density is given by\n\\[\n\\mathbb{E}\\left[ p \\left( \\mathbf{u}_2 | \\mathbf{u}_1 \\right) \\right ] =    \\boldsymbol{\\mu}_2 + \\boldsymbol{\\Sigma}_{21}\\boldsymbol{\\Sigma}_{11}^{-1}\\left(\\mathbf{u}_1 - \\boldsymbol{\\mu}_{1} \\right)\n\\]\n\\[\nCov \\left[ p \\left( \\mathbf{u}_2 | \\mathbf{u}_1 \\right) \\right ] =  \\boldsymbol{\\Sigma}_{22} - \\boldsymbol{\\Sigma}_{21} \\boldsymbol{\\Sigma}_{11}^{-1} \\boldsymbol{\\Sigma}_{12}\n\\]\nThe scaling constant in the equation prior can be expanded using the determinant of a block matrix identity."
  },
  {
    "objectID": "useful_codes/eigen.html",
    "href": "useful_codes/eigen.html",
    "title": "Eigenfunction analysis of kernels",
    "section": "",
    "text": "This attempts to describe kernels. The hope is after going through this, the reader appreciates just how powerful kernels are, and the central role they play in Gaussian process models.\n\n\nCode\n### Data \nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom IPython.display import display, HTML\n\n\n\n\nNow, we shall be interested in mapping from a kernel to a feature map. This leads us to Mercer’s theorem, which states that: A symmetric function \\(k \\left( \\mathbf{x}, \\mathbf{x}' \\right)\\) can be expressed as the inner product\n\\[\nk \\left( \\mathbf{x}, \\mathbf{x}' \\right) = \\phi^{T} \\left( \\mathbf{x} \\right) \\phi \\left( \\mathbf{x}'\\right) = \\left\\langle \\phi \\left( \\mathbf{x} \\right), \\phi\\left( \\mathbf{x}' \\right) \\right\\rangle\n\\]\nfor some feature map \\(\\phi\\) if and only if \\(k \\left( \\mathbf{x}, \\mathbf{x}' \\right)\\) is positive semidefinite, i.e.,\n\\[\n\\int k \\left( \\mathbf{x}, \\mathbf{x}' \\right) g \\left(  \\mathbf{x} \\right)  g \\left(  \\mathbf{x}' \\right) d \\mathbf{x} d \\mathbf{x}' \\geq 0\n\\]\nfor all real \\(g\\).\nOne possible set of features corresponds to eigenfunctions. A function \\(\\nu\\left( \\mathbf{x} \\right)\\) that satisfies the integral equation\n\\[\n\\int k \\left( \\mathbf{x}, \\mathbf{x}' \\right) \\nu \\left( \\mathbf{x} \\right) d  \\mathbf{x}  = \\lambda  \\nu \\left( \\mathbf{x} \\right)\n\\]\nis termed an eigenfunction of the kernel \\(k\\). In the expression above, \\(\\lambda\\) is the corresponding eigenvalue. While the integral above is taken with respect to \\(\\mathbf{x}\\), more formally, it can be taken with respect to either a density \\(\\rho \\left( \\mathbf{x} \\right)\\), or the Lebesgue measure over a compact subset of \\(\\mathbb{R}^{D}\\), which reduces to \\(d \\mathbf{x}\\). The eigenfunctions form an orthogonal basis and thus\n\\[\n\\int \\nu_{i} \\left( \\mathbf{x} \\right) \\nu_{j} \\left( \\mathbf{x} \\right) d \\mathbf{x} = \\delta_{ij}\n\\]\nwhere \\(\\delta_{ij}\\) is the Kronecker delta. When \\(i=j\\), its value is \\(1\\); zero otherwise. Thus, one can define a kernel using its eigenfunctions\n\\[\nk \\left(  \\mathbf{x}, \\mathbf{x}' \\right) = \\sum_{i=1}^{\\infty} \\lambda_i \\nu \\left( \\mathbf{x} \\right) \\nu \\left( \\mathbf{x}' \\right).\n\\]"
  },
  {
    "objectID": "useful_codes/eigen.html#overview",
    "href": "useful_codes/eigen.html#overview",
    "title": "Eigenfunction analysis of kernels",
    "section": "",
    "text": "This attempts to describe kernels. The hope is after going through this, the reader appreciates just how powerful kernels are, and the central role they play in Gaussian process models.\n\n\nCode\n### Data \nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom IPython.display import display, HTML\n\n\n\n\nNow, we shall be interested in mapping from a kernel to a feature map. This leads us to Mercer’s theorem, which states that: A symmetric function \\(k \\left( \\mathbf{x}, \\mathbf{x}' \\right)\\) can be expressed as the inner product\n\\[\nk \\left( \\mathbf{x}, \\mathbf{x}' \\right) = \\phi^{T} \\left( \\mathbf{x} \\right) \\phi \\left( \\mathbf{x}'\\right) = \\left\\langle \\phi \\left( \\mathbf{x} \\right), \\phi\\left( \\mathbf{x}' \\right) \\right\\rangle\n\\]\nfor some feature map \\(\\phi\\) if and only if \\(k \\left( \\mathbf{x}, \\mathbf{x}' \\right)\\) is positive semidefinite, i.e.,\n\\[\n\\int k \\left( \\mathbf{x}, \\mathbf{x}' \\right) g \\left(  \\mathbf{x} \\right)  g \\left(  \\mathbf{x}' \\right) d \\mathbf{x} d \\mathbf{x}' \\geq 0\n\\]\nfor all real \\(g\\).\nOne possible set of features corresponds to eigenfunctions. A function \\(\\nu\\left( \\mathbf{x} \\right)\\) that satisfies the integral equation\n\\[\n\\int k \\left( \\mathbf{x}, \\mathbf{x}' \\right) \\nu \\left( \\mathbf{x} \\right) d  \\mathbf{x}  = \\lambda  \\nu \\left( \\mathbf{x} \\right)\n\\]\nis termed an eigenfunction of the kernel \\(k\\). In the expression above, \\(\\lambda\\) is the corresponding eigenvalue. While the integral above is taken with respect to \\(\\mathbf{x}\\), more formally, it can be taken with respect to either a density \\(\\rho \\left( \\mathbf{x} \\right)\\), or the Lebesgue measure over a compact subset of \\(\\mathbb{R}^{D}\\), which reduces to \\(d \\mathbf{x}\\). The eigenfunctions form an orthogonal basis and thus\n\\[\n\\int \\nu_{i} \\left( \\mathbf{x} \\right) \\nu_{j} \\left( \\mathbf{x} \\right) d \\mathbf{x} = \\delta_{ij}\n\\]\nwhere \\(\\delta_{ij}\\) is the Kronecker delta. When \\(i=j\\), its value is \\(1\\); zero otherwise. Thus, one can define a kernel using its eigenfunctions\n\\[\nk \\left(  \\mathbf{x}, \\mathbf{x}' \\right) = \\sum_{i=1}^{\\infty} \\lambda_i \\nu \\left( \\mathbf{x} \\right) \\nu \\left( \\mathbf{x}' \\right).\n\\]"
  },
  {
    "objectID": "useful_codes/eigen.html#numerical-solution",
    "href": "useful_codes/eigen.html#numerical-solution",
    "title": "Eigenfunction analysis of kernels",
    "section": "Numerical solution",
    "text": "Numerical solution\nIf the covariance matrix is already available, one write its eigendecomposition\n\\[\n\\mathbf{K} = \\mathbf{V} \\boldsymbol{\\Lambda} \\mathbf{V}^{T}\n\\]\nwhere \\(\\mathbf{V}\\) is a matrix of formed by the eigenvectors of \\(\\mathbf{K}\\) and \\(\\boldsymbol{\\Lambda}\\) is a diagonal matrix of its eigenvalues, i.e.,\n\\[\n\\mathbf{V} = \\left[\\begin{array}{cccc}\n| & | &  & |\\\\\n\\mathbf{v}_{1} & \\mathbf{v}_{2} & \\ldots & \\mathbf{v}_{N}\\\\\n| & | &  & |\n\\end{array}\\right], \\; \\; \\; \\; \\textrm{and} \\; \\; \\; \\; \\boldsymbol{\\Lambda}=\\left[\\begin{array}{cccc}\n\\lambda_{1}\\\\\n& \\lambda_{2}\\\\\n&  & \\ddots\\\\\n&  &  & \\lambda_{N}\n\\end{array}\\right],\n\\]\nwhere \\(\\lambda_1 \\geq \\lambda_2 \\geq \\ldots \\lambda_{N} \\geq 0\\). This expansion permits one to express each element of \\(\\mathbf{K}\\) as\n\\[\n\\mathbf{K} = \\sum_{i=1}^{N} \\left( \\sqrt{\\lambda_{i}} \\mathbf{v}_{i} \\right) \\left(  \\sqrt{\\lambda_{i}} \\mathbf{v}_{i}\\right)^{T}.\n\\]\nBeyond numerical solutions, for many kernels there exists analytical solutions for the eigenvalues and eigenvectors. For further details please see page 97 in RW. For now, we simply consider numerical solutions as shown below.\n\n\nCode\nN = 30\nx = np.linspace(-2, 2, N).reshape(N,1)\nR = (np.tile(x, [1, N]) - np.tile(x.T, [N, 1]))**2\nl = 0.5\nK = np.exp(-0.5 * R * 1/l**2)\n\nfig = plt.figure()\nd = plt.imshow(K)\nplt.colorbar(d)\nplt.title('Squared exponential')\nplt.show()\n\n\n\n\n\n\n\nCode\nLambda, V = np.linalg.eigh(K)\nidx = Lambda.argsort()[::-1]\nlambdas = Lambda[idx]\nV = V[:, idx]\n\n\n\n\nCode\nfig = plt.figure()\nplt.semilogy(lambdas, 'o-')\nplt.ylabel('Eigenvalues of covariance matrix (log)')\nplt.xlabel('Number of data points')\nplt.show()\n\n\n\n\n\n\n\nCode\nT = 5 # truncated basis\nK_approx = np.zeros((N, N))\nfor i in range(0, T):\n    feature = (np.sqrt(lambdas[i]) * V[:,i]).reshape(N,1)\n    K_approx += feature @ feature.T\n\n\n\n\nCode\nfig = plt.figure()\nplt.subplot(121)\nd = plt.imshow(K_approx, vmin=0, vmax=1)\nplt.colorbar(d, shrink=0.3)\nplt.title('Truncated approximation (5 terms)')\n\nplt.subplot(122)\ne = plt.imshow(K, vmin=0, vmax=1)\nplt.colorbar(e, shrink=0.3)\nplt.title('Squared exponential')\nplt.show()"
  },
  {
    "objectID": "useful_codes/gp101.html",
    "href": "useful_codes/gp101.html",
    "title": "GP 101",
    "section": "",
    "text": "This notebook provides a quick start guide to building a Gaussian process model using only numpy, scipy, pandas, and matplotlib.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal\nfrom scipy.linalg import cholesky, solve_triangular\nimport seaborn as sns\n\n\n\n\nIn this tutorial, we will use the Olympic gold dataset that we have used quite a few times in Lecture. First, we shall use pandas to retrieve the data.\n\n\nCode\ndf = pd.read_csv('data/data100m.csv')\ndf.columns=['Year', 'Time']\nN = df.shape[0]\n\n\nThe three code blocks below define the kernel, a utility function for tiling, and the posterior calculation. To clarify, this is the predictive posterior distribution evaluated at some test locations, \\(\\mathbf{X}_{\\ast}\\). The expression for both the predictive posterior mean and covariance are given by:\n\\[\n\\begin{aligned}\n\\mathbb{E} \\left[ \\mathbf{y}_{\\ast} | \\mathbf{X}_{\\ast} \\right] & = \\mathbf{K}\\left( \\mathbf{X}_{\\ast}, \\mathbf{X} \\right) \\left[\\mathbf{K}\\left( \\mathbf{X}, \\mathbf{X} \\right) + \\sigma_{n}^2 \\mathbf{I} \\right]^{-1} \\mathbf{y} \\\\\nCovar\\left[ \\mathbf{y}_{\\ast} | \\mathbf{X}_{\\ast} \\right] & = \\mathbf{K}\\left( \\mathbf{X}_{\\ast}, \\mathbf{X}_{\\ast} \\right) - \\mathbf{K}\\left( \\mathbf{X}_{\\ast}, \\mathbf{X} \\right) \\left[\\mathbf{K}\\left( \\mathbf{X}, \\mathbf{X} \\right) + \\sigma_{n}^2 \\mathbf{I} \\right]^{-1} \\mathbf{K}\\left( \\mathbf{X}, \\mathbf{X}{\\ast} \\right)\n\\end{aligned}\n\\]\n\n\nCode\ndef kernel(xa, xb, amp, ll):\n    Xa, Xb = get_tiled(xa, xb)\n    return amp**2 * np.exp(-0.5 * 1./ll**2 * (Xa - Xb)**2 )\n\ndef get_tiled(xa, xb):\n    m, n = len(xa), len(xb)\n    xa, xb = xa.reshape(m,1) , xb.reshape(n,1)\n    Xa = np.tile(xa, (1, n))\n    Xb = np.tile(xb.T, (m, 1))\n    return Xa, Xb\n\ndef get_posterior(amp, ll, x, x_data, y_data, noise):\n    u = y_data.shape[0]\n    mu_y = np.mean(y_data)\n    y = (y_data - mu_y).reshape(u,1)\n    Sigma = noise * np.eye(u)\n    \n    Kxx = kernel(x_data, x_data, amp, ll)\n    Kxpx = kernel(x, x_data, amp, ll)\n    Kxpxp = kernel(x, x, amp, ll)\n    \n    # Inverse\n    jitter = np.eye(u) * 1e-12\n    L = cholesky(Kxx + Sigma + jitter)\n    S1 = solve_triangular(L.T, y, lower=True)\n    S2 = solve_triangular(L.T, Kxpx.T, lower=True).T\n    \n    mu = S2 @ S1  + mu_y\n    cov = Kxpxp - S2 @ S2.T\n    return mu, cov\n\n\n\n\nCode\nXt = np.linspace(1890, 2022, 200) # test data locations (years)\n\n# Hyperparameters (note these are not optimized!)\nlength_scale = 7.0\namplitude = 0.8\n\n\nnoise_variance = 0.1\nmu, cov = get_posterior(amplitude, length_scale, Xt, df['Year'].values, df['Time'].values, noise_variance)\n\n\n\n\nCode\nXt = Xt.flatten()\nmu = mu.flatten() \nstd = np.sqrt(np.diag(cov)).flatten()\n\nfig = plt.figure(figsize=(8, 5))\nplt.plot(Xt, mu, '-', label=r'$\\mu$', color='navy')\nplt.fill_between(Xt, mu+std, mu-std, color='blue', alpha=0.2, label=r'$\\sigma$')\nplt.plot(df['Year'].values, df['Time'].values, 'go', label='Data', ms=8)\nplt.xlabel('Years')\nplt.ylabel('Winning times')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "useful_codes/gp101.html#overview",
    "href": "useful_codes/gp101.html#overview",
    "title": "GP 101",
    "section": "",
    "text": "This notebook provides a quick start guide to building a Gaussian process model using only numpy, scipy, pandas, and matplotlib.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal\nfrom scipy.linalg import cholesky, solve_triangular\nimport seaborn as sns\n\n\n\n\nIn this tutorial, we will use the Olympic gold dataset that we have used quite a few times in Lecture. First, we shall use pandas to retrieve the data.\n\n\nCode\ndf = pd.read_csv('data/data100m.csv')\ndf.columns=['Year', 'Time']\nN = df.shape[0]\n\n\nThe three code blocks below define the kernel, a utility function for tiling, and the posterior calculation. To clarify, this is the predictive posterior distribution evaluated at some test locations, \\(\\mathbf{X}_{\\ast}\\). The expression for both the predictive posterior mean and covariance are given by:\n\\[\n\\begin{aligned}\n\\mathbb{E} \\left[ \\mathbf{y}_{\\ast} | \\mathbf{X}_{\\ast} \\right] & = \\mathbf{K}\\left( \\mathbf{X}_{\\ast}, \\mathbf{X} \\right) \\left[\\mathbf{K}\\left( \\mathbf{X}, \\mathbf{X} \\right) + \\sigma_{n}^2 \\mathbf{I} \\right]^{-1} \\mathbf{y} \\\\\nCovar\\left[ \\mathbf{y}_{\\ast} | \\mathbf{X}_{\\ast} \\right] & = \\mathbf{K}\\left( \\mathbf{X}_{\\ast}, \\mathbf{X}_{\\ast} \\right) - \\mathbf{K}\\left( \\mathbf{X}_{\\ast}, \\mathbf{X} \\right) \\left[\\mathbf{K}\\left( \\mathbf{X}, \\mathbf{X} \\right) + \\sigma_{n}^2 \\mathbf{I} \\right]^{-1} \\mathbf{K}\\left( \\mathbf{X}, \\mathbf{X}{\\ast} \\right)\n\\end{aligned}\n\\]\n\n\nCode\ndef kernel(xa, xb, amp, ll):\n    Xa, Xb = get_tiled(xa, xb)\n    return amp**2 * np.exp(-0.5 * 1./ll**2 * (Xa - Xb)**2 )\n\ndef get_tiled(xa, xb):\n    m, n = len(xa), len(xb)\n    xa, xb = xa.reshape(m,1) , xb.reshape(n,1)\n    Xa = np.tile(xa, (1, n))\n    Xb = np.tile(xb.T, (m, 1))\n    return Xa, Xb\n\ndef get_posterior(amp, ll, x, x_data, y_data, noise):\n    u = y_data.shape[0]\n    mu_y = np.mean(y_data)\n    y = (y_data - mu_y).reshape(u,1)\n    Sigma = noise * np.eye(u)\n    \n    Kxx = kernel(x_data, x_data, amp, ll)\n    Kxpx = kernel(x, x_data, amp, ll)\n    Kxpxp = kernel(x, x, amp, ll)\n    \n    # Inverse\n    jitter = np.eye(u) * 1e-12\n    L = cholesky(Kxx + Sigma + jitter)\n    S1 = solve_triangular(L.T, y, lower=True)\n    S2 = solve_triangular(L.T, Kxpx.T, lower=True).T\n    \n    mu = S2 @ S1  + mu_y\n    cov = Kxpxp - S2 @ S2.T\n    return mu, cov\n\n\n\n\nCode\nXt = np.linspace(1890, 2022, 200) # test data locations (years)\n\n# Hyperparameters (note these are not optimized!)\nlength_scale = 7.0\namplitude = 0.8\n\n\nnoise_variance = 0.1\nmu, cov = get_posterior(amplitude, length_scale, Xt, df['Year'].values, df['Time'].values, noise_variance)\n\n\n\n\nCode\nXt = Xt.flatten()\nmu = mu.flatten() \nstd = np.sqrt(np.diag(cov)).flatten()\n\nfig = plt.figure(figsize=(8, 5))\nplt.plot(Xt, mu, '-', label=r'$\\mu$', color='navy')\nplt.fill_between(Xt, mu+std, mu-std, color='blue', alpha=0.2, label=r'$\\sigma$')\nplt.plot(df['Year'].values, df['Time'].values, 'go', label='Data', ms=8)\nplt.xlabel('Years')\nplt.ylabel('Winning times')\nplt.legend()\nplt.show()"
  }
]