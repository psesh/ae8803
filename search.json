[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Overview",
    "section": "",
    "text": "This graduate-level course offers a practical approach to probabilistic learning with Gaussian processes (GPs). GPs represent a powerful set of methods for modeling and predicting a wide variety of spatio-temporal phenomena. Today, they are used for problems that span both regression and classification, with theoretical foundations in Bayesian inference, reproducing kernel Hilbert spaces, eigenvalue problems, and numerical integration. Rather than focus solely on these theoretical foundations, this course balances theory with practical probabilistic programming, using a variety of python-based packages. Moreover, practical engineering problems will also be discussed, that see GP models that cut across other areas of machine learning including transfer learning, convolutional networks, and normalizing flows."
  },
  {
    "objectID": "index.html#lectures",
    "href": "index.html#lectures",
    "title": "Overview",
    "section": "Lectures",
    "text": "Lectures\nBelow you will find a list of the lectures that form the backbone of this course.\n\nL1 | Probability fundamentals I\n\n\nContents\n\n\nProbability\nConditional probability and independence\nExpectation of a random variable\nProbability density function for a continuous random variable\nKey discrete probability mass functions\nKey continuous probability density functions\n\n\n\n\nL2 | Probability fundamentals II\n\n\nContents\n\n\nFunctions of random variables\nMultivariate distributions\nDecision and estimation: basic definitions\nTests of significance\n\n\n\n\nL3 | Introduction to Bayesian inference\n\n\nContents\n\n\nIntroduction to Bayesian modelling\nConjugacy with distributions\nBayesian polynomial regression\n\n\n\n\nL4 | The uniqueness of the Normal distribution\n\n\nContents\n\n\nMarginal distributions\nConditional distributions\nNataf (and other) transforms\n\n\n\n\nL5 | Exact vs. approximate inference\n\n\nContents\n\n\nMaximum likelihood and maximum aposteriori estimate\nMarkov chain monte Carlo\nExpectation maximization\n\n\n\n\nL6 | Introduction to Gaussian processes\n\n\nContents\n\n\nMotivation and parallels with Bayesian optimization\nMercer kernels (spectral densities, periodic, Matern, squared exponential)\nMaking new kernels from old ones\nHyperparameters (and their hyperparameters)\n\n\n\n\nL7 | Gaussian likelihoods\n\n\nContents\n\n\nPrediction using noise-free observations\nPrediction with noisy observations\nWeight-space vs. function-space perspectives\nSemi-parametric models\n\n\n\n\nL8 | Gaussian & non-Gaussian likelihoods\n\n\nContents\n\n\nReproducing kernel Hilbert spaces\nRepresenter theorem\nNon-Gaussian likelihoods and classification\n\n\n\n\nL9 | Scaling Gaussian processes\n\n\nContents\n\n\nComputing the matrix inverse via Cholesky decomposition\nSubset of data approaches\nNystrom approximation\nInducing points\n\n\n\n\nL10 | Scaling Gaussian processes II\n\n\nContents\n\n\nVariational inference\nELBO derivation\nMinimizing the KL-divergence practically\n\n\n\n\nL11 | Multiple kernel learning\n\n\nContents\n\n\nEmpirical Bayes\nMultiple kernel learning\nGeneralized additive models\n\n\n\n\nL12 | Revisiting hyperparameter training\n\n\nContents\n\n\nHMC, NUTS, and Gibbs sampling for MCMC\n\n\n\n\nL13 | Gaussian processes and deep neural networks\n\n\nContents\n\n\nSingle and deep MLPs\nDeep Gaussian processes\nPosterior inference\n\n\n\n\nL14 | Designing bespoke deep Gaussian processes\n\n\nContents\n\n\nCovariance structure\nWarping inputs\nParallels to normalizing flows\nHierarchy of hyperparameters\n\n\n\n\nL15 | Time-series forecasting with Gaussian processes\n\n\nContents\n\n\nLinear Gaussian state space model\nKalman smoother\n\n\n\n\nL16 | Conditioning on linear operators\n\n\nContents\n\n\nIntegral operators\nDifferential operators\n\n\n\n\nL17 | Multi-output Gaussian processes\n\n\nContents\n\n\nCoregional models\nTransfer learning across outputs\n\n\n\n\nL18 | Group Polynomial and other kernels\n\n\nContents\n\n\nOrthogonality and data distribution\nNumerical quadrature"
  },
  {
    "objectID": "index.html#office-hours",
    "href": "index.html#office-hours",
    "title": "Overview",
    "section": "Office hours",
    "text": "Office hours\nProfessor Seshadri’s office hours:\n\n\n\nLocation\nTime\n\n\n\n\nGU 341\nTBD\n\n\nGU 341\nTBD\n\n\n\nLocation may change during term."
  },
  {
    "objectID": "index.html#textbooks",
    "href": "index.html#textbooks",
    "title": "Overview",
    "section": "Textbooks",
    "text": "Textbooks\nThis course will make heavy use of the following texts:\n\nRasmussen, C. E., Williams, C. K. Gaussian Processes for Machine Learning, The MIT Press, 2006.\nMurphy, K. P., Probabilistic Machine Learning: Advanced Topics, The MIT Press, 2023."
  },
  {
    "objectID": "index.html#important-papers",
    "href": "index.html#important-papers",
    "title": "Overview",
    "section": "Important papers",
    "text": "Important papers\nStudents are encouraged to read through the following papers:\n\nRoberts, S., Osborne, M., Ebden, M., Reece, S., Gibson, N., Aigrain, S., (2013) Gaussian processes for time-series modelling, Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences.\nDunlop, M., Girolami, M., Stuart, A., Teckentrup, A., (2018) How Deep Are Deep Gaussian Processes?, Journal of Machine Learning Research 19, 1-46\nAlvarez, M., Lawrence, N., (2011) Computationally Efficient Convolved Multiple Output Gaussian Processes, Journal of Machine Learning Research 12, 1459-1500\nVan der Wilk, M., Rasmussen, C., Hensman, J., (2017) Convolutional Gaussian Processes, 31st Conference on Neural Information Processing Systems"
  }
]