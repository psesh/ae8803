[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Overview",
    "section": "",
    "text": "This graduate-level course offers a practical approach to probabilistic learning with Gaussian processes (GPs). GPs represent a powerful set of methods for modeling and predicting a wide variety of spatio-temporal phenomena. Today, they are used for problems that span both regression and classification, with theoretical foundations in Bayesian inference, reproducing kernel Hilbert spaces, eigenvalue problems, and numerical integration. Rather than focus solely on these theoretical foundations, this course balances theory with practical probabilistic programming, using a variety of python-based packages. Moreover, practical engineering problems will also be discussed that see GP models that cut across other areas of machine learning including transfer learning, convolutional networks, and normalizing flows."
  },
  {
    "objectID": "index.html#grading",
    "href": "index.html#grading",
    "title": "Overview",
    "section": "Grading",
    "text": "Grading\nThis course has four assignments; the grades are given below:\n\n\n\n\n\n\n\nAssignment\nGrade percentage (%)\n\n\n\n\nAssignment 1: Mid-term (covering fundamentals)\n20\n\n\nAssignment 2: Build your own GP from scratch for a given dataset\n20\n\n\nAssignment 3: Proposal (data and literature review)\n20\n\n\nAssignment 4: Final project (presentation and notebook)\n40\n\n\n\n\nPre-requisites:\n\nCS1371, MATH2551, MATH2552 (or equivalent)\nWorking knowledge of python including familiarity with numpy and matplotlib libraries.\nWorking local version of python and Jupyter."
  },
  {
    "objectID": "index.html#lectures",
    "href": "index.html#lectures",
    "title": "Overview",
    "section": "Lectures",
    "text": "Lectures\nBelow you will find a list of the lectures that form the backbone of this course. Sub-topics for each lecture will be updated in due course.\n01.08: L1. Introduction & probability fundamentals | Slides | Examples\n\n\n\nContents\n\n\nCourse overview.\nProbability fundamentals (and Bayes’ theorem).\nRandom variables.\nExpectation.\n\n\n01.10: L2. Discrete probability distributions\n\n\nContents\n\n\nVariance.\nIndependence.\nBernoulli, Binomial, Geometric, and Poisson distributions.\n\n\n01.15: No Class (Institute Holiday)\n01.17: L3. Continuous distributions\n\n\nContents\n\n\nFundamentals of continuous random variables.\nProbability density function.\nExponential, Beta, and Gaussian distributions.\n\n\n01.22: L4. Manipulating and combining distributions\n\n\nContents\n\n\nFunctions of random variables.\nSums of random variables.\nTransforming a distribution.\nCentral limit theorem.\n\n\n01.24: L5. Multivariate Gaussian distributions\n\n\nContents\n\n\nMarginal distributions.\nConditional distributions.\nJoint distribution and Schur complement.\nKullback-Leibler divergence and Wasserstein-2 distance.\n\n\n01.29: L6. Bayesian inference in practice\n\n\nContents\n\n\nConjugacy in Bayesian inference.\nPolynomial Bayesian inference: an example\n\n\n01.31: L7. Gaussian process regression\n\n\nContents\n\n\nContrast weight-space vs function-space perspective.\nIntroduction to a kernel.\nLikelihood and prior for a Gaussian process.\nPosterior mean and covariance.\n\n\n02.05: Fundamentals Mid-term\n02.07: L8. Hyperparameters and model selection\n\n\nContents\n\n\nMaximum likelihood and maximum aposteriori estimate.\nCross validation.\nExpectation maximization.\nMarkov chain Monte Carlo (Gibbs, NUTS, HMC).\n\n\n02.12: L9. Variational inference\n\n\nContents\n\n\nVariational problem.\nDeriving the ELBO.\nStochastic variational inference in practice.\n\n\n02.14: L10. Open-source resources\n\n\nContents\n\n\npymc.\ngpytorch, gpflow.\nGPjax.\n\n\n02.14: L11. Kernel learning\n\n\nContents\n\n\nKernel trick re-visited. 2. Constructing kernels piece-by-piece. 3. Constructing kernels from learnt features. 4. Spectral representations of kernels.\n\n\n02.19: L12. Gaussian process classification\n\n\nContents\n\n\nBernoulli prior\nSoftmax for multi-class classification\n\n\n02.21: L13. Scaling up Gausssian processes I\n\n\nContents\n\n\nReview of matrix inverse via Cholesky.\nSubset of data approaches\nNystrom approximation\nInducing points\nKronecker product kernels.\n\n\n02.26: L14. Scaling up Gausssian processes II\n\n\nContents\n\n\nVariational inference\nELBO derivation\nMinimizing the KL-divergence practically using Adam.\n\n\n02.28: L15. Subspace-based projections for Gaussian processes\n\n\nContents\n\n\nBrief introduction to matrix manifolds.\nSubspace-based projections.\nActive subspaces.\nRegression over an unknown subspace.\n\n\n03.04: L16. Proposal and project\n\n\nContents\n\n\nChosen data-set(s) and problem statement.\nLiterature review.\nPrior and likelihood definitions.\n\n\n03.06: Coding assignment due\n03.06: L17. Reproducing Kernel Hilbert Spaces\n\n\nContents\n\n\nHilbert space\nUnderstanding a kernel.\nReproducing kernel Hilbert spaces.\nRepresenter theoreom.\n\n\n03.11: L18. Multi-output Gaussian processes\n\n\nContents\n\n\nCoregional models.\nTransfer learning across covariance blocks.\nDerivative (or gradient) enhancement.\n\n\n03.13: L19. Deep Gaussian processes\n\n\nContents\n\n\nSingle and deep MLPs\nDepth in Gaussian processes.\nPosterior inference and stochastic variational inference.\n\n\n03.13: Withdrawal Deadline\n03.18-03.22: Spring Break\n03.25: Project proposals due\n03.25: L20. Convolutional Gaussian processes\n\n\nContents\n\n\nConvolution as a linear operator.\nDeep convolutional Gaussian processes.\n\n\n03.27: L21. Latent models and unsupervised learning\n\n\nContents\n\n\nContrast standard regression with latent variable model.\nGaussian process latent variable model.\nCoding demo.\n\n\n04.01: L22. State-space Gaussian processes\n\n\nContents\n\n\nApplication: time series models.\nGaussian state space model.\nParallels with Kalman filtering and smoothing.\nCreating custom state-space kernels.\n\n\n04.03: L23. Bayesian optimization\n\n\nContents\n\n\nGaussian process surrogate.\nAcquisition function.\nThompson’s sampling.\nGaussian process dynamic model.\n\n\n04.08: L24. Guest Lecture\n04.22: L25. Project presentations"
  },
  {
    "objectID": "index.html#office-hours",
    "href": "index.html#office-hours",
    "title": "Overview",
    "section": "Office hours",
    "text": "Office hours\nProfessor Seshadri’s office hours:\n\n\n\nLocation\nTime\n\n\n\n\nMK 421\nFridays 14:30 to 15:30"
  },
  {
    "objectID": "index.html#textbooks",
    "href": "index.html#textbooks",
    "title": "Overview",
    "section": "Textbooks",
    "text": "Textbooks\nThis course will make heavy use of the following texts:\n\nRasmussen, C. E., Williams, C. K. Gaussian Processes for Machine Learning, The MIT Press, 2006.\nMurphy, K. P., Probabilistic Machine Learning: Advanced Topics, The MIT Press, 2023.\n\nBoth these texts have been made freely available by the authors."
  },
  {
    "objectID": "index.html#important-papers",
    "href": "index.html#important-papers",
    "title": "Overview",
    "section": "Important papers",
    "text": "Important papers\nStudents are encouraged to read through the following papers:\n\nRoberts, S., Osborne, M., Ebden, M., Reece, S., Gibson, N., Aigrain, S., (2013) Gaussian processes for time-series modelling, Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences.\nDunlop, M., Girolami, M., Stuart, A., Teckentrup, A., (2018) How Deep Are Deep Gaussian Processes?, Journal of Machine Learning Research 19, 1-46\nAlvarez, M., Lawrence, N., (2011) Computationally Efficient Convolved Multiple Output Gaussian Processes, Journal of Machine Learning Research 12, 1459-1500\nVan der Wilk, M., Rasmussen, C., Hensman, J., (2017) Convolutional Gaussian Processes, 31st Conference on Neural Information Processing Systems"
  },
  {
    "objectID": "sample_problems/lecture_1.html",
    "href": "sample_problems/lecture_1.html",
    "title": "L1 examples",
    "section": "",
    "text": "The probability that a scheduled flight departs on time is 0.83 and the probability that it arrives on time is 0.92. The probability that it both departs and arrives on time is 0.78. Find the probability that\n\nthe plane arrives on time given that it departed on time;\nthe plane did not depart on time given that it did not arrive on time.\n\n\n\nSolution\n\nIt will be useful to consider the Venn diagram shown below.\n\nLet \\(\\require{color}{\\color[rgb]{0.000066,0.001801,0.998229}A}\\) denote the event that the plane arrives on time, while \\({\\color[rgb]{0.986252,0.007236,0.027423}D}\\) denotes th event that the plane departs on time. To construct the Venn diagram above note that \\(P\\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) = 0.78\\). From the sum rule of probabilities, we have:\n\\[\n\\require{color}\n\\large\nP\\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\right) = P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap \\bar{{\\color[rgb]{0.986252,0.007236,0.027423}D}} \\right) + P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right)\n\\]\n\\[\n\\require{color}\n\\large\n0.92= 0.78+ P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right)\n\\]\nwhich implies that \\(\\require{color} P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) = 0.92 - 0.78 = 0.14\\). Similarly, we have:\n\\[\n\\require{color}\n\\large\nP \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) = P \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\cap \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) + P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right)\n\\]\n\\[\n\\require{color}\n\\large\n\\Rightarrow 0.83 = P \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\cap \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) + 0.78\n\\]\nwhich implies that \\(P \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\cap \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) = 0.05\\). With these probabilities, we can now answer the questions.\n\nThe plane arrives on time conditioned that it departed on time:\n\n\\[\n\\large\n\\require{color}\nP \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} | {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) = \\frac{P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) }{P \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) } = \\frac{0.78}{0.83} = 0.94\n\\]\n\nThe plane did not depart on time conditioned on it having not arrived on time:\n\n\\[\n\\large\n\\require{color}\nP \\left( \\bar{{\\color[rgb]{0.986252,0.007236,0.027423}D}} | \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) = \\frac{P \\left( \\bar{{\\color[rgb]{0.986252,0.007236,0.027423}D } }\\cap \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) }{P \\left( \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) }\n\\]\n\\[\n\\large\n\\require{color}\n= \\frac{1 - P\\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\right) - P \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) + P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) }{1 - P\\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\right) } = \\frac{1 - 0.92 - 0.83 + 0.78}{1 - 0.92} = \\frac{0.03}{0.08} = 0.375\n\\]"
  },
  {
    "objectID": "sample_problems/lecture_1.html#problem-1",
    "href": "sample_problems/lecture_1.html#problem-1",
    "title": "L1 examples",
    "section": "",
    "text": "The probability that a scheduled flight departs on time is 0.83 and the probability that it arrives on time is 0.92. The probability that it both departs and arrives on time is 0.78. Find the probability that\n\nthe plane arrives on time given that it departed on time;\nthe plane did not depart on time given that it did not arrive on time.\n\n\n\nSolution\n\nIt will be useful to consider the Venn diagram shown below.\n\nLet \\(\\require{color}{\\color[rgb]{0.000066,0.001801,0.998229}A}\\) denote the event that the plane arrives on time, while \\({\\color[rgb]{0.986252,0.007236,0.027423}D}\\) denotes th event that the plane departs on time. To construct the Venn diagram above note that \\(P\\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) = 0.78\\). From the sum rule of probabilities, we have:\n\\[\n\\require{color}\n\\large\nP\\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\right) = P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap \\bar{{\\color[rgb]{0.986252,0.007236,0.027423}D}} \\right) + P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right)\n\\]\n\\[\n\\require{color}\n\\large\n0.92= 0.78+ P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right)\n\\]\nwhich implies that \\(\\require{color} P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) = 0.92 - 0.78 = 0.14\\). Similarly, we have:\n\\[\n\\require{color}\n\\large\nP \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) = P \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\cap \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) + P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right)\n\\]\n\\[\n\\require{color}\n\\large\n\\Rightarrow 0.83 = P \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\cap \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) + 0.78\n\\]\nwhich implies that \\(P \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\cap \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) = 0.05\\). With these probabilities, we can now answer the questions.\n\nThe plane arrives on time conditioned that it departed on time:\n\n\\[\n\\large\n\\require{color}\nP \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} | {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) = \\frac{P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) }{P \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) } = \\frac{0.78}{0.83} = 0.94\n\\]\n\nThe plane did not depart on time conditioned on it having not arrived on time:\n\n\\[\n\\large\n\\require{color}\nP \\left( \\bar{{\\color[rgb]{0.986252,0.007236,0.027423}D}} | \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) = \\frac{P \\left( \\bar{{\\color[rgb]{0.986252,0.007236,0.027423}D } }\\cap \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) }{P \\left( \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) }\n\\]\n\\[\n\\large\n\\require{color}\n= \\frac{1 - P\\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\right) - P \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) + P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) }{1 - P\\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\right) } = \\frac{1 - 0.92 - 0.83 + 0.78}{1 - 0.92} = \\frac{0.03}{0.08} = 0.375\n\\]"
  },
  {
    "objectID": "sample_problems/lecture_1.html#problem-2",
    "href": "sample_problems/lecture_1.html#problem-2",
    "title": "L1 examples",
    "section": "Problem 2",
    "text": "Problem 2\nToss a coin three times, what is the probability of at least two heads?\n\n\nSolution\n\nThere are 8 possible outcomes which, if the coin is unbiased, should all be equally likely:\n\nHHH\nHHT\nHTH\nHTT\nTHH\nTHT\nTTH\nTTT\n\nTwo or more heads result from 4 outcomes. The probability of two or more heads is therefore \\(4/8=1/2\\)."
  },
  {
    "objectID": "sample_problems/lecture_1.html#problem-3",
    "href": "sample_problems/lecture_1.html#problem-3",
    "title": "L1 examples",
    "section": "Problem 3",
    "text": "Problem 3\nThis problem introduces the idea that whilst it may be tempting to add probabilities, the context is very important.\nAround 0.9% of the population are blue-green color blind and roughly 1 in 5 is left-handed. Assuming these characteristics are inherited independently, calculate the probability that a person, chosen at random will:\n\nbe both color-blind and left-handed\nbe color-blind and not left-handed\nbe color-blind or left-handed\nbe neither color-blind nor left-handed\n\n\n\nSolution\n\nConsider the diagram shown below; given that the characteristics are inherited independently, each sub-branch of the population can be divided into color-blind and non-color-blind groups.\n\n\nthe probability of being both color-blind and left-handed is: \\(0.009 \\times 0.2 = 0.0018\\) or \\(0.18 \\%\\).\nthe probability of being color-blind and right-handed is: \\(0.009 \\times 0.8 = 0.0072\\).\nthis is the sum of all probabilities within the first branch and the probability calculated in the prior step, i.e., \\(0.20 + 0.0072 = 0.2072\\).\nthis is given by the last group, i.e., \\(0.991 \\times 0.8 = 0.7928\\)."
  }
]