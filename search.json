[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Overview",
    "section": "",
    "text": "This graduate-level course offers a practical approach to probabilistic learning with Gaussian processes (GPs). GPs represent a powerful set of methods for modeling and predicting a wide variety of spatio-temporal phenomena. Today, they are used for problems that span both regression and classification, with theoretical foundations in Bayesian inference, reproducing kernel Hilbert spaces, eigenvalue problems, and numerical integration. Rather than focus solely on these theoretical foundations, this course balances theory with practical probabilistic programming, using a variety of python-based packages. Moreover, practical engineering problems will also be discussed that see GP models that cut across other areas of machine learning including transfer learning, convolutional networks, and normalizing flows."
  },
  {
    "objectID": "index.html#grading",
    "href": "index.html#grading",
    "title": "Overview",
    "section": "Grading",
    "text": "Grading\nThis course has four assignments; the grades are given below:\n\n\n\n\n\n\n\nAssignment\nGrade percentage (%)\n\n\n\n\nAssignment 1: Mid-term (covering fundamentals)\n20\n\n\nAssignment 2: Build your own GP from scratch for a given dataset\n20\n\n\nAssignment 3: Proposal (data and literature review)\n20\n\n\nAssignment 4: Final project (presentation and notebook)\n40\n\n\n\n\nPre-requisites:\n\nCS1371, MATH2551, MATH2552 (or equivalent)\nWorking knowledge of python including familiarity with numpy and matplotlib libraries.\nWorking local version of python and Jupyter."
  },
  {
    "objectID": "index.html#lectures",
    "href": "index.html#lectures",
    "title": "Overview",
    "section": "Lectures",
    "text": "Lectures\nBelow you will find a list of the lectures that form the backbone of this course. Sub-topics for each lecture will be updated in due course.\n01.08: L1. Introduction & probability fundamentals | Slides | Examples\n\n\n\nContents\n\n\nCourse overview.\nProbability fundamentals (and Bayes’ theorem).\nRandom variables.\nExpectation.\n\n\n01.10: L2. Discrete probability distributions\n\n\nContents\n\n\nVariance.\nIndependence.\nBernoulli, Binomial, Geometric, and Poisson distributions.\n\n\n01.15: No Class (Institute Holiday)\n01.17: L3. Continuous distributions\n\n\nContents\n\n\nFundamentals of continuous random variables.\nProbability density function.\nExponential, Beta, and Gaussian distributions.\n\n\n01.22: L4. Manipulating and combining distributions\n\n\nContents\n\n\nFunctions of random variables.\nSums of random variables.\nTransforming a distribution.\nCentral limit theorem.\n\n\n01.24: L5. Multivariate Gaussian distributions\n\n\nContents\n\n\nMarginal distributions.\nConditional distributions.\nJoint distribution and Schur complement.\nKullback-Leibler divergence and Wasserstein-2 distance.\n\n\n01.29: L6. Bayesian inference in practice\n\n\nContents\n\n\nConjugacy in Bayesian inference.\nPolynomial Bayesian inference: an example\n\n\n01.31: L7. Gaussian process regression\n\n\nContents\n\n\nContrast weight-space vs function-space perspective.\nIntroduction to a kernel.\nLikelihood and prior for a Gaussian process.\nPosterior mean and covariance.\n\n\n02.05: Fundamentals Mid-term\n02.07: L8. Hyperparameters and model selection\n\n\nContents\n\n\nMaximum likelihood and maximum aposteriori estimate.\nCross validation.\nExpectation maximization.\nMarkov chain Monte Carlo (Gibbs, NUTS, HMC).\n\n\n02.12: L9. Variational inference\n\n\nContents\n\n\nVariational problem.\nDeriving the ELBO.\nStochastic variational inference in practice.\n\n\n02.14: L10. Open-source resources\n\n\nContents\n\n\npymc.\ngpytorch, gpflow.\nGPjax.\n\n\n02.14: L11. Kernel learning\n\n\nContents\n\n\nKernel trick re-visited. 2. Constructing kernels piece-by-piece. 3. Constructing kernels from learnt features. 4. Spectral representations of kernels.\n\n\n02.19: L12. Gaussian process classification\n\n\nContents\n\n\nBernoulli prior\nSoftmax for multi-class classification\n\n\n02.21: L13. Scaling up Gausssian processes I\n\n\nContents\n\n\nReview of matrix inverse via Cholesky.\nSubset of data approaches\nNystrom approximation\nInducing points\nKronecker product kernels.\n\n\n02.26: L14. Scaling up Gausssian processes II\n\n\nContents\n\n\nVariational inference\nELBO derivation\nMinimizing the KL-divergence practically using Adam.\n\n\n02.28: L15. Subspace-based projections for Gaussian processes\n\n\nContents\n\n\nBrief introduction to matrix manifolds.\nSubspace-based projections.\nActive subspaces.\nRegression over an unknown subspace.\n\n\n03.04: L16. Proposal and project\n\n\nContents\n\n\nChosen data-set(s) and problem statement.\nLiterature review.\nPrior and likelihood definitions.\n\n\n03.06: Coding assignment due\n03.06: L17. Reproducing Kernel Hilbert Spaces\n\n\nContents\n\n\nHilbert space\nUnderstanding a kernel.\nReproducing kernel Hilbert spaces.\nRepresenter theoreom.\n\n\n03.11: L18. Multi-output Gaussian processes\n\n\nContents\n\n\nCoregional models.\nTransfer learning across covariance blocks.\nDerivative (or gradient) enhancement.\n\n\n03.13: L19. Deep Gaussian processes\n\n\nContents\n\n\nSingle and deep MLPs\nDepth in Gaussian processes.\nPosterior inference and stochastic variational inference.\n\n\n03.13: Withdrawal Deadline\n03.18-03.22: Spring Break\n03.25: Project proposals due\n03.25: L20. Convolutional Gaussian processes\n\n\nContents\n\n\nConvolution as a linear operator.\nDeep convolutional Gaussian processes.\n\n\n03.27: L21. Latent models and unsupervised learning\n\n\nContents\n\n\nContrast standard regression with latent variable model.\nGaussian process latent variable model.\nCoding demo.\n\n\n04.01: L22. State-space Gaussian processes\n\n\nContents\n\n\nApplication: time series models.\nGaussian state space model.\nParallels with Kalman filtering and smoothing.\nCreating custom state-space kernels.\n\n\n04.03: L23. Bayesian optimization\n\n\nContents\n\n\nGaussian process surrogate.\nAcquisition function.\nThompson’s sampling.\nGaussian process dynamic model.\n\n\n04.08: L24. Guest Lecture\n04.22: L25. Project presentations"
  },
  {
    "objectID": "index.html#office-hours",
    "href": "index.html#office-hours",
    "title": "Overview",
    "section": "Office hours",
    "text": "Office hours\nProfessor Seshadri’s office hours:\n\n\n\nLocation\nTime\n\n\n\n\nMK 421\nFridays 14:30 to 15:30"
  },
  {
    "objectID": "index.html#textbooks",
    "href": "index.html#textbooks",
    "title": "Overview",
    "section": "Textbooks",
    "text": "Textbooks\nThis course will make heavy use of the following texts:\n\nRasmussen, C. E., Williams, C. K. Gaussian Processes for Machine Learning, The MIT Press, 2006.\nMurphy, K. P., Probabilistic Machine Learning: Advanced Topics, The MIT Press, 2023.\n\nBoth these texts have been made freely available by the authors."
  },
  {
    "objectID": "index.html#important-papers",
    "href": "index.html#important-papers",
    "title": "Overview",
    "section": "Important papers",
    "text": "Important papers\nStudents are encouraged to read through the following papers:\n\nRoberts, S., Osborne, M., Ebden, M., Reece, S., Gibson, N., Aigrain, S., (2013) Gaussian processes for time-series modelling, Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences.\nDunlop, M., Girolami, M., Stuart, A., Teckentrup, A., (2018) How Deep Are Deep Gaussian Processes?, Journal of Machine Learning Research 19, 1-46\nAlvarez, M., Lawrence, N., (2011) Computationally Efficient Convolved Multiple Output Gaussian Processes, Journal of Machine Learning Research 12, 1459-1500\nVan der Wilk, M., Rasmussen, C., Hensman, J., (2017) Convolutional Gaussian Processes, 31st Conference on Neural Information Processing Systems"
  }
]