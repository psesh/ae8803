[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Overview",
    "section": "",
    "text": "This graduate-level course offers a practical approach to probabilistic learning with Gaussian processes (GPs). GPs represent a powerful set of methods for modeling and predicting a wide variety of spatio-temporal phenomena. Today, they are used for problems that span both regression and classification, with theoretical foundations in Bayesian inference, reproducing kernel Hilbert spaces, eigenvalue problems, and numerical integration. Rather than focus solely on these theoretical foundations, this course balances theory with practical probabilistic programming, using a variety of python-based packages. Moreover, practical engineering problems will also be discussed that see GP models that cut across other areas of machine learning including transfer learning, convolutional networks, and normalizing flows."
  },
  {
    "objectID": "index.html#grading",
    "href": "index.html#grading",
    "title": "Overview",
    "section": "Grading",
    "text": "Grading\nThis course has four assignments; the grades are given below:\n\n\n\n\n\n\n\nAssignment\nGrade percentage (%)\n\n\n\n\nAssignment 1: Mid-term (covering fundamentals)\n20\n\n\nAssignment 2: Build your own GP from scratch for a given dataset\n20\n\n\nAssignment 3: Proposal (data and literature review)\n20\n\n\nAssignment 4: Final project (presentation and notebook)\n40\n\n\n\n\nPre-requisites:\n\nCS1371, MATH2551, MATH2552 (or equivalent)\nWorking knowledge of python including familiarity with numpy and matplotlib libraries.\nWorking local version of python and Jupyter."
  },
  {
    "objectID": "index.html#lectures",
    "href": "index.html#lectures",
    "title": "Overview",
    "section": "Lectures",
    "text": "Lectures\nBelow you will find a list of the lectures that form the backbone of this course. Sub-topics for each lecture will be updated in due course.\n01.08: L1. Introduction & probability fundamentals | Slides | Examples\n\n\n\nContents\n\n\nCourse overview.\nProbability fundamentals (and Bayes’ theorem).\nRandom variables.\n\n\n01.10: L2. Discrete probability distributions | Slides | Examples | Notebook\n\n\nContents\n\n\nExpectation and variance.\nIndependence.\nBernoulli and Binomial distributions.\n\n\n01.15: No Class (Institute Holiday)\n01.17: L3. Continuous distributions\n\n\nContents\n\n\nFundamentals of continuous random variables.\nProbability density function.\nExponential, Beta, and Gaussian distributions.\n\n\n01.22: L4. Manipulating and combining distributions\n\n\nContents\n\n\nFunctions of random variables.\nSums of random variables.\nTransforming a distribution.\nCentral limit theorem.\n\n\n01.24: L5. Multivariate Gaussian distributions\n\n\nContents\n\n\nMarginal distributions.\nConditional distributions.\nJoint distribution and Schur complement.\nKullback-Leibler divergence and Wasserstein-2 distance.\n\n\n01.29: L6. Bayesian inference in practice\n\n\nContents\n\n\nConjugacy in Bayesian inference.\nPolynomial Bayesian inference: an example\n\n\n01.31: L7. Gaussian process regression\n\n\nContents\n\n\nContrast weight-space vs function-space perspective.\nIntroduction to a kernel.\nLikelihood and prior for a Gaussian process.\nPosterior mean and covariance.\n\n\n02.05: Fundamentals Mid-term\n02.07: L8. Hyperparameters and model selection\n\n\nContents\n\n\nMaximum likelihood and maximum aposteriori estimate.\nCross validation.\nExpectation maximization.\nMarkov chain Monte Carlo (Gibbs, NUTS, HMC).\n\n\n02.12: L9. Variational inference\n\n\nContents\n\n\nVariational problem.\nDeriving the ELBO.\nStochastic variational inference in practice.\n\n\n02.14: L10. Open-source resources\n\n\nContents\n\n\npymc.\ngpytorch, gpflow.\nGPjax.\n\n\n02.14: L11. Kernel learning\n\n\nContents\n\n\nKernel trick re-visited. 2. Constructing kernels piece-by-piece. 3. Constructing kernels from learnt features. 4. Spectral representations of kernels.\n\n\n02.19: L12. Gaussian process classification\n\n\nContents\n\n\nBernoulli prior\nSoftmax for multi-class classification\n\n\n02.21: L13. Scaling up Gausssian processes I\n\n\nContents\n\n\nReview of matrix inverse via Cholesky.\nSubset of data approaches\nNystrom approximation\nInducing points\nKronecker product kernels.\n\n\n02.26: L14. Scaling up Gausssian processes II\n\n\nContents\n\n\nVariational inference\nELBO derivation\nMinimizing the KL-divergence practically using Adam.\n\n\n02.28: L15. Sparse (and subspace-based) Gaussian processes\n\n\nContents\n\n\nBrief introduction to matrix manifolds.\nSubspace-based projections.\nActive subspaces.\nSparsity promoting priors.\n\n\n03.04: L16. Proposal and project\n\n\nContents\n\n\nChosen data-set(s) and problem statement.\nLiterature review.\nPrior and likelihood definitions.\n\n\n03.06: Coding assignment due\n03.06: L17. Reproducing Kernel Hilbert Spaces\n\n\nContents\n\n\nHilbert space\nUnderstanding a kernel.\nReproducing kernel Hilbert spaces.\nRepresenter theoreom.\n\n\n03.11: L18. Multi-output Gaussian processes\n\n\nContents\n\n\nCoregional models.\nTransfer learning across covariance blocks.\nDerivative (or gradient) enhancement.\n\n\n03.13: L19. Deep Gaussian processes\n\n\nContents\n\n\nSingle and deep MLPs\nDepth in Gaussian processes.\nPosterior inference and stochastic variational inference.\n\n\n03.13: Withdrawal Deadline\n03.18-03.22: Spring Break\n03.25: Project proposals due\n03.25: L20. Convolutional Gaussian processes\n\n\nContents\n\n\nConvolution as a linear operator.\nDeep convolutional Gaussian processes.\n\n\n03.27: L21. Latent models and unsupervised learning\n\n\nContents\n\n\nContrast standard regression with latent variable model.\nGaussian process latent variable model.\nCoding demo.\n\n\n04.01: L22. State-space Gaussian processes\n\n\nContents\n\n\nApplication: time series models.\nGaussian state space model.\nParallels with Kalman filtering and smoothing.\nCreating custom state-space kernels.\n\n\n04.03: L23. Bayesian optimization\n\n\nContents\n\n\nGaussian process surrogate.\nAcquisition function.\nThompson’s sampling.\nGaussian process dynamic model.\n\n\n04.08: L24. Guest Lecture\n04.22: L25. Project presentations"
  },
  {
    "objectID": "index.html#office-hours",
    "href": "index.html#office-hours",
    "title": "Overview",
    "section": "Office hours",
    "text": "Office hours\nProfessor Seshadri’s office hours:\n\n\n\nLocation\nTime\n\n\n\n\nMK 421\nFridays 14:30 to 15:30"
  },
  {
    "objectID": "index.html#textbooks",
    "href": "index.html#textbooks",
    "title": "Overview",
    "section": "Textbooks",
    "text": "Textbooks\nThis course will make heavy use of the following texts:\n\nRasmussen, C. E., Williams, C. K. Gaussian Processes for Machine Learning, The MIT Press, 2006.\nMurphy, K. P., Probabilistic Machine Learning: Advanced Topics, The MIT Press, 2023.\n\nBoth these texts have been made freely available by the authors."
  },
  {
    "objectID": "index.html#important-papers",
    "href": "index.html#important-papers",
    "title": "Overview",
    "section": "Important papers",
    "text": "Important papers\nStudents are encouraged to read through the following papers:\n\nRoberts, S., Osborne, M., Ebden, M., Reece, S., Gibson, N., Aigrain, S., (2013) Gaussian processes for time-series modelling, Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences.\nDunlop, M., Girolami, M., Stuart, A., Teckentrup, A., (2018) How Deep Are Deep Gaussian Processes?, Journal of Machine Learning Research 19, 1-46\nAlvarez, M., Lawrence, N., (2011) Computationally Efficient Convolved Multiple Output Gaussian Processes, Journal of Machine Learning Research 12, 1459-1500\nVan der Wilk, M., Rasmussen, C., Hensman, J., (2017) Convolutional Gaussian Processes, 31st Conference on Neural Information Processing Systems"
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Overview",
    "section": "References",
    "text": "References\nMaterial used in this course has been adapted from\n\nCUED Part IB probability course notes\nAlto University’s module on Gaussian Processes\nSlides from the Gaussian Process Summer Schools"
  },
  {
    "objectID": "sample_problems/lecture_2.html",
    "href": "sample_problems/lecture_2.html",
    "title": "L2 examples",
    "section": "",
    "text": "Commercial airline pilots need to pass four out of five separate tests for certification. Assume that the tests are equally difficult, and that the performance on separate tests are independent.\n\nIf the probability of failing each separate test is \\(p=0.2\\), then what is the probability of failing certification?\nTo improve safety, more stringent regulations require that pilots pass all five tests. To be able to meet the demand, the individual tests are made easier. What should the new individual failure rate be if the overall certification probability is to remain unchanged?\n\n\n\nSolution\n\n\nGiven that each test is independent, the combined probabilities follow a Binomial distribution. A pilot will fail certification if they fail two or more tests, and they will pass if they fail zero or one of the individual tests. Thus, the probability of passing certification is\n\n\\[\n\\large\np_{pass} = \\left(\\begin{array}{c}\n5\\\\\n0\n\\end{array}\\right) p^{0} \\left( 1 - p \\right)^{5} + \\left(\\begin{array}{c}\n5\\\\\n1\n\\end{array}\\right)p^{1} \\left( 1 - p \\right)^{4}\n\\]\nFrom the code snippet below this is roughly 0.737. Thus the combined failure rate is \\(1 - 0.7373 = 0.2627\\).\n\nUnder the new certification protocol, as there is no possibility of failing a test, we have\n\n\\[\n\\large\n\\left(1 - p_{fail, new} \\right)^{5} = 1 - 0.2627 \\Rightarrow p_{fail, new} = 0.06\n\\]\n\n\n\nCode\nfrom scipy.special import comb\nimport numpy as np\n\n# part a.\np = 0.2\np_pass = comb(5, 0) * p**0 * (1 - p)**5 + comb(5, 1) * p**1 * (1 - p)**4\np_fail = 1 - p_pass\nprint(p_fail)\n\n# part b.\np_fail_new = 1 - (1 - p_fail)**(1/5)\nprint(p_fail_new)\n\n\n0.26271999999999984\n0.059136781980261066"
  },
  {
    "objectID": "sample_problems/lecture_2.html#problem-1",
    "href": "sample_problems/lecture_2.html#problem-1",
    "title": "L2 examples",
    "section": "",
    "text": "Commercial airline pilots need to pass four out of five separate tests for certification. Assume that the tests are equally difficult, and that the performance on separate tests are independent.\n\nIf the probability of failing each separate test is \\(p=0.2\\), then what is the probability of failing certification?\nTo improve safety, more stringent regulations require that pilots pass all five tests. To be able to meet the demand, the individual tests are made easier. What should the new individual failure rate be if the overall certification probability is to remain unchanged?\n\n\n\nSolution\n\n\nGiven that each test is independent, the combined probabilities follow a Binomial distribution. A pilot will fail certification if they fail two or more tests, and they will pass if they fail zero or one of the individual tests. Thus, the probability of passing certification is\n\n\\[\n\\large\np_{pass} = \\left(\\begin{array}{c}\n5\\\\\n0\n\\end{array}\\right) p^{0} \\left( 1 - p \\right)^{5} + \\left(\\begin{array}{c}\n5\\\\\n1\n\\end{array}\\right)p^{1} \\left( 1 - p \\right)^{4}\n\\]\nFrom the code snippet below this is roughly 0.737. Thus the combined failure rate is \\(1 - 0.7373 = 0.2627\\).\n\nUnder the new certification protocol, as there is no possibility of failing a test, we have\n\n\\[\n\\large\n\\left(1 - p_{fail, new} \\right)^{5} = 1 - 0.2627 \\Rightarrow p_{fail, new} = 0.06\n\\]\n\n\n\nCode\nfrom scipy.special import comb\nimport numpy as np\n\n# part a.\np = 0.2\np_pass = comb(5, 0) * p**0 * (1 - p)**5 + comb(5, 1) * p**1 * (1 - p)**4\np_fail = 1 - p_pass\nprint(p_fail)\n\n# part b.\np_fail_new = 1 - (1 - p_fail)**(1/5)\nprint(p_fail_new)\n\n\n0.26271999999999984\n0.059136781980261066"
  },
  {
    "objectID": "sample_problems/lecture_1.html",
    "href": "sample_problems/lecture_1.html",
    "title": "L1 examples",
    "section": "",
    "text": "The probability that a scheduled flight departs on time is 0.83 and the probability that it arrives on time is 0.92. The probability that it both departs and arrives on time is 0.78. Find the probability that\n\nthe plane arrives on time given that it departed on time;\nthe plane did not depart on time given that it did not arrive on time.\n\n\n\nSolution\n\nIt will be useful to consider the Venn diagram shown below.\n\nLet \\(\\require{color}{\\color[rgb]{0.000066,0.001801,0.998229}A}\\) denote the event that the plane arrives on time, while \\({\\color[rgb]{0.986252,0.007236,0.027423}D}\\) denotes th event that the plane departs on time. To construct the Venn diagram above note that \\(P\\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) = 0.78\\). From the sum rule of probabilities, we have:\n\\[\n\\require{color}\n\\large\nP\\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\right) = P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap \\bar{{\\color[rgb]{0.986252,0.007236,0.027423}D}} \\right) + P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right)\n\\]\n\\[\n\\require{color}\n\\large\n0.92= 0.78+ P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right)\n\\]\nwhich implies that \\(\\require{color} P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) = 0.92 - 0.78 = 0.14\\). Similarly, we have:\n\\[\n\\require{color}\n\\large\nP \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) = P \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\cap \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) + P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right)\n\\]\n\\[\n\\require{color}\n\\large\n\\Rightarrow 0.83 = P \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\cap \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) + 0.78\n\\]\nwhich implies that \\(P \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\cap \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) = 0.05\\). With these probabilities, we can now answer the questions.\n\nThe plane arrives on time conditioned that it departed on time:\n\n\\[\n\\large\n\\require{color}\nP \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} | {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) = \\frac{P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) }{P \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) } = \\frac{0.78}{0.83} = 0.94\n\\]\n\nThe plane did not depart on time conditioned on it having not arrived on time:\n\n\\[\n\\large\n\\require{color}\nP \\left( \\bar{{\\color[rgb]{0.986252,0.007236,0.027423}D}} | \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) = \\frac{P \\left( \\bar{{\\color[rgb]{0.986252,0.007236,0.027423}D } }\\cap \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) }{P \\left( \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) }\n\\]\n\\[\n\\large\n\\require{color}\n= \\frac{1 - P\\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\right) - P \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) + P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) }{1 - P\\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\right) } = \\frac{1 - 0.92 - 0.83 + 0.78}{1 - 0.92} = \\frac{0.03}{0.08} = 0.375\n\\]"
  },
  {
    "objectID": "sample_problems/lecture_1.html#problem-1",
    "href": "sample_problems/lecture_1.html#problem-1",
    "title": "L1 examples",
    "section": "",
    "text": "The probability that a scheduled flight departs on time is 0.83 and the probability that it arrives on time is 0.92. The probability that it both departs and arrives on time is 0.78. Find the probability that\n\nthe plane arrives on time given that it departed on time;\nthe plane did not depart on time given that it did not arrive on time.\n\n\n\nSolution\n\nIt will be useful to consider the Venn diagram shown below.\n\nLet \\(\\require{color}{\\color[rgb]{0.000066,0.001801,0.998229}A}\\) denote the event that the plane arrives on time, while \\({\\color[rgb]{0.986252,0.007236,0.027423}D}\\) denotes th event that the plane departs on time. To construct the Venn diagram above note that \\(P\\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) = 0.78\\). From the sum rule of probabilities, we have:\n\\[\n\\require{color}\n\\large\nP\\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\right) = P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap \\bar{{\\color[rgb]{0.986252,0.007236,0.027423}D}} \\right) + P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right)\n\\]\n\\[\n\\require{color}\n\\large\n0.92= 0.78+ P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right)\n\\]\nwhich implies that \\(\\require{color} P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) = 0.92 - 0.78 = 0.14\\). Similarly, we have:\n\\[\n\\require{color}\n\\large\nP \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) = P \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\cap \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) + P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right)\n\\]\n\\[\n\\require{color}\n\\large\n\\Rightarrow 0.83 = P \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\cap \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) + 0.78\n\\]\nwhich implies that \\(P \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\cap \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) = 0.05\\). With these probabilities, we can now answer the questions.\n\nThe plane arrives on time conditioned that it departed on time:\n\n\\[\n\\large\n\\require{color}\nP \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} | {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) = \\frac{P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) }{P \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) } = \\frac{0.78}{0.83} = 0.94\n\\]\n\nThe plane did not depart on time conditioned on it having not arrived on time:\n\n\\[\n\\large\n\\require{color}\nP \\left( \\bar{{\\color[rgb]{0.986252,0.007236,0.027423}D}} | \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) = \\frac{P \\left( \\bar{{\\color[rgb]{0.986252,0.007236,0.027423}D } }\\cap \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) }{P \\left( \\bar{{\\color[rgb]{0.000066,0.001801,0.998229}A}} \\right) }\n\\]\n\\[\n\\large\n\\require{color}\n= \\frac{1 - P\\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\right) - P \\left( {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) + P \\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}D} \\right) }{1 - P\\left( {\\color[rgb]{0.000066,0.001801,0.998229}A} \\right) } = \\frac{1 - 0.92 - 0.83 + 0.78}{1 - 0.92} = \\frac{0.03}{0.08} = 0.375\n\\]"
  },
  {
    "objectID": "sample_problems/lecture_1.html#problem-2",
    "href": "sample_problems/lecture_1.html#problem-2",
    "title": "L1 examples",
    "section": "Problem 2",
    "text": "Problem 2\nToss a coin three times, what is the probability of at least two heads?\n\n\nSolution\n\nThere are 8 possible outcomes which, if the coin is unbiased, should all be equally likely:\n\nHHH\nHHT\nHTH\nHTT\nTHH\nTHT\nTTH\nTTT\n\nTwo or more heads result from 4 outcomes. The probability of two or more heads is therefore \\(4/8=1/2\\)."
  },
  {
    "objectID": "sample_problems/lecture_1.html#problem-3",
    "href": "sample_problems/lecture_1.html#problem-3",
    "title": "L1 examples",
    "section": "Problem 3",
    "text": "Problem 3\nThis problem introduces the idea that whilst it may be tempting to add probabilities, the context is very important.\nAround 0.9% of the population are blue-green color blind and roughly 1 in 5 is left-handed. Assuming these characteristics are inherited independently, calculate the probability that a person, chosen at random will:\n\nbe both color-blind and left-handed\nbe color-blind and not left-handed\nbe color-blind or left-handed\nbe neither color-blind nor left-handed\n\n\n\nSolution\n\nConsider the diagram shown below; given that the characteristics are inherited independently, each sub-branch of the population can be divided into color-blind and non-color-blind groups.\n\n\nthe probability of being both color-blind and left-handed is: \\(0.009 \\times 0.2 = 0.0018\\) or \\(0.18 \\%\\).\nthe probability of being color-blind and right-handed is: \\(0.009 \\times 0.8 = 0.0072\\).\nthis is the sum of all probabilities within the first branch and the probability calculated in the prior step, i.e., \\(0.20 + 0.0072 = 0.2072\\).\nthis is given by the last group, i.e., \\(0.991 \\times 0.8 = 0.7928\\)."
  },
  {
    "objectID": "sample_problems/lecture_1.html#problem-4",
    "href": "sample_problems/lecture_1.html#problem-4",
    "title": "L1 examples",
    "section": "Problem 4",
    "text": "Problem 4\nThis problem has two parts.\n\nDerive Bayes’ rule.\nThe chance of an honest citizen lying is 1 in 1000. Assume that such a citizen is tested with a lie detector which correctly identifies both truth and false statements 95 times out of 100.\n\n\nWhat is the probability that the lie detector indicates falsehood?\nIn this case, what is the probability that the person is actually lying?\n\n\n\nSolution\n\n\nTo derive Bayes’ rule, we will use the definition of the conditional probability, and the fact that \\(p \\left({\\color[rgb]{0.986252,0.007236,0.027423}A} \\cap {\\color[rgb]{0.131302,0.999697,0.023594}B} \\right) = p \\left( {\\color[rgb]{0.131302,0.999697,0.023594}B} \\cap {\\color[rgb]{0.986252,0.007236,0.027423}A} \\right)\\), which leads to\n\n\\[\n\\large\n\\require{color}\np \\left( {\\color[rgb]{0.986252,0.007236,0.027423}A} | {\\color[rgb]{0.131302,0.999697,0.023594}B} \\right) p \\left( {\\color[rgb]{0.131302,0.999697,0.023594}B} \\right) = p \\left( {\\color[rgb]{0.131302,0.999697,0.023594}B}| {\\color[rgb]{0.986252,0.007236,0.027423}A} \\right) p \\left({\\color[rgb]{0.986252,0.007236,0.027423}A} \\right)\n\\]\nFrom this one can write\n\\[\n\\large\n\\require{color}\np \\left( {\\color[rgb]{0.986252,0.007236,0.027423}A} | {\\color[rgb]{0.131302,0.999697,0.023594}B} \\right) = \\frac{p \\left( {\\color[rgb]{0.131302,0.999697,0.023594}B} | {\\color[rgb]{0.986252,0.007236,0.027423}A} \\right) p \\left( {\\color[rgb]{0.986252,0.007236,0.027423}A} \\right) }{p \\left( {\\color[rgb]{0.131302,0.999697,0.023594}B} \\right) }\n\\]\n\nThe probability that the lie detector indicates a falsehood is based on (i) the citizen is lying, and (ii) the citizen is being honest, but the detector makes an error. Let \\(F\\) be the probability that the lie detector indicates a falsehood. Thus\n\n\\[\n\\large\np \\left( F \\right) = \\frac{1}{1000} \\times 0.95 + \\frac{999}{1000} \\times 0.05 = 0.0509.\n\\]\nLet $p ( L ) be the probability that the person is actually lying. Thus, what we want is\n\\[\n\\large\np \\left( L | F \\right) = \\frac{p \\left( F | L \\right) p \\left( L \\right) }{p \\left( F \\right) } = \\frac{0.95 \\times 0.001}{0.0509} = 0.01866.\n\\]"
  },
  {
    "objectID": "useful_codes/discrete.html",
    "href": "useful_codes/discrete.html",
    "title": "Discrete distributions",
    "section": "",
    "text": "This notebook is has useful boiler plate code for generating distributions and visualizing them.\n\n\nCode\nimport numpy as np \nfrom scipy.stats import bernoulli, binom\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.special import comb\nsns.set(font_scale=1.0)\nsns.set_style(\"white\")\nsns.set_style(\"ticks\")\npalette = sns.color_palette('deep')\n#plt.style.use('dark_background') # cosmetic!\n\n\n\n\nThe probability mass function for a Bernoulli distribution is given by\n\\[\np \\left( x \\right) = \\begin{cases}\n\\begin{array}{c}\n1 - p  \\; \\; \\; \\textrm{if} \\; x = 0 \\\\\np \\; \\; \\; \\textrm{if} \\; x = 1\n\\end{array}\\end{cases}\n\\]\nfor \\(x \\in \\left\\{0, 1 \\right\\}\\) and where \\(0 \\leq p \\leq 1\\).\n\n\nCode\np = 0.4 # Bernoulli parameter\nx = np.linspace(0, 1, 2)\nprobabilities = bernoulli.pmf(x, p)\n\nfig = plt.figure(figsize=(8,4))\n\nplt.plot(x, probabilities, 'o', ms=8, color='orangered')\nplt.vlines(x, 0, probabilities, colors='orangered', lw=5, alpha=0.5)\nplt.xlabel('x')\nplt.ylabel('Probability')\nplt.savefig('pdf.png', dpi=150, bbox_inches='tight', transparent=True)\n\nplt.show()\n\n\n\n\n\nOne can generate random values from this distribution, i.e.,\n\n\nCode\nX = bernoulli.rvs(p, size=500)\nprint(X)\n\n\n[1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0 1 0 1 1 0 1 0 0\n 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0\n 0 1 1 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0\n 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0 0\n 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0\n 0 0 1 0 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 0 1\n 0 0 0 1 0 1 0 0 0 1 1 1 0 0 1 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0\n 1 1 0 1 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0\n 0 1 0 0 0 1 0 0 1 1 1 0 1 1 0 0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 0 0 0 1\n 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 1 0 0 0 0 1 1 0 1\n 1 0 1 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 1\n 1 0 1 0 1 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 1\n 1 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0 1]\n\n\nThus, random values from a Bernoulli distribution are inherently binary, and the number of 0s vs 1s will vary depending on the choice of the parameter, \\(p\\). We will see later on (in another notebook) how this relatively simple idea can be used to train a Naive Bayes Classifier. For now, we will plot the expected value of the Bernoulli random variable with increasing number of samples.\n\n\nCode\nnumbers = [10, 50, 100, 200, 300, 500, 1000, 2000, 5000, 10000]\nmeans = []\nstds = []\nfor j in numbers:\n    X_val = []\n    for q in range(0, 10):\n        X = bernoulli.rvs(p, size=j)\n        X_val.append(np.mean(X))\n    means.append(np.mean(X_val))\n    stds.append(np.std(X_val))\n\nmeans = np.array(means)\nstds = np.array(stds)\nnumbers = np.array(numbers)\n\nfig = plt.figure(figsize=(8,4))\nplt.plot(numbers, means, 'ro-', lw=2)\nplt.fill_between(numbers, means + stds, means - stds, color='crimson', alpha=0.3)\nplt.xlabel('Number of random samples')\nplt.ylabel('Expectation')\nplt.savefig('convergence.png', dpi=150, bbox_inches='tight', transparent=True)\nplt.show()\n\n\n\n\n\n\n\n\nNext, we consider the Binomial distribution. It has a probability mass function\n\\[\np \\left( x \\right) = \\left(\\begin{array}{c}\nn\\\\\nx\n\\end{array}\\right)p^{x}\\left(1-p\\right)^{n-x}\n\\]\nfor \\(x \\in \\left\\{0, 1, \\ldots, n \\right\\}\\) and where \\(0 \\leq p \\leq 1\\).\n\n\nCode\np = 0.3 # Bernoulli parameter\nn = 7\nx = np.arange(0, n+1)\nprobabilities = binom(n, p)\n\nfig = plt.figure(figsize=(8,4))\nplt.plot(x, probabilities.pmf(x), 'o', ms=8, color='deeppink')\nplt.vlines(x, 0, probabilities.pmf(x), colors='deeppink', lw=5 )\nplt.xlabel('x')\nplt.ylabel('Probability')\nplt.savefig('pdf_2.png', dpi=150, bbox_inches='tight', transparent=True)\nplt.show()\n\n\n\n\n\nTo work out the probability at \\(x=3\\), we can compute:\n\n\nCode\nprob = comb(N=n, k=3) * p**3 * (1 - p)**(n - 3)\nprint(prob)\n\n\n0.22689449999999992"
  },
  {
    "objectID": "useful_codes/discrete.html#scope",
    "href": "useful_codes/discrete.html#scope",
    "title": "Discrete distributions",
    "section": "",
    "text": "This notebook is has useful boiler plate code for generating distributions and visualizing them.\n\n\nCode\nimport numpy as np \nfrom scipy.stats import bernoulli, binom\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.special import comb\nsns.set(font_scale=1.0)\nsns.set_style(\"white\")\nsns.set_style(\"ticks\")\npalette = sns.color_palette('deep')\n#plt.style.use('dark_background') # cosmetic!\n\n\n\n\nThe probability mass function for a Bernoulli distribution is given by\n\\[\np \\left( x \\right) = \\begin{cases}\n\\begin{array}{c}\n1 - p  \\; \\; \\; \\textrm{if} \\; x = 0 \\\\\np \\; \\; \\; \\textrm{if} \\; x = 1\n\\end{array}\\end{cases}\n\\]\nfor \\(x \\in \\left\\{0, 1 \\right\\}\\) and where \\(0 \\leq p \\leq 1\\).\n\n\nCode\np = 0.4 # Bernoulli parameter\nx = np.linspace(0, 1, 2)\nprobabilities = bernoulli.pmf(x, p)\n\nfig = plt.figure(figsize=(8,4))\n\nplt.plot(x, probabilities, 'o', ms=8, color='orangered')\nplt.vlines(x, 0, probabilities, colors='orangered', lw=5, alpha=0.5)\nplt.xlabel('x')\nplt.ylabel('Probability')\nplt.savefig('pdf.png', dpi=150, bbox_inches='tight', transparent=True)\n\nplt.show()\n\n\n\n\n\nOne can generate random values from this distribution, i.e.,\n\n\nCode\nX = bernoulli.rvs(p, size=500)\nprint(X)\n\n\n[1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0 1 0 1 1 0 1 0 0\n 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0\n 0 1 1 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0\n 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0 0\n 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0\n 0 0 1 0 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 0 1\n 0 0 0 1 0 1 0 0 0 1 1 1 0 0 1 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0\n 1 1 0 1 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0\n 0 1 0 0 0 1 0 0 1 1 1 0 1 1 0 0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 0 0 0 1\n 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 1 0 0 0 0 1 1 0 1\n 1 0 1 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 1\n 1 0 1 0 1 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 1\n 1 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0 1]\n\n\nThus, random values from a Bernoulli distribution are inherently binary, and the number of 0s vs 1s will vary depending on the choice of the parameter, \\(p\\). We will see later on (in another notebook) how this relatively simple idea can be used to train a Naive Bayes Classifier. For now, we will plot the expected value of the Bernoulli random variable with increasing number of samples.\n\n\nCode\nnumbers = [10, 50, 100, 200, 300, 500, 1000, 2000, 5000, 10000]\nmeans = []\nstds = []\nfor j in numbers:\n    X_val = []\n    for q in range(0, 10):\n        X = bernoulli.rvs(p, size=j)\n        X_val.append(np.mean(X))\n    means.append(np.mean(X_val))\n    stds.append(np.std(X_val))\n\nmeans = np.array(means)\nstds = np.array(stds)\nnumbers = np.array(numbers)\n\nfig = plt.figure(figsize=(8,4))\nplt.plot(numbers, means, 'ro-', lw=2)\nplt.fill_between(numbers, means + stds, means - stds, color='crimson', alpha=0.3)\nplt.xlabel('Number of random samples')\nplt.ylabel('Expectation')\nplt.savefig('convergence.png', dpi=150, bbox_inches='tight', transparent=True)\nplt.show()\n\n\n\n\n\n\n\n\nNext, we consider the Binomial distribution. It has a probability mass function\n\\[\np \\left( x \\right) = \\left(\\begin{array}{c}\nn\\\\\nx\n\\end{array}\\right)p^{x}\\left(1-p\\right)^{n-x}\n\\]\nfor \\(x \\in \\left\\{0, 1, \\ldots, n \\right\\}\\) and where \\(0 \\leq p \\leq 1\\).\n\n\nCode\np = 0.3 # Bernoulli parameter\nn = 7\nx = np.arange(0, n+1)\nprobabilities = binom(n, p)\n\nfig = plt.figure(figsize=(8,4))\nplt.plot(x, probabilities.pmf(x), 'o', ms=8, color='deeppink')\nplt.vlines(x, 0, probabilities.pmf(x), colors='deeppink', lw=5 )\nplt.xlabel('x')\nplt.ylabel('Probability')\nplt.savefig('pdf_2.png', dpi=150, bbox_inches='tight', transparent=True)\nplt.show()\n\n\n\n\n\nTo work out the probability at \\(x=3\\), we can compute:\n\n\nCode\nprob = comb(N=n, k=3) * p**3 * (1 - p)**(n - 3)\nprint(prob)\n\n\n0.22689449999999992"
  }
]