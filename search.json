[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Overview",
    "section": "",
    "text": "This graduate-level course offers a practical approach to probabilistic learning with Gaussian processes (GPs). GPs represent a powerful set of methods for modeling and predicting a wide variety of spatio-temporal phenomena. It is used for problems that span both regression and classification, with theoretical foundations in Bayesian inference, reproducing kernel Hilbert spaces, eigenvalue problems, and numerical integration. Rather than focus solely on these theoretical foundations, this course balances theory with practical probabilistic programming, using a variety of python-based packages. Practical engineering-relevant problems will also be discussed, cutting across other areas of machine learning such as transfer learning, deep models, and normalizing flows."
  },
  {
    "objectID": "index.html#lectures",
    "href": "index.html#lectures",
    "title": "Overview",
    "section": "Lectures",
    "text": "Lectures\nThis is a preliminary schedule; it may change throughout term.\n\nL1 | Probability fundamentals I\n\n\nContents\n\n\nProbability\nConditional probability and independence\nExpectation of a random variable\nProbability density function for a continuous random variable\nKey discrete probability mass functions\nKey continuous probability density functions\n\n\n\n\nL2 | Probability fundamentals II\n\n\nContents\n\n\nFunctions of random variables\nMultivariate distributions\nDecision and estimation: basic definitions\nTests of significance\n\n\n\n\nL3 | Introduction to Bayesian inference\n\n\nContents\n\n\nIntroduction to Bayesian modelling\nConjugacy with distributions\nBayesian polynomial regression\n\n\n\n\nL4 | The uniqueness of the Normal distribution\n\n\nContents\n\n\nMarginal distributions\nConditional distributions\nNataf (and other) transforms\n\n\n\n\nL5 | Exact and approximate inference\n\n\nContents\n\n\nMaximum likelihood and maximum aposteriori estimate\nMarkov chain monte Carlo\nExpectation maximization\n\n\n\n\nL6 | Introduction to Gaussian processes\n\n\nContents\n\n\nMotivation and parallels with Bayesian optimization\nMercer kernels (spectral densities, periodic, Matern, squared exponential)\nMaking new kernels from old ones\nHyperparameters (and their hyperparameters)\n\n\n\n\nL7 | Gaussian likelihoods\n\n\nContents\n\n\nPrediction using noise-free observations\nPrediction with noisy observations\nWeight-space vs. function-space perspectives\nSemi-parametric models\n\n\n\n\nL8 | Gaussian & non-Gaussian likelihoods\n\n\nContents\n\n\nReproducing kernel Hilbert spaces\nRepresenter theorem\nNon-Gaussian likelihoods and classification\n\n\n\n\nL9 | Scaling Gaussian processes\n\n\nContents\n\n\nComputing the matrix inverse via Cholesky decomposition\nSubset of data approaches\nNystrom approximation\nInducing points\n\n\n\n\nL10 | Scaling Gaussian processes II\n\n\nContents\n\n\nVariational inference\nELBO derivation\nMinimizing the KL-divergence practically\n\n\n\n\nL11 | Multiple kernel learning\n\n\nContents\n\n\nEmpirical Bayes\nMultiple kernel learning\nGeneralized additive models\n\n\n\n\nL12 | Gaussian processes and deep neural networks\n\n\nContents\n\n\nSingle and deep MLPs\nDeep Gaussian processes\nPosterior inference\n\n\n\n\nL13 | Designing bespoke deep Gaussian processes\n\n\nContents\n\n\nCovariance structure\nWarping inputs\nParallels to normalizing flows\nHierarchy of hyperparameters\n\n\n\n\nL14 | Time-series forecasting with Gaussian processes\n\n\nContents\n\n\nLinear Gaussian state space model"
  },
  {
    "objectID": "index.html#office-hours",
    "href": "index.html#office-hours",
    "title": "Overview",
    "section": "Office hours",
    "text": "Office hours\nProfessor Seshadri’s office hours:\n\n\n\nLocation\nTime\n\n\n\n\nGU 341\nTBD\n\n\nGU 341\nTBD\n\n\n\nLocation may change during term."
  },
  {
    "objectID": "index.html#textbooks",
    "href": "index.html#textbooks",
    "title": "Overview",
    "section": "Textbooks",
    "text": "Textbooks\nThis course will make heavy use of the following texts:\n\nRasmussen, C. E., Williams, C. K. Gaussian Processes for Machine Learning, The MIT Press, 2006.\nMurphy, K. P., Probabilistic Machine Learning: Advanced Topics, The MIT Press, 2023.\n\n\n\n\nCode\n## Papers\n\nThis course will also rely on the following papers; students are encouraged to read this in their own time. \n\n- Deep GP\n- Aeroengine\n- GPs with polynomial kernels\n-"
  }
]